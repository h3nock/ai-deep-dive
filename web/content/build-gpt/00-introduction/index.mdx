---
title: "Introduction"
step: 0
description: "The big picture: how Large Language Models actually work."
---

<Description>
  You send a prompt to ChatGPT and seconds later, a response streams back word by word. What's actually happening inside? In this chapter, we'll see the full picture before diving into technical details. Understanding how the pieces fit together will make each component much easier to grasp as we build them.
</Description>


<Step title="The Core Idea">
  <Description>
    At its core, a Large Language Model does one thing: **predict the next word**. When you give it "The cat sat on the", the model looks at those words and outputs a probability for every possible next word in its vocabulary.
  </Description>

  <div style={{ marginTop: "var(--space-flow)", marginBottom: "var(--space-flow)" }}>
    <div className="text-xs font-medium text-muted uppercase tracking-wider mb-2">Iteration 1</div>
    <div className="p-4 bg-terminal rounded-lg border border-border">
    <div className="flex items-start gap-4">
      <div className="flex items-center gap-1.5 flex-wrap font-mono text-base">
        <span className="px-2.5 py-1 bg-surface rounded">The</span>
        <span className="px-2.5 py-1 bg-surface rounded">cat</span>
        <span className="px-2.5 py-1 bg-surface rounded">sat</span>
        <span className="px-2.5 py-1 bg-surface rounded">on</span>
        <span className="px-2.5 py-1 bg-surface rounded">the</span>
      </div>
      <span className="text-muted mt-1 text-lg">→</span>
      <div className="flex flex-col gap-1.5 font-mono text-base">
        <span className="px-2.5 py-1 bg-success/10 border border-success/30 rounded text-success">mat (15%)</span>
        <span className="px-2.5 py-1 bg-surface rounded text-muted">floor (12%)</span>
        <span className="px-2.5 py-1 bg-surface rounded text-muted">roof (3%)</span>
        <span className="text-muted text-sm">...</span>
      </div>
    </div>
    </div>
  </div>

  <Description>
    The model picks one word (usually sampling from the high-probability ones), appends it to the input, and repeats.
  </Description>

  <div style={{ marginTop: "var(--space-flow)", marginBottom: "var(--space-flow)" }}>
    <div className="text-xs font-medium text-muted uppercase tracking-wider mb-2">Iteration 2</div>
    <div className="p-4 bg-terminal rounded-lg border border-border">
    <div className="flex items-start gap-4">
      <div className="flex items-center gap-1.5 flex-wrap font-mono text-base">
        <span className="px-2.5 py-1 bg-surface rounded">The</span>
        <span className="px-2.5 py-1 bg-surface rounded">cat</span>
        <span className="px-2.5 py-1 bg-surface rounded">sat</span>
        <span className="px-2.5 py-1 bg-surface rounded">on</span>
        <span className="px-2.5 py-1 bg-surface rounded">the</span>
        <span className="px-2.5 py-1 bg-success/20 rounded text-success">mat</span>
      </div>
      <span className="text-muted mt-1 text-lg">→</span>
      <div className="flex flex-col gap-1.5 font-mono text-base">
        <span className="px-2.5 py-1 bg-success/10 border border-success/30 rounded text-success">and (18%)</span>
        <span className="px-2.5 py-1 bg-surface rounded text-muted">. (14%)</span>
        <span className="px-2.5 py-1 bg-surface rounded text-muted">while (8%)</span>
        <span className="text-muted text-sm">...</span>
      </div>
    </div>
    </div>
  </div>

  <Description>
    This loop continues until the response is complete. This simple mechanism, repeated many times during training on massive text data, produces behavior that looks remarkably like understanding.
  </Description>
</Step>


<Step title="The Full Pipeline">
  <Description>
    When you send a prompt to an LLM and receive a response, your text goes through a series of transformations.
  </Description>

  <Callout type="note" muted>
    Don't worry about understanding exactly how or why these steps happen right now. We will explore each one in detail later. For now, just focus on the high-level stages your text passes through.
  </Callout>

  <ProcessTimeline steps={[
    {
      title: "Text → Bytes",
      data: '"Hello" → [72, 101, 108, 108, 111]',
      description: "Characters become numbers computers can process."
    },
    {
      title: "Bytes → Tokens",
      data: "[72, 101, 108, 108, 111] → [15496]",
      description: "We group bytes into chunks so there's a shorter sequence to process."
    },
    {
      title: "Tokens → Vectors",
      data: "[15496] → [0.12, -0.48, 0.91, ...]",
      description: "Plain token IDs don't capture meaning. This step converts each token into a vector that represents what it means."
    },
    {
      title: "Vectors → Transformer → Next Token",
      description: "The Transformer reads context and predicts the next token."
    },
    {
      title: "Token → Text",
      description: "Tokens convert back to text, looping until the response is complete."
    }
  ]} />
</Step>


<Step title="Learning from Data">
  <Description>
    LLMs are built to predict the next word, but how do they get *good* at it? A freshly created model is just millions of random numbers. It knows nothing about language, grammar, or the world. If you asked it to complete "The cat sat on the", it might confidently answer "purple" or "seventeen".
  </Description>

  <Description>
    This is where training comes in. We show the model enormous amounts of text and let it learn from its mistakes, over and over, until patterns emerge.
  </Description>

  <ProcessTimeline steps={[
    {
      title: "Input",
      data: '"The king sat on the"',
      description: "A training example from text data."
    },
    {
      title: "Prediction",
      description: 'The model guesses the next word. At first, it\'s random (e.g., "banana").'
    },
    {
      title: "Comparison",
      description: 'The actual next word is "throne". We measure the error.'
    },
    {
      title: "Update",
      description: 'The model adjusts its internal numbers to make "throne" more likely next time.'
    }
  ]} />

  <Description>
    By repeating this process billions of times on massive datasets, the model's parameters gradually tune themselves. Words with similar meanings align in the vector space, and the model learns to recognize complex patterns, grammar, and facts.
  </Description>
</Step>


<Step title="What You Will Build">
  <Description>
    In this course, we will build every piece of this pipeline from scratch. By the end, you will understand not just *what* an LLM does, but *how* and *why* each component exists.
  </Description>

  <Description>
    Throughout the course, you'll implement what you learn through coding challenges at the end of each chapter. Most challenges run directly in the built‑in editor and give feedback immediately. For local project workflows, check the <a href="/setup" className="underline underline-offset-2 hover:text-primary">setup page</a> for the current CLI status and rollout plan.
  </Description>

  <Description>
    Let's start with the very first step: how computers see text.
  </Description>
</Step>
