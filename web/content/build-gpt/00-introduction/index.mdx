---
title: "Introduction"
step: 0
description: "The big picture: how Large Language Models actually work."
---

<Description>
  You send a prompt to ChatGPT and seconds later, a response streams back word by word. What's actually happening inside? This chapter walks through the process end to end, from the moment text enters the model to the moment a prediction comes out. The rest of the course builds each piece of this pipeline from the ground up.
</Description>


<Step title="The Core Idea">
  <Description>
    At its core, a Large Language Model does one thing: **predict the next word**. When you give it "The cat sat on the", the model looks at those words and outputs a probability for every possible next word in its vocabulary.
  </Description>

  <div style={{ marginTop: "var(--space-flow)", marginBottom: "var(--space-flow)" }}>
    <div className="text-xs font-medium text-muted uppercase tracking-wider mb-2">Iteration 1</div>
    <div className="p-4 bg-terminal rounded-lg border border-border">
    <div className="flex items-start gap-4">
      <div className="flex items-center gap-1.5 flex-wrap font-mono text-base">
        <span className="px-2.5 py-1 bg-surface rounded">The</span>
        <span className="px-2.5 py-1 bg-surface rounded">cat</span>
        <span className="px-2.5 py-1 bg-surface rounded">sat</span>
        <span className="px-2.5 py-1 bg-surface rounded">on</span>
        <span className="px-2.5 py-1 bg-surface rounded">the</span>
      </div>
      <span className="text-muted mt-1 text-lg">→</span>
      <div className="flex flex-col gap-1.5 font-mono text-base">
        <span className="px-2.5 py-1 bg-success/10 border border-success/30 rounded text-success">mat (15%)</span>
        <span className="px-2.5 py-1 bg-surface rounded text-muted">floor (12%)</span>
        <span className="px-2.5 py-1 bg-surface rounded text-muted">roof (3%)</span>
        <span className="text-muted text-sm">...</span>
      </div>
    </div>
    </div>
  </div>

  <Description>
    The model samples one word from the high-probability candidates, appends it to the input, and repeats.
  </Description>

  <div style={{ marginTop: "var(--space-flow)", marginBottom: "var(--space-flow)" }}>
    <div className="text-xs font-medium text-muted uppercase tracking-wider mb-2">Iteration 2</div>
    <div className="p-4 bg-terminal rounded-lg border border-border">
    <div className="flex items-start gap-4">
      <div className="flex items-center gap-1.5 flex-wrap font-mono text-base">
        <span className="px-2.5 py-1 bg-surface rounded">The</span>
        <span className="px-2.5 py-1 bg-surface rounded">cat</span>
        <span className="px-2.5 py-1 bg-surface rounded">sat</span>
        <span className="px-2.5 py-1 bg-surface rounded">on</span>
        <span className="px-2.5 py-1 bg-surface rounded">the</span>
        <span className="px-2.5 py-1 bg-success/20 rounded text-success">mat</span>
      </div>
      <span className="text-muted mt-1 text-lg">→</span>
      <div className="flex flex-col gap-1.5 font-mono text-base">
        <span className="px-2.5 py-1 bg-success/10 border border-success/30 rounded text-success">and (18%)</span>
        <span className="px-2.5 py-1 bg-surface rounded text-muted">. (14%)</span>
        <span className="px-2.5 py-1 bg-surface rounded text-muted">while (8%)</span>
        <span className="text-muted text-sm">...</span>
      </div>
    </div>
    </div>
  </div>

  <Description>
    Each new word feeds back in as input, and the process repeats until the response is complete. Every response you see from a language model is built one word at a time through this loop.
  </Description>
</Step>


<Step title="The Full Pipeline">
  <Description>
    When you send a prompt to an LLM and receive a response, your text goes through a series of transformations.
  </Description>

  <Callout type="note" muted>
    Don't worry about understanding exactly how or why these steps happen right now. We will explore each one in detail later. For now, just focus on the high-level stages your text passes through.
  </Callout>

  <ProcessTimeline steps={[
    {
      title: "Text → Bytes",
      data: '"Hello" → [72, 101, 108, 108, 111]',
      description: "Characters become numbers computers can process."
    },
    {
      title: "Bytes → Tokens",
      data: "[72, 101, 108, 108, 111] → [15496]",
      description: "Bytes are grouped into meaningful chunks, giving the model a shorter sequence to work with."
    },
    {
      title: "Tokens → Vectors",
      data: "[15496] → [0.12, -0.48, 0.91, ...]",
      description: "Each token ID becomes a vector, a list of numbers that captures the token's meaning."
    },
    {
      title: "Vectors → Transformer → Next Token",
      description: "The Transformer processes the full sequence and predicts what comes next."
    },
    {
      title: "Token → Text",
      description: "The predicted token converts back to text, and the loop continues until the response is complete."
    }
  ]} />
</Step>


<Step title="Learning from Data">
  <Description>
    LLMs are built to predict the next word, but how do they get *good* at it? A freshly created model is just millions of random numbers. It knows nothing about language, grammar, or the world. If you asked it to complete "The cat sat on the", it might confidently answer "purple" or "seventeen".
  </Description>

  <Description>
    These random numbers become useful through training. The model processes billions of text examples, and after each prediction, its parameters are adjusted to reduce the error between what it guessed and what actually came next.
  </Description>

  <ProcessTimeline steps={[
    {
      title: "Input",
      data: '"The king sat on the"',
      description: "A training example from text data."
    },
    {
      title: "Prediction",
      description: 'The model guesses the next word. At first, it\'s random (e.g., "banana").'
    },
    {
      title: "Comparison",
      description: 'The actual next word is "throne". We measure the error.'
    },
    {
      title: "Update",
      description: 'The model adjusts its internal numbers to make "throne" more likely next time.'
    }
  ]} />

  <Description>
    By repeating this process billions of times across massive datasets, the model's parameters gradually shift from random noise into structured representations. It learns to recognize grammar, word relationships, and factual associations, all from predicting the next word.
  </Description>
</Step>


<Step title="What You Will Build">
  <Description>
    In this course, we will build every piece of this pipeline from scratch. By the end, you will understand not just *what* an LLM does, but *how* and *why* each component exists.
  </Description>

  <Description>
    Throughout the course, you'll implement what you learn through coding challenges at the end of each chapter. Most challenges run directly in the built‑in editor and give feedback immediately. For local project workflows, check the <a href="/setup" className="underline underline-offset-2 hover:text-primary">setup page</a> for the current CLI status and rollout plan.
  </Description>

  <Description>
    Let's start with the very first step: how computers see text.
  </Description>
</Step>
