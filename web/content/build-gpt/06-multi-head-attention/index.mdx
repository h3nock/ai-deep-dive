---
title: "Multi-Head Attention"
step: 6
description: "The limitation of single-head attention and how multiple heads overcome it."
---

<div className="flex flex-col" style={{ gap: 'var(--space-flow)' }}>
<Description noMargin>
  The previous chapter introduced single-head attention, where each token examines the full sequence, measures relevance, and gathers information from other tokens. The representation of each token is now shaped by its context rather than fixed at the start.
</Description>

<Description>
  However, there's a limitation to what a single attention head can capture, and understanding this limitation explains why transformers use multiple heads running in parallel rather than just one.
</Description>
</div>


<Step title="The Limitation of One Head">
  <Description>
    Single-head attention uses one set of projection matrices: `W_Q`, `W_K`, and `W_V`. These transform each token's embedding into a Query, a Key, and a Value. The choice of projection determines which aspects of the embedding influence the attention computation.
  </Description>

  <Description>
    With one `W_Q` and one `W_K`, there is one way of measuring similarity between tokens. The dot product between a Query and a Key reflects one learned notion of relevance. Relationships that would require a different similarity measure cannot be captured alongside this one.
  </Description>

  <Description>
    These similarity scores pass through softmax, producing attention weights that sum to 1. Each token gets one distribution over all positions, and these weights determine how Value vectors combine into the output. Because the weights must sum to 1, every position competes for share in this single distribution.
  </Description>

  <Description>
    The result is one weighted sum of Values per token. One projection, one similarity measure, one attention distribution, one output blend. If the data contains multiple patterns of relevance, each requiring different projections, similarity measures, or weightings, a single attention operation cannot capture them all.
  </Description>
</Step>


<Step title="Multi-Head Attention">
  <Description>
    Multi-head attention addresses these limitations by running multiple attention operations in parallel. Each operation is called a head, and each head has its own independent set of projection matrices: its own `W_Q`, `W_K`, and `W_V`.
  </Description>

  <Description>
    With separate projection matrices, each head transforms the embeddings differently and learns its own notion of similarity through its own `W_Q` and `W_K`. As a result, what appears relevant in one head may be irrelevant in another, and the heads can capture different patterns of relevance from the same input.
  </Description>

  <Description>
    Each head also computes its own attention distribution and produces its own weighted sum of Values. Within each head, weights still sum to 1 and positions still compete for attention weight. But different heads operate independently of each other. A position that receives weak attention in one head can receive strong attention in another. By having multiple heads, the model is no longer forced to commit to a single attention pattern. Each head can weight the sequence according to its own learned relevance, and their outputs combine to form the final result.
  </Description>
</Step>

