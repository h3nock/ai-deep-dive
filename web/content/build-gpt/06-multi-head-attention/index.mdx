---
title: "Multi-Head Attention"
step: 6
description: "The limitation of single-head attention and how multiple heads overcome it."
---

<div className="flex flex-col" style={{ gap: 'var(--space-flow)' }}>
<Description noMargin>
  The previous chapter introduced single-head attention, where each token examines the full sequence, measures relevance, and gathers information from other tokens. The representation of each token is now shaped by its context rather than fixed at the start.
</Description>

<Description>
  However, there's a limitation to what a single attention head can capture, and understanding this limitation explains why transformers use multiple heads running in parallel rather than just one.
</Description>
</div>


<Step title="The Limitation of One Head">
  <Description>
    Single-head attention uses one set of projection matrices: `W_Q`, `W_K`, and `W_V`. These transform each token's embedding into a Query, a Key, and a Value. With one `W_Q` and one `W_K`, there is only one way to measure similarity between tokens, through the dot product of their projected Query and Key.
  </Description>

  <Description>
    The result is one attention distribution based on this single similarity measure.  If tokens need to attend to each other based on multiple notions of similarity independently, single-head attention has no way to provide that.
  </Description>
</Step>


<Step title="Multi-Head Attention">
  <Description>
    Multi-head attention addresses these limitations by running multiple attention operations in parallel. Each operation is called a head, and each head has its own independent set of projection matrices: its own `W_Q`, `W_K`, and `W_V`.
  </Description>

  <Description>
    With separate projection matrices, each head transforms the embeddings differently and learns its own notion of similarity through its own `W_Q` and `W_K`. As a result, what appears relevant in one head may be irrelevant in another, and the heads can capture different patterns of relevance from the same input.
  </Description>

  <Description>
    Each head also computes its own attention distribution and produces its own weighted sum of Values. Within each head, weights still sum to 1 and positions still compete for attention weight. But different heads operate independently of each other. A position that receives weak attention in one head can receive strong attention in another. By having multiple heads, the model is no longer forced to commit to a single attention pattern. Each head can weight the sequence according to its own learned relevance, and their outputs combine to form the final result.
  </Description>
</Step>

