---
title: "Positional Encoding"
step: 4
description: "Why giving AI a sense of order requires re-inventing how we count."
---

<div className="flex flex-col" style={{ gap: 'var(--space-flow)' }}>
<Description noMargin>
  In the previous chapter, we solved the "meaning representation" problem by mapping each token to an embedding vector. Each token now has an embedding vector that the model can use to capture *semantic* features. But we left one question unanswered: how does the model know *where* each word appears?
</Description>

<Description>
  Recall that Transformers process all words **in parallel** rather than one-by-one. This is what makes them fast. But it also means the model receives all embeddings at once, with no inherent notion of "first" or "last."
</Description>

<Description>
  To see why this matters, consider two sentences: **"Alice gave Bob a book"** and **"Bob gave Alice a book"**. They contain the exact same words, so if we look up their embeddings, both sentences produce the same set of vectors. The only difference is the order.
</Description>

<div className="my-6 grid grid-cols-1 md:grid-cols-2 gap-4">
  <div className="p-4 bg-surface rounded-lg border border-border">
    <div className="text-xs font-semibold text-muted uppercase tracking-wider mb-2">Sentence A</div>
    <div className="text-primary font-medium mb-3">"Alice gave Bob a book"</div>
    <div className="font-mono text-xs text-secondary">
      [<span className="text-emerald-400">E(Alice)</span>, E(gave), <span className="text-sky-400">E(Bob)</span>, ...]
    </div>
  </div>
  <div className="p-4 bg-surface rounded-lg border border-border">
    <div className="text-xs font-semibold text-muted uppercase tracking-wider mb-2">Sentence B</div>
    <div className="text-primary font-medium mb-3">"Bob gave Alice a book"</div>
    <div className="font-mono text-xs text-secondary">
      [<span className="text-sky-400">E(Bob)</span>, E(gave), <span className="text-emerald-400">E(Alice)</span>, ...]
    </div>
  </div>
</div>

<Description>
  The Transformer's core mechanism, **Self-Attention**, determines how words relate by computing a **compatibility score** for each pair of tokens. Internally, it's a (scaled) dot product between their embedding vectors, producing a **single number** representing connection strength.
</Description>

<Description>
  Here is the problem: if we only feed word embeddings (no position info), the compatibility between "Alice" and "Bob" depends on *what* they are, not *where* they appear. Both sentences would produce identical scores. The model can see that "Alice" and "Bob" are related, but it cannot tell which one is the Subject (giver) and which is the Object (receiver). Order is essential for meaning, yet nothing in these vectors encodes it. We need to find a way to inject position information.
</Description>
</div>



<Step title="1. Attempt 1: Counting Up">
  <Description>
    The most naive idea is to number the positions 1, 2, 3... and inject the raw index into the token embedding by adding that number to every dimension. Let's trace through what happens when the word "Apple" appears at position 1000 in a long document:
  </Description>

  <div className="my-4">
    <div className="p-4 bg-[#121212] rounded-lg border border-zinc-800 font-mono text-sm">
      <div className="flex items-center gap-4">
        <span className="text-muted w-28 shrink-0">E("Apple")</span>
        <span className="text-primary">[0.05, -0.02, 0.11]</span>
      </div>
      <div className="flex items-center gap-4 mt-1">
        <span className="text-muted w-28 shrink-0">+ Position 1000</span>
        <span className="text-secondary">[1000, 1000, 1000]</span>
      </div>
      <div className="border-t border-zinc-700 mt-2 pt-2 flex items-center gap-4">
        <span className="text-muted w-28 shrink-0">= Result</span>
        <span className="text-amber-400 font-bold">[1000.05, 999.98, 1000.11]</span>
      </div>
    </div>
  </div>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">Problem 1: Drowned Meaning</h4>

  <Description>
    Look at the result. The values that encoded "Apple" (0.05, -0.02, 0.11) have become invisible, completely swamped by the position signal. Whether the original word was "Apple", "Banana", or "King", the output is essentially [1000, 1000, 1000]. The semantic meaning has been **drowned**.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">Problem 2: Scale Mismatch</h4>

  <Description>
    Recall from Chapter 3 that neural networks learn by applying **weights** to their inputs. If we used raw position indices like 1, 1000, and 10,000, the model has to handle a huge dynamic range. That makes learning harder because the same weight has to work for all scales. A weight tuned for position 1 can behave very differently at position 1000.
  </Description>


  <Description>
    Both problems stem from the same root cause: **unbounded values**. As sequences get longer, the position numbers grow without limit.
  </Description>
</Step>

<Step title="2. Attempt 2: Scaling to [0, 1]">
  <Description>
    Let's fix the magnitude problem by dividing by the total sequence length, keeping every position between 0 and 1:
  </Description>

  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800 font-mono text-sm">
    Position = index / total_length
  </div>

  <Description>
    This keeps values bounded, but introduces two new problems.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">Problem 1: Variable Step Size</h4>

  <Description>
    The model often benefits from learning relative patterns, like "pay attention to the word immediately before me." But what does "one position back" mean in this scheme? Because we divide by the sequence length, the numeric step size changes with length. In a 10-word sentence, one step back is -0.10. In a 50-word sentence, the same step is only -0.02. So the same relative idea ("one token away") corresponds to different numeric values depending on sentence length. If you want to explore, play the animation or drag the slider:
  </Description>

  <NormalizedStepSize />

  <Description>
    This inconsistency makes it harder for the model to learn a single, length-independent rule for "neighbor" or any relative pattern.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">Problem 2: Length-Dependent Absolute Positions</h4>

  <Description>
    Consider the sentence **"The cat sat."** where "cat" is at index 1, giving it a position value of 1/3 ≈ 0.33. Now consider **"The cat sat on the mat."** The word "cat" is still at index 1, but now its position value is 1/6 ≈ 0.17. Same word, same index, completely different encoding. This means "second word" does not map to a consistent numeric value across examples, undermining any attempt to learn stable absolute position patterns. While normalization keeps magnitudes small, neither relative nor absolute positions are stable, making generalization messy.
  </Description>
</Step>

<Step title="3. What Makes a Good Positional Encoding?">
  <Description>
    We've tried two simple approaches and saw why they are awkward. Before we try again, let's step back and define exactly what properties a positional encoding should have to make the signal useful and easy for the model to learn from.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">1. Scale Compatibility</h4>

  <Description>
    As we learned from Attempt 1, the position signal should live in a reasonable numeric range so it doesn't overwhelm the token embedding. We want the model to represent "Apple at position 5", not a vector dominated by the position index.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">2. Unique and Deterministic</h4>

  <Description>
    Every position within the context window should produce a distinct encoding, and that encoding should be identical every time. Position 5 must always map to the exact same vector, whether during training or inference. If two positions share an encoding, the model has no way to tell them apart.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">3. Consistent Relative Structure</h4>

  <Description>
    A good positional encoding makes relative offsets easy for the model to use. Ideally, "three words ahead" should behave consistently whether you're near the start of a sentence or deep into a long sequence. The relationship between any position P and position P+k should behave consistently regardless of P, allowing the model to learn portable attention patterns.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">4. Generalization to Longer Sequences</h4>

  <Description>
    Sometimes we want the model to handle sequences longer than what it saw during training. The encoding should still behave sensibly at larger positions instead of breaking or becoming meaningless.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">5. Smooth and Continuous</h4>

  <Description>
    Positions are discrete, but it still helps if the positional signal changes in a predictable, structured way as we move from one position to the next. This is because neural networks learn by making small adjustments. If the model's prediction is slightly wrong, it nudges its weights a little bit in the right direction. This works better when small changes in input lead to small changes in output. 
  </Description>

  <Description>
    Now we have our design goals. The puzzle is how can we represent many positions using only small, bounded values while keeping the signal smooth and consistent?
  </Description>
</Step>



<Step title="4. Borrowing from Number Systems">
  <Description>
    Now that we know what properties the encoding needs, let's look for inspiration. We already use systems that satisfy most of these requirements every day. Consider how we write the number **459**: each digit is **bounded** (0 to 9), the full pattern is **unique** (no other number looks like 459), and moving by +1 follows a **consistent pattern** (459 → 460 → 461).
  </Description>

  <Description>
    The key insight is that we use multiple columns changing at different speeds. The ones column cycles fastest (0→9, then wraps). The tens column cycles slower. The hundreds column slower still. This multi-speed structure lets us represent arbitrarily large numbers using only small, bounded digits.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">What About Decimal?</h4>

  <Description>
Decimal has the multi-column idea and handles infinite positions, but it is not a great positional signal if we try to inject it directly into embeddings. First, **scale:** digits 0 to 9 are not huge, but if we literally inject them as-is, they can still shift the embedding distribution, which is often kept roughly zero-centered. Second, **discontinuities:** decimal digits are discrete. When the ones digit wraps from 9 to 0, the representation changes abruptly. That makes the signal less structured for learning local relationships.  
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">What About Binary?</h4>

  <Description>
    Binary brings the scale down further. Each digit is just 0 or 1. It still has the multi-column idea, so it can represent large positions compactly.
  </Description>

  <Description>
    The limitation is that binary is still discrete. Each dimension can only take two values (0 or 1), so moving from one position to the next always causes abrupt changes. Sometimes a single bit changes, sometimes several bits change, but in all cases the signal is not gradual. This discreteness means nearby positions do not reliably map to nearby vectors, which makes it harder to learn simple "local" rules from the positional signal. What we want instead is the same multi-speed idea to represent large positions, but with values that vary continuously. If we can fix this one remaining issue, we have our encoding.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">Making Binary Smooth</h4>

  <Description>
    Binary's problem is that 0 and 1 are just two disconnected points with no values in between. So let's keep the best parts (scale and multi-speed structure) but replace discrete digits with a smooth signal. What we want in each dimension is a value that changes continuously and repeats over time, like a dial. As we move through positions, the values should rise and fall smoothly instead of jumping between fixed states.
  </Description>

  <BinaryVsSmooth />

  <Description>
    This smooth oscillation preserves the core insight from our number system analogy: multiple dimensions changing at different speeds, all staying in a bounded range, giving each position a distinctive signature. Unlike binary, the values change gradually from one position to the next, providing the model with a cleaner signal for learning local patterns. All that remains is choosing the function that produces this kind of smooth cycle.
  </Description>
</Step>

<Step title="5. Finding the Function">
  <Description>
    If you have taken trigonometry, you might recognize the shape of our smooth oscillation. It looks like a **sine wave**. Our visualization matches a shifted version (sin(x) + 1) / 2 to keep values between 0 and 1. But the raw sine function naturally oscillates between -1 and +1, which raises a design choice: should we shift the wave to stay positive, or use the raw centered values?
  </Description>

  <Description>
    In practice, the centered range -1 to +1 is the better default. Embedding values are typically initialized around 0, and techniques like **Layer Normalization** (covered later) keep them there throughout the network. A positional signal centered around 0 blends naturally with this distribution, while a 0 to 1 shift would add an unnecessary constant bias to every dimension.
  </Description>

  <Description>
    Now we have the final function, each dimension gets its own sine wave, cycling at a different frequency. Play the animation to see how each dimension evolves across positions:
  </Description>

  <FrequencyWaves />

  <Description>
    Fast waves change quickly, helping separate nearby positions. Slow waves change gradually, helping distinguish distant positions. Together, they create a unique fingerprint for every position within the context window
  </Description>
</Step>

<Step title="6. Making Relative Positions Easy">
  <Description>
    We now have a working positional signal using sine waves at different frequencies, giving each position a unique fingerprint. However, the original Transformer paper actually pairs every sine with a **cosine** at the same frequency. Why?
  </Description>

  <Description>
    The answer lies in how positions **relate to each other**. Language understanding often depends on relative positions: "the word immediately before" or "three tokens ahead." We want the encoding to make these relationships easy for the model to learn. Ideally, going from position `pos` to position `pos + k` should follow a simple, predictable pattern.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">The Limitation of Sine Alone</h4>

  <Description>
    Suppose we only store the sine value for a given frequency ω. At position `pos`, our encoding is sin(ω · pos). What happens when we shift to position `pos + k`? From high school trigonometry, we can expand sin(ω(pos + k)) using the angle addition formula:
  </Description>

  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800">
    <div className="font-mono text-sm text-center text-primary">
      sin(ω · (pos + k)) = sin(ω · pos) · cos(ω · k) + cos(ω · pos) · sin(ω · k)
    </div>
  </div>

  <Description>
    Look at what's needed on the right side: sin(ω · pos), which we have (it's our current encoding), and cos(ω · pos), which we **don't** have. Without the cosine, calculating the shifted value is mathematically messy. The angle addition formula requires both components. Without it, we lose the direct linear relationship between positions, forcing the model to learn a more complex non-linear approximation instead of a simple linear transformation.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">Pairing Sine with Cosine</h4>

  <Description>
    The fix is simple: **store both**. For each frequency ω, we keep both sin(ω · pos) and cos(ω · pos) in our encoding. This gives us a 2D pair for each frequency:
  </Description>

  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800 text-center">
    <span className="font-mono text-primary">PE(pos) = [ sin(ω · pos), cos(ω · pos) ]</span>
  </div>

  <Description>
    Now we have everything we need. When we want to express the encoding at a shifted position `pos + k`, we can use both angle addition formulas:
  </Description>

  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800 space-y-1">
    <div className="font-mono text-sm text-center text-primary">
      sin(ω(pos + k)) = cos(ω·k) · sin(ω·pos) + sin(ω·k) · cos(ω·pos)
    </div>
    <div className="font-mono text-sm text-center text-primary">
      cos(ω(pos + k)) = cos(ω·k) · cos(ω·pos) - sin(ω·k) · sin(ω·pos)
    </div>
  </div>

  <Description>
    Notice something important: the right side only uses sin(ω·pos) and cos(ω·pos), which we already have stored, multiplied by constants that depend only on the offset `k`. This means we can write the shifted encoding as a **matrix multiplication**:
  </Description>

  <div className="my-6 p-5 bg-surface rounded-lg border border-border overflow-x-auto">
    <div className="flex items-center justify-center gap-3 font-mono text-sm">
      {/* Target Vector */}
      <div className="flex flex-col gap-1 p-2 border-l-2 border-r-2 border-amber-500/50 rounded shrink-0">
        <span className="text-amber-400 whitespace-nowrap">sin(ω(pos+k))</span>
        <span className="text-amber-400 whitespace-nowrap">cos(ω(pos+k))</span>
      </div>
      
      <span className="text-muted shrink-0">=</span>

      {/* Rotation Matrix */}
      <div className="flex flex-col gap-1 p-2 border-l-2 border-r-2 border-blue-500/50 rounded bg-[#121212] shrink-0">
        <div className="flex gap-4">
          <span className="min-w-[70px] text-center text-blue-400 whitespace-nowrap">cos(ω·k)</span>
          <span className="min-w-[70px] text-center text-blue-400 whitespace-nowrap">sin(ω·k)</span>
        </div>
        <div className="flex gap-4">
          <span className="min-w-[70px] text-center text-blue-400 whitespace-nowrap">-sin(ω·k)</span>
          <span className="min-w-[70px] text-center text-blue-400 whitespace-nowrap">cos(ω·k)</span>
        </div>
      </div>

      <span className="text-muted shrink-0">×</span>

      {/* Current Vector */}
      <div className="flex flex-col gap-1 p-2 border-l-2 border-r-2 border-emerald-500/50 rounded shrink-0">
        <span className="text-emerald-400 whitespace-nowrap">sin(ω·pos)</span>
        <span className="text-emerald-400 whitespace-nowrap">cos(ω·pos)</span>
      </div>
    </div>
  </div>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">The Rotation Matrix</h4>

  <Description>
    The 2×2 matrix in the middle is a **rotation matrix**, the standard matrix that rotates any 2D point by a fixed angle. In our case, that angle is ω · k. This has a beautiful geometric interpretation: the (sin, cos) pair traces out a circle as position increases, and shifting by k positions corresponds to rotating this point around the circle by angle ω · k. Explore this in the interactive visualization below:
  </Description>

  <RotationVisualization />

  <Description>
    Notice that the rotation matrix depends **only on k** (the offset), not on the starting position. Whether you start at position 5, 50, or 500, shifting by k positions always applies the same transformation. This is exactly what we wanted: a simple, predictable pattern for relative positions. The relationship between any position and "k steps ahead" is always the same rotation, regardless of where you are in the sequence. And because rotation is a **linear operation** (just matrix multiplication), it fits naturally into the Transformer's architecture. The model can learn weights that capture patterns like "attend to the previous word" or "look 3 tokens ahead" and apply them uniformly across the entire sequence.
  </Description>
</Step>


<Step title="7. Decoding the Formula">
  <Description>
    At this point, you already understand *why* we need multiple frequencies and why sine/cosine pairs matter. The original Transformer formula is just a compact way to express these ideas. Let's decode each piece.
  </Description>

  <div className="my-6 p-5 bg-surface rounded-lg border border-border">
    <div className="text-center mb-4">
      <span className="text-sm font-medium text-muted">The Sinusoidal Positional Encoding</span>
    </div>
    <div className="text-center space-y-2">
      <div className="font-mono text-lg text-primary">
        PE(pos, 2i) = sin(pos / 10000<sup>2i/d<sub>model</sub></sup>)
      </div>
      <div className="font-mono text-lg text-primary">
        PE(pos, 2i+1) = cos(pos / 10000<sup>2i/d<sub>model</sub></sup>)
      </div>
    </div>
  </div>

  <Description>
    Let's decode this piece by piece.
  </Description>

</Step>
