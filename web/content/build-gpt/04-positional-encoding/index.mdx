---
title: "Positional Encoding"
step: 4
description: "Why giving AI a sense of order requires re-inventing how we count."
---

<div className="flex flex-col" style={{ gap: 'var(--space-flow)' }}>
<Description noMargin>
  In the previous chapter, we solved the "meaning representation" problem by mapping each token to an embedding vector. Each token now has an embedding vector that the model can use to capture *semantic* features. But we left one question unanswered: how does the model know *where* each word appears?
</Description>

<Description>
  Recall that Transformers process all words **in parallel** rather than one-by-one. This is what makes them fast. But it also means the model receives all embeddings at once, with no inherent notion of "first" or "last."
</Description>

<Description>
  To see why this matters, consider two sentences: **"Alice gave Bob a book"** and **"Bob gave Alice a book"**. They contain the exact same words, so if we look up their embeddings, both sentences produce the same set of vectors. The only difference is the order.
</Description>

<div className="my-6 grid grid-cols-1 md:grid-cols-2 gap-4">
  <div className="p-4 bg-surface rounded-lg border border-border">
    <div className="text-xs font-semibold text-muted uppercase tracking-wider mb-2">Sentence A</div>
    <div className="text-primary font-medium mb-3">"Alice gave Bob a book"</div>
    <div className="font-mono text-xs text-secondary">
      [<span className="text-emerald-400">E(Alice)</span>, E(gave), <span className="text-sky-400">E(Bob)</span>, ...]
    </div>
  </div>
  <div className="p-4 bg-surface rounded-lg border border-border">
    <div className="text-xs font-semibold text-muted uppercase tracking-wider mb-2">Sentence B</div>
    <div className="text-primary font-medium mb-3">"Bob gave Alice a book"</div>
    <div className="font-mono text-xs text-secondary">
      [<span className="text-sky-400">E(Bob)</span>, E(gave), <span className="text-emerald-400">E(Alice)</span>, ...]
    </div>
  </div>
</div>

<Description>
  The Transformer's core mechanism, **Self-Attention**, determines how words relate by computing a **compatibility score** for each pair of tokens. Internally, it's a (scaled) dot product between their embedding vectors, producing a **single number** representing connection strength.
</Description>

<Description>
  This raises a question: if attention only looks at what words mean, how would it know that "Alice" is the subject in one sentence and the object in another? Both cases produce identical scores. It seems like we need some way to encode where each word appears. What follows is one line of reasoning that leads to the original Transformer's solution.
</Description>
</div>



<Step title="1. Attempt 1: Counting Up">
  <Description>
    The most naive approach is to number the positions (1, 2, 3...) and inject that number into the embedding. There are two ways we might do this:
  </Description>

  <Description>
    **Adding the index to every dimension** is the simplest method. If "Apple" appears at position 1000, the addition looks like this:
  </Description>

  <div className="my-4">
    <div className="p-4 bg-[#121212] rounded-lg border border-zinc-800 font-mono text-sm">
      <div className="flex items-center gap-4">
        <span className="text-muted w-28 shrink-0">E("Apple")</span>
        <span className="text-primary">[0.05, -0.02, 0.11]</span>
        <span className="text-zinc-500 text-xs">// initialized small, zero-centered, usually N(0,1)</span>
      </div>
      <div className="flex items-center gap-4 mt-1">
        <span className="text-muted w-28 shrink-0">+ Position 1000</span>
        <span className="text-secondary">[1000, 1000, 1000]</span>
      </div>
      <div className="border-t border-zinc-700 mt-2 pt-2 flex items-center gap-4">
        <span className="text-muted w-28 shrink-0">= Result</span>
        <span className="text-amber-400 font-bold">[1000.05, 999.98, 1000.11]</span>
      </div>
    </div>
  </div>

  <Description>
    Look at the result: the small values that distinguished "Apple" (0.05, -0.02, 0.11) have become invisible. The output is essentially `[1000, 1000, 1000]` regardless of the original word. The semantic information is drowned by the position signal.
  </Description>

  <Description>
    This magnitude imbalance also causes training instability and makes it difficult for the model to learn meaningful word representations. The problem only gets worse as sequences get longer, since position values grow without bound.
  </Description>

  <Description>
    **Treating dimensions as digit slots** is an alternative. Position 999 becomes `[..., 0, 9, 9, 9]` and position 1000 becomes `[..., 1, 0, 0, 0]`, with leading zeros padding to a fixed width. This is more stable than the first approach since values stay bounded between 0 and 9, but still deviates from the small, zero-centered inputs that weight initialization assumes, risking instability.
  </Description>

  <Description>
    Beyond stability, this encoding is sparse. Most dimensions are zero, wasting vector capacity. And adjacent positions like 999 and 1000 produce completely different vectors, giving the model no hint that they are neighbors.
  </Description>

</Step>

<Step title="2. Attempt 2: Scaling to [0, 1]">
  <Description>
    Since raw counts are unbounded and cause instability, a logical next step is to constrain the range. We can normalize the position indices by dividing by the total sequence length to keep every position between 0 and 1:
  </Description>

  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800 font-mono text-sm">
    Position = index / total_length
  </div>

  <Description>
    This keeps values bounded, but it introduces two new problems.
  </Description>

  <Description>
    **Variable step size:** Models often benefit from learning relative patterns, such as attending to the immediate neighbor. However, dividing by sequence length creates inconsistent steps. In a 10-word sentence, moving one position back changes the value by 0.10. In a 50-word sentence, it changes by only 0.02. The concept of "one token away" corresponds to different numeric values depending on the length of the text.
  </Description>

  <NormalizedStepSize />

  <Description>
    This makes it difficult for the model to generalize simple rules like "look at the previous word."
  </Description>

  <Description>
    **Unstable absolute positions:** In the sentence **"The cat sat"** (length 3), the word "cat" at index 1 gets a value of 0.33. In **"The cat sat on the mat"** (length 6), "cat" is still at index 1 but receives a value of 0.17. The same physical position maps to different values depending on the context window, undermining the ability to learn consistent patterns for specific locations.
  </Description>
</Step>

<Step title="3. What Might Make a Good Positional Encoding?">
  <Description>
    Both approaches so far have failed. Raw counting causes magnitude issues and instability. Normalization keeps values small, but creates inconsistent steps and unstable absolute positions. Before we try again, let's step back and think about what properties we *might* want in a positional encoding. These aren't hard requirements, just reasonable hypotheses about what could make the signal useful for the model.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">1. Scale Compatibility</h4>

  <Description>
    As we learned from Attempt 1, the position signal should live in a reasonable numeric range so it doesn't overwhelm the token embedding. We want the model to represent "Apple at position 5", not a vector dominated by the position index.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">2. Unique and Deterministic</h4>

  <Description>
    Every position within the context window should produce a distinct encoding, and that encoding should be identical every time. Position 5 must always map to the exact same vector, whether during training or inference. If two positions share an encoding, the model has no way to tell them apart.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">3. Consistent Relative Structure</h4>

  <Description>
    A good positional encoding makes relative offsets easy for the model to use. Ideally, "three words ahead" should behave consistently whether you're near the start of a sentence or deep into a long sequence. The relationship between any position P and position P+k should behave consistently regardless of P, allowing the model to learn portable attention patterns.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">4. Generalization to Longer Sequences</h4>

  <Description>
    Ideally, we'd want the model to handle sequences longer than what it saw during training. An encoding that extends naturally to larger positions seems appealing.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">5. Smooth and Continuous</h4>

  <Description>
    Positions are discrete, but it still helps if the positional signal changes in a predictable, structured way as we move from one position to the next. This is because neural networks learn by making small adjustments. If the model's prediction is slightly wrong, it nudges its weights a little bit in the right direction. This works better when small changes in input lead to small changes in output.
  </Description>

  <Description>
    With these properties in mind, the question becomes: how might we represent many positions using only small, bounded values while keeping the signal smooth and consistent?
  </Description>
</Step>

<Step title="4. From Numbers to Waves">
  <Description>
    Number systems offer a key insight: they use multiple columns cycling at different speeds, allowing us to represent arbitrarily large numbers with bounded symbols. For example, in decimal the ones column cycles 0→9, the tens column changes every 10 counts, the hundreds every 100. This multi-rate structure is exactly the kind of bounded, unique encoding we want.
  </Description>

  <Description>
    We already saw scale issues with raw decimal digits in Attempt 1. Even if we use binary, which keeps values at just 0 and 1, we run into the same neighbor problem we saw with 999→1000. In binary, going from 7 (0111) to 8 (1000) flips four bits at once. Adjacent positions can have drastically different representations.
  </Description>

  <Description>
    The core issue behind these different representations is the abrupt flipping between discrete states. Looking at binary, as shown on the left side of the visual below, each dimension jumps instantly from 0 to 1 and back, with no values in between. So, what if we kept the multi-frequency structure but replaced these sharp jumps with smooth waves that rise and fall gradually?
  </Description>

  <BinaryVsSmooth />

  <Description>
    This smooth oscillation preserves the core insight: multiple dimensions changing at different speeds, all staying in a bounded range, giving each position a distinctive signature. Unlike discrete digits, the values change gradually from one position to the next, so nearby positions have similar encodings.
  </Description>
</Step>

<Step title="5. Finding the Function">
  <Description>
    If you have taken trigonometry, you might recognize the shape of our smooth oscillation. It looks like a **sine wave**. Our visualization matches a shifted version (sin(x) + 1) / 2 to keep values between 0 and 1. But the raw sine function naturally oscillates between -1 and +1, which raises a design choice: should we shift the wave to stay positive, or use the raw centered values?
  </Description>

  <Description>
    In practice, the centered range -1 to +1 is the better default. Embedding values are typically initialized around 0, and techniques like **Layer Normalization** (covered later) keep them there throughout the network. A positional signal centered around 0 blends naturally with this distribution, while a 0 to 1 shift would add an unnecessary constant bias to every dimension.
  </Description>

  <Description>
    Now we have the final function, each dimension gets its own sine wave, cycling at a different frequency. Play the animation to see how each dimension evolves across positions:
  </Description>

  <FrequencyWaves />

  <Description>
    Fast waves change quickly, helping separate nearby positions. Slow waves change gradually, helping distinguish distant positions. Together, they create a unique fingerprint for every position within the context window
  </Description>
</Step>

<Step title="6. Making Relative Positions Easy">
  <Description>
    We now have a working positional signal using sine waves at different frequencies, giving each position a unique fingerprint. However, the original Transformer paper actually pairs every sine with a **cosine** at the same frequency. Why might that be?
  </Description>

  <Description>
    One motivation involves how positions **relate to each other**. Language understanding often depends on relative positions: "the word immediately before" or "three tokens ahead." If the encoding makes these relationships easy to compute, it *might* help the model learn them. Ideally, going from position `pos` to position `pos + k` would follow a simple, predictable pattern.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">The Limitation of Sine Alone</h4>

  <Description>
    Suppose we only store the sine value for a given frequency ω. At position `pos`, our encoding is sin(ω · pos). What happens when we shift to position `pos + k`? From high school trigonometry, we can expand sin(ω(pos + k)) using the angle addition formula:
  </Description>

  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800">
    <div className="font-mono text-sm text-center text-primary">
      sin(ω · (pos + k)) = sin(ω · pos) · cos(ω · k) + cos(ω · pos) · sin(ω · k)
    </div>
  </div>

  <Description>
    Look at what's needed on the right side: sin(ω · pos), which we have, and cos(ω · pos), which we don't. The model could learn to approximate the missing cosine, but if we store both values from the start, the entire shift operation becomes a simple matrix multiplication.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">Pairing Sine with Cosine</h4>

  <Description>
    With that reasoning, we store both values together. For each frequency ω, we keep sin(ω · pos) and cos(ω · pos) as a 2D pair:
  </Description>

  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800 text-center">
    <span className="font-mono text-primary">PE(pos) = [ sin(ω · pos), cos(ω · pos) ]</span>
  </div>

  <Description>
    Now we have everything we need. When we want to express the encoding at a shifted position `pos + k`, we can use both angle addition formulas:
  </Description>

  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800 space-y-1">
    <div className="font-mono text-sm text-center text-primary">
      sin(ω(pos + k)) = cos(ω·k) · sin(ω·pos) + sin(ω·k) · cos(ω·pos)
    </div>
    <div className="font-mono text-sm text-center text-primary">
      cos(ω(pos + k)) = cos(ω·k) · cos(ω·pos) - sin(ω·k) · sin(ω·pos)
    </div>
  </div>

  <Description>
    Notice something important: the right side only uses sin(ω·pos) and cos(ω·pos), which we already have stored, multiplied by constants that depend only on the offset `k`. We can write the shifted encoding as a **matrix multiplication**:
  </Description>

  <div className="my-6 p-5 bg-surface rounded-lg border border-border overflow-x-auto">
    <div className="flex items-center justify-center gap-3 font-mono text-sm">
      {/* Target Vector */}
      <div className="flex flex-col gap-1 p-2 border-l-2 border-r-2 border-amber-500/50 rounded shrink-0">
        <span className="text-amber-400 whitespace-nowrap">sin(ω(pos+k))</span>
        <span className="text-amber-400 whitespace-nowrap">cos(ω(pos+k))</span>
      </div>
      
      <span className="text-muted shrink-0">=</span>

      {/* Rotation Matrix */}
      <div className="flex flex-col gap-1 p-2 border-l-2 border-r-2 border-blue-500/50 rounded bg-[#121212] shrink-0">
        <div className="flex gap-4">
          <span className="min-w-[70px] text-center text-blue-400 whitespace-nowrap">cos(ω·k)</span>
          <span className="min-w-[70px] text-center text-blue-400 whitespace-nowrap">sin(ω·k)</span>
        </div>
        <div className="flex gap-4">
          <span className="min-w-[70px] text-center text-blue-400 whitespace-nowrap">-sin(ω·k)</span>
          <span className="min-w-[70px] text-center text-blue-400 whitespace-nowrap">cos(ω·k)</span>
        </div>
      </div>

      <span className="text-muted shrink-0">×</span>

      {/* Current Vector */}
      <div className="flex flex-col gap-1 p-2 border-l-2 border-r-2 border-emerald-500/50 rounded shrink-0">
        <span className="text-emerald-400 whitespace-nowrap">sin(ω·pos)</span>
        <span className="text-emerald-400 whitespace-nowrap">cos(ω·pos)</span>
      </div>
    </div>
  </div>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">The Rotation Matrix</h4>

  <Description>
    The 2×2 matrix in the middle is a **rotation matrix**, the standard matrix that rotates any 2D point by a fixed angle. In our case, that angle is ω · k. Geometrically, the (sin, cos) pair traces out a circle as position increases, and shifting by k positions corresponds to rotating this point around the circle by angle ω · k:
  </Description>

  <RotationVisualization />

  <Description>
    The rotation matrix depends **only on k** (the offset), not on the starting position. Whether you start at position 5, 50, or 500, shifting by k positions applies the same rotation. The relationship between any position and "k steps ahead" is always the same angular shift, regardless of where you are in the sequence.
  </Description>
</Step>


<Step title="7. Building the Formula">
  <Description>
    At this point, you already understand *why* we use multiple frequencies and why sine/cosine pairs help. The original Transformer formula is just a compact way to express these ideas. Let's build it up.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">Combining Position with Meaning</h4>

  <Description>
    Each token starts as an embedding vector of size `d_model` (e.g., 512 dimensions). How do we combine this with position information? Concatenation would work, appending a position vector to the embedding, but it increases the dimensionality that all downstream layers must handle. Addition keeps things simple: if the position vector is also `d_model` dimensions, we add them element-wise and the model learns to work with the combined signal.
  </Description>

  <div className="my-6 p-4 bg-[#121212] rounded-lg border border-zinc-800">
    <div className="text-center font-mono text-lg text-primary">
      input = token_embedding + positional_encoding
    </div>
  </div>

  <Description>
    This choice constrains what follows. Our positional encoding must produce exactly `d_model` numbers. With a sine/cosine pair per frequency taking 2 slots, we can fit `d_model / 2` frequencies. With `d_model = 512`, that's 256 frequencies (i = 0 to 255). One way to arrange them is to interleave: for each frequency `i`, put sine at index `2i` and cosine at index `2i+1`. So frequency 0 uses indices 0 and 1, frequency 1 uses indices 2 and 3, and so on.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">Choosing the Frequencies</h4>

  <Description>
    Each sine/cosine needs an argument of the form `ω · pos`, where `ω` is the frequency and `pos` is the position. We want frequencies spanning from fast (to separate nearby positions) to slow (to distinguish distant ones).
  </Description>

  <Description>
    How slow should the slowest frequency be? Think of a 2-digit counter that wraps from 99 back to 00. A sinusoid has similar wrap-around: for a given ω, the pair (sin(ω · pos), cos(ω · pos)) repeats every 2π/ω. If that wavelength is shorter than your sequence length, that component might reuse the same values for multiple positions. A practical heuristic is to make the slowest wavelength much longer than the longest sequences you expect. With the paper’s 10000 scale, the longest wavelength is on the order of 2π · 10000 (about 6 × 10<sup>4</sup> positions), so it changes very slowly across typical windows and provides a coarse notion of position.
  </Description>

  <Description>
    So we want frequencies ranging from 1 (fastest) down to about 1/10000 (slowest), spread across our d_model/2 slots. We can spread them evenly on a logarithmic scale using:
  </Description>

  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800 font-mono text-sm text-center">
    <span className="text-primary">ω<sub>i</sub> = 1 / 10000<sup>2i/d<sub>model</sub></sup></span>
  </div>

  <Description>
    The exponent `2i/d_model` maps `i` over `[0, d_model/2)` into `[0, 1)`, which spaces the frequencies evenly on a log scale. At `i = 0`, the exponent is 0, so ω = 1 (fastest). At the last sine/cosine pair, the exponent is 1, so ω is 1/10000 (slowest).
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">The Formula</h4>

  <Description>
    Putting it together: for position `pos` and dimension pair `i`, we compute:
  </Description>

  <div className="my-6 p-5 bg-surface rounded-lg border border-border">
    <div className="text-center space-y-2">
      <div className="font-mono text-lg text-primary">
        PE(pos, 2i) = sin(pos / 10000<sup>2i/d<sub>model</sub></sup>)
      </div>
      <div className="font-mono text-lg text-primary">
        PE(pos, 2i+1) = cos(pos / 10000<sup>2i/d<sub>model</sub></sup>)
      </div>
    </div>
  </div>

  <Description>
    Each piece maps directly to a choice we already made: interleaved sin/cos pairs, geometric frequency progression, and a base large enough to avoid repetition within the context window. The formula is just compact notation for these ideas.
  </Description>

</Step>


<Step title="8. Summary">
  <Callout type="success" title="What We Learned">
    * **Raw position indices** cannot be injected directly because their unbounded magnitude (1, 1000, 10000...) drowns out the semantic signal in the embedding
    * **Multiple dimensions cycling at different speeds** allow large positions to be encoded with small, bounded values—similar to how decimal uses ones, tens, and hundreds columns
    * **Sinusoidal waves** provide smooth, continuous values that change gradually from one position to the next while staying bounded between -1 and +1
    * **Sine/cosine pairs** at each frequency enable position shifts to be computed as a simple rotation matrix, making relative positions ("3 words ahead") consistent regardless of absolute position
    * **Frequency spread** from fast to slow gives each position a unique fingerprint: fast frequencies separate nearby positions, while slow frequencies distinguish distant ones
    * **Addition** combines position and meaning into a single vector without increasing dimensionality, letting the model learn from both signals together
  </Callout>

  <Description>
    This isn't the only solution. Learned positional embeddings work well, and modern architectures take different approaches. With both meaning and position now encoded, we have the complete input to the Transformer. In the next chapter, we'll see how it uses these vectors to let words **attend** to each other.
  </Description>
</Step>
