---
title: "Positional Encoding"
step: 4
description: "How to encode sequence order into token embeddings."
---

<div className="flex flex-col" style={{ gap: 'var(--space-flow)' }}>
<Description noMargin>
  The previous chapter ended with a limitation: embeddings capture what tokens mean, but not where they appear. Since Transformers process all tokens in parallel, "Alice helped Bob" and "Bob helped Alice" produce identical embedding sets with no indication of order. We need a way to encode position.
</Description>

<Description>
  This chapter explores one line of reasoning that leads to the original Transformer's solution. We'll start with naive approaches, see why they fail, and build toward the sinusoidal encoding that the "Attention Is All You Need" paper introduced.
</Description>
</div>



<Step title="1. Attempt 1: Counting Up">
  <Description>
    The most naive approach is to number the positions (1, 2, 3...) and inject that number into the embedding. There are two ways we might do this:
  </Description>

  <Description>
    **Adding the index to every dimension** is the simplest method. If "Apple" appears at position 1000, the addition looks like this:
  </Description>

  <div className="my-4">
    <div className="p-4 bg-[#121212] rounded-lg border border-zinc-800 font-mono text-sm">
      <div className="flex items-center gap-4">
        <span className="text-muted w-28 shrink-0">E("Apple")</span>
        <span className="text-primary">[0.05, -0.02, 0.11]</span>
        <span className="text-zinc-500 text-xs">// initialized small, zero-centered, usually N(0,1)</span>
      </div>
      <div className="flex items-center gap-4 mt-1">
        <span className="text-muted w-28 shrink-0">+ Position 1000</span>
        <span className="text-secondary">[1000, 1000, 1000]</span>
      </div>
      <div className="border-t border-zinc-700 mt-2 pt-2 flex items-center gap-4">
        <span className="text-muted w-28 shrink-0">= Result</span>
        <span className="text-amber-400 font-bold">[1000.05, 999.98, 1000.11]</span>
      </div>
    </div>
  </div>

  <Description>
    Look at the result: the small values that distinguished "Apple" (0.05, -0.02, 0.11) have become invisible. The output is essentially `[1000, 1000, 1000]` regardless of the original word. The semantic information is drowned by the position signal.
  </Description>

  <Description>
    This magnitude imbalance also causes training instability and makes it difficult for the model to learn meaningful word representations. The problem only gets worse as sequences get longer, since position values grow without bound.
  </Description>

  <Description>
    **Treating dimensions as digit slots** is an alternative. Position 999 becomes `[..., 0, 9, 9, 9]` and position 1000 becomes `[..., 1, 0, 0, 0]`, with leading zeros padding to a fixed width. This is more stable than the first approach since values stay bounded between 0 and 9, but still deviates from the small, zero-centered inputs that weight initialization assumes, risking instability.
  </Description>

  <Description>
    Beyond stability, this encoding is sparse. Most dimensions are zero, wasting vector capacity. And adjacent positions like 999 and 1000 produce completely different vectors, giving the model no hint that they are neighbors.
  </Description>

</Step>

<Step title="2. Attempt 2: Scaling to [0, 1]">
  <Description>
    Since raw counts are unbounded and cause instability, a logical next step is to constrain the range. We can normalize the position indices by dividing by the total sequence length to keep every position between 0 and 1:
  </Description>

  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800 font-mono text-sm">
    Position = index / total_length
  </div>

  <Description>
    This keeps values bounded, but it introduces two new problems.
  </Description>

  <Description>
    **Variable step size:** Models often benefit from learning relative patterns, such as attending to the immediate neighbor. However, dividing by sequence length creates inconsistent steps. In a 10-word sentence, moving one position back changes the value by 0.10. In a 50-word sentence, it changes by only 0.02. The concept of "one token away" corresponds to different numeric values depending on the length of the text, making it difficult for the model to generalize simple rules like "look at the previous word."
  </Description>

  <Description>
    **Unstable absolute positions:** In the sentence **"The cat sat"** (length 3), the word "cat" at index 1 gets a value of 0.33. In **"The cat sat on the mat"** (length 6), "cat" is still at index 1 but receives a value of 0.17. The same physical position maps to different values depending on the context window, undermining the ability to learn consistent patterns for specific locations.
  </Description>
</Step>

<Step title="3. What Might Make a Good Positional Encoding?">
  <Description>
    Both approaches so far have failed. Raw counting causes magnitude issues and instability. Normalization keeps values small, but creates inconsistent steps and unstable absolute positions. Before we try again, let's step back and think about what properties we *might* want in a positional encoding. These aren't hard requirements, just reasonable hypotheses about what could make the signal useful for the model.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">1. Scale Compatibility</h4>

  <Description>
    As we learned from Attempt 1, the position signal should live in a reasonable numeric range so it doesn't overwhelm the token embedding. We want the model to represent "Apple at position 5", not a vector dominated by the position index.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">2. Unique and Deterministic</h4>

  <Description>
    Every position within the context window should produce a distinct encoding, and that encoding should be identical every time. Position 5 must always map to the exact same vector, whether during training or inference. If two positions share an encoding, the model has no way to tell them apart.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">3. Consistent Relative Structure</h4>

  <Description>
    A good positional encoding makes relative offsets easy for the model to use. Ideally, "three words ahead" should behave consistently whether you're near the start of a sentence or deep into a long sequence. The relationship between any position P and position P+k should behave consistently regardless of P, allowing the model to learn portable attention patterns.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">4. Generalization to Longer Sequences</h4>

  <Description>
    Ideally, we'd want the model to handle sequences longer than what it saw during training. An encoding that extends naturally to larger positions seems appealing.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">5. Smooth and Continuous</h4>

  <Description>
    Positions are discrete, but it still helps if the positional signal changes in a predictable, structured way as we move from one position to the next. This is because neural networks learn by making small adjustments. If the model's prediction is slightly wrong, it nudges its weights a little bit in the right direction. This works better when small changes in input lead to small changes in output.
  </Description>

  <Description>
    With these properties in mind, the question becomes: how might we represent many positions using only small, bounded values while keeping the signal smooth and consistent?
  </Description>
</Step>

<Step title="4. From Numbers to Waves">
  <Description>
    Number systems offer a key insight: they use multiple columns cycling at different speeds, allowing us to represent arbitrarily large numbers with bounded symbols. For example, in decimal the ones column cycles 0→9, the tens column changes every 10 counts, the hundreds every 100. This multi-rate structure is exactly the kind of bounded, unique encoding we want.
  </Description>

  <Description>
    We already saw scale issues with raw decimal digits in Attempt 1. Even if we use binary, which keeps values at just 0 and 1, we run into the same neighbor problem we saw with 999→1000. In binary, going from 7 (0111) to 8 (1000) flips four bits at once. Adjacent positions can have drastically different representations.
  </Description>

  <Description>
    The core issue behind these different representations is the abrupt flipping between discrete states. Looking at binary, as shown on the left side of the visual below, each dimension jumps instantly from 0 to 1 and back, with no values in between. So, what if we kept the multi-frequency structure but replaced these sharp jumps with smooth waves that rise and fall gradually?
  </Description>

  <BinaryVsSmooth />

  <Description>
    This smooth oscillation preserves the core insight: multiple dimensions changing at different speeds, all staying in a bounded range, giving each position a distinctive signature. Unlike discrete digits, the values change gradually from one position to the next, so nearby positions have similar encodings.
  </Description>
</Step>

<Step title="5. Finding the Function">
  <Description>
    If you have taken trigonometry, the most natural function to model this smooth, bounded oscillation is the **sine wave**. The visualization above uses a shifted version (sin(x) + 1) / 2 to keep values between 0 and 1.
  </Description>

  <Description>
    In practice, the normal sine with its centered range [-1, +1] is usually preferred. Embedding values are typically initialized around 0, and techniques like **Layer Normalization (covered later)** keep them there throughout the network. A zero-centered positional signal blends naturally with this distribution, whereas a [0, 1] shift would add an unnecessary constant positive bias to every dimension.
  </Description>

  <Description>
    Applying this to our multi-frequency idea, each dimension gets its own sine wave cycling at a different frequency. Play the animation to see how each dimension evolves across positions:
  </Description>

  <FrequencyWaves />

  <Description>
    Fast waves change quickly, helping separate nearby positions. Slow waves change gradually, helping distinguish distant positions. Together, they create a unique fingerprint for every position within the context window.
  </Description>
</Step>

<Step title="6. Making Relative Positions Easy">
  <Description>
    We now have a working positional signal using sine waves at different frequencies, giving each position a unique fingerprint. However, the original Transformer paper actually pairs every sine with a **cosine** at the same frequency. Why might that be?
  </Description>

  <Description>
    One motivation involves how positions **relate to each other**. Language understanding often depends on relative positions: "the word immediately before" or "three tokens ahead." If the encoding makes these relationships easy to compute, it *might* help the model learn them. Ideally, going from position `pos` to position `pos + k` would follow a simple, predictable pattern.
  </Description>

  <Description>
    **What if we only stored sine?** Suppose our encoding is just sin(ω · pos) for a given frequency. What happens when we shift to position `pos + k`? From high school trigonometry, we can expand sin(ω(pos + k)) using the angle addition formula:
  </Description>

  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800">
    <div className="font-mono text-sm text-center text-primary">
      sin(ω · (pos + k)) = sin(ω · pos) · cos(ω · k) + cos(ω · pos) · sin(ω · k)
    </div>
  </div>

  <Description>
    Look at what's needed on the right side: sin(ω · pos), which we have, and cos(ω · pos), which we don't. The model could learn to approximate the missing cosine internally, but we can make its job easier by storing both values from the start.
  </Description>

  <Description>
    **Pairing sine with cosine** for each frequency ω, we store both sin(ω · pos) and cos(ω · pos) as a 2D pair:
  </Description>

  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800 text-center">
    <span className="font-mono text-primary">PE(pos) = [ sin(ω · pos), cos(ω · pos) ]</span>
  </div>

  <Description>
    With cosine now available, here's how the shift looks for both components:
  </Description>

  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800 space-y-2">
    <div className="font-mono text-sm text-center">
      <span className="text-amber-400">sin(ω(pos + k))</span>
      <span className="text-muted"> = </span>
      <span className="text-blue-400">cos(ω·k)</span>
      <span className="text-muted"> · </span>
      <span className="text-emerald-400">sin(ω·pos)</span>
      <span className="text-muted"> + </span>
      <span className="text-blue-400">sin(ω·k)</span>
      <span className="text-muted"> · </span>
      <span className="text-emerald-400">cos(ω·pos)</span>
    </div>
    <div className="font-mono text-sm text-center">
      <span className="text-amber-400">cos(ω(pos + k))</span>
      <span className="text-muted"> = </span>
      <span className="text-blue-400">cos(ω·k)</span>
      <span className="text-muted"> · </span>
      <span className="text-emerald-400">cos(ω·pos)</span>
      <span className="text-muted"> - </span>
      <span className="text-blue-400">sin(ω·k)</span>
      <span className="text-muted"> · </span>
      <span className="text-emerald-400">sin(ω·pos)</span>
    </div>
  </div>

  <Description>
    Notice something important: the right side only uses sin(ω·pos) and cos(ω·pos), which we already have stored, multiplied by constants that depend only on the offset `k`. We can write the shifted encoding as a **matrix multiplication**:
  </Description>

  <div className="my-6 p-5 bg-surface rounded-lg border border-border overflow-x-auto">
    <div className="flex items-center justify-center gap-3 font-mono text-sm">
      {/* Target Vector */}
      <div className="flex flex-col gap-1 p-2 border-l-2 border-r-2 border-amber-500/50 rounded shrink-0">
        <span className="text-amber-400 whitespace-nowrap">sin(ω(pos+k))</span>
        <span className="text-amber-400 whitespace-nowrap">cos(ω(pos+k))</span>
      </div>
      
      <span className="text-muted shrink-0">=</span>

      {/* Rotation Matrix */}
      <div className="flex flex-col gap-1 p-2 border-l-2 border-r-2 border-blue-500/50 rounded bg-[#121212] shrink-0">
        <div className="flex gap-4">
          <span className="min-w-[70px] text-center text-blue-400 whitespace-nowrap">cos(ω·k)</span>
          <span className="min-w-[70px] text-center text-blue-400 whitespace-nowrap">sin(ω·k)</span>
        </div>
        <div className="flex gap-4">
          <span className="min-w-[70px] text-center text-blue-400 whitespace-nowrap">-sin(ω·k)</span>
          <span className="min-w-[70px] text-center text-blue-400 whitespace-nowrap">cos(ω·k)</span>
        </div>
      </div>

      <span className="text-muted shrink-0">×</span>

      {/* Current Vector */}
      <div className="flex flex-col gap-1 p-2 border-l-2 border-r-2 border-emerald-500/50 rounded shrink-0">
        <span className="text-emerald-400 whitespace-nowrap">sin(ω·pos)</span>
        <span className="text-emerald-400 whitespace-nowrap">cos(ω·pos)</span>
      </div>
    </div>
  </div>

  <Description>
    **The rotation matrix:** The 2×2 matrix in the middle is a rotation matrix, the standard matrix that rotates any 2D point by a fixed angle. In our case, that angle is ω · k. Geometrically, the (sin, cos) pair traces out a circle as position increases, and shifting by k positions corresponds to rotating this point around the circle by angle ω · k:
  </Description>

  <RotationVisualization />

  <Description>
    Since the rotation matrix depends only on k (the offset), "k steps ahead" applies the same linear transformation regardless of starting position. Transformers are built on matrix multiplication, so expressing position shifts this way could help the model learn consistent relative position patterns.
  </Description>
</Step>


<Step title="7. Building the Formula">
  <Description>
    We now have all the design choices in place. In this section, we derive the final positional encoding formula. First, we need to decide how to combine the position vector with the token embedding (size `d_model`). There are two main approaches:
  </Description>

  <Description>
    **Concatenation** would append the position vector to the embedding. If the embedding is `d_model` and the position vector is `d_pos`, the result would be `d_model + d_pos` dimensions. This keeps signals separate but increases input size, growing parameter counts in all downstream layers and breaking residual connections (covered later) which require matching dimensions.
  </Description>

  <Description>
    **Addition** adds them element-wise, requiring both vectors to be the same size. This preserves the original dimensionality, keeping the architecture efficient and compatible with residual connections. Although it mixes position and meaning into one vector, neural networks have sufficient capacity to disentangle these signals during training, which is why this is the standard choice (Vaswani et al., 2017).
  </Description>

  <Description>
    Choosing addition means our positional encoding must produce exactly `d_model` numbers per position. Each sine/cosine pair occupies 2 dimensions, so we can fit `d_model / 2` pairs total. With `d_model = 512`, that gives us 256 pairs, indexed from 0 to 255. A simple arrangement is interleaving: the `i`-th pair puts its sine at dimension `2i` and its cosine at dimension `2i + 1`. For example, pair 0 uses dimensions 0 and 1, pair 1 uses dimensions 2 and 3, all the way up to pair 255 using dimensions 510 and 511.
  </Description>

  <Description>
    Now we need to choose the actual frequencies. We need a fastest frequency to distinguish neighboring positions (like the ones digit in decimal distinguishes consecutive numbers), and a slowest frequency that doesn't repeat within our context window.
  </Description>

  <Description>
    For the fastest frequency, `ω = 1` cycles quickly, completing a full wave every ~6 positions (`wavelength = 2π/ω = 2π/1 ≈ 6.28`), giving fine-grained resolution near each position. For the slowest frequency, we want its wavelength to span the entire context window. Just as two decimal digits can only count to 99 before wrapping, our slowest wave should complete less than one full cycle within our target sequence length. Let's call this wavelength parameter **base**.
  </Description>

  <Description>
    With our endpoints set (`ω = 1` at the fast end, `ω = 1/base` at the slow end), we need to fill the frequencies in between. Number systems do this naturally: in binary, each column cycles 2× slower than the previous; in decimal, 10× slower. We adopt the same approach, computing the ratio that steps us smoothly from 1 down to `1/base` across our `d_model / 2` frequency pairs. This gives us evenly spaced coverage across all scales.
  </Description>

  <Description>
    The formula `ωᵢ = 1/base^(2i/d_model)` achieves exactly what we need. The exponent `2i/d_model` slides from 0 at the first pair (giving `ω = 1`) toward 1 at the last pair (giving `ω ≈ 1/base`), placing all intermediate frequencies on a smooth geometric curve. The original Transformer uses `base = 10000`, which yields a slowest wavelength of about 63,000 positions, well beyond typical context windows at the time. Any base large enough to cover your target sequence length works. The value 10000 was simply a safe default. Putting it all together:
  </Description>

  <div className="my-6 p-5 bg-surface rounded-lg border border-border">
    <div className="text-center space-y-3">
      <div className="font-mono text-lg text-primary">
        PE<sub>i</sub>(pos) = [ sin(pos / 10000<sup>2i/d<sub>model</sub></sup>), cos(pos / 10000<sup>2i/d<sub>model</sub></sup>) ]
      </div>
      <div className="text-sm text-muted">
        where <span className="font-mono">i ∈ [0, d<sub>model</sub>/2)</span> and the i-th pair fills dimensions <span className="font-mono">2i</span> and <span className="font-mono">2i+1</span>
      </div>
    </div>
  </div>

  <Description>
    That's the complete formula. Every piece traces back to a design choice we made along the way: sine/cosine pairs for rotation-friendly relative positions, multiple frequencies for scale separation, and addition to preserve the architecture's dimensionality.
  </Description>

  <Callout type="success" title="Summary">
    * **Raw position indices** don't work well because their unbounded magnitude (1, 1000, 10000...) can overwhelm the semantic signal in the embedding
    * **Multiple dimensions cycling at different speeds** allow large positions to be encoded with small, bounded values—similar to how decimal uses ones, tens, and hundreds columns
    * **Sinusoidal waves** provide smooth, continuous values that change gradually from one position to the next while staying bounded between -1 and +1
    * **Sine/cosine pairs** enable position shifts to be expressed as linear transformations (rotation matrices), playing to the Transformer's strength in matrix operations
    * **Frequency spread** from fast (1) to slow (1/10000) gives each position a unique fingerprint across multiple scales
    * **Addition** combines position and meaning while preserving `d_model` dimensionality, keeping the architecture efficient and compatible with residual connections
  </Callout>

  <Description>
    This isn't the only line of reasoning to arrive at positional encoding, nor the only solution. With this in place, we have the complete input representation. In the next chapter, we'll see how the Transformer uses these vectors to let words **attend** to each other.
  </Description>

</Step>
