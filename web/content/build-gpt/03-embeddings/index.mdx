---
title: "The Vector Space"
step: 3
description: "How to represent tokens as vectors to capture semantic meaning."
---

<div className="flex flex-col" style={{ gap: 'var(--space-flow)' }}>
<Description noMargin>
  In the previous chapter, we built a Tokenizer that converts text into integers. A sentence like "I love cats" becomes something like `[40, 1842, 9246]`. Since computers work with numbers, it might seem like we are ready to feed these integers directly into a neural network.
</Description>

<Description noMargin>
  But raw token IDs have some fundamental issues. For one, neural networks assume that the size of a number means something. If one word has an ID 10√ó larger than another, the network will mathematically treat it as "10√ó more" of something, even though these IDs are just arbitrary labels. Beyond that, a single number cannot capture the complexity of what a word means. Think about the word "Queen": to truly understand it, the model needs to know about gender, status, grammar, and more. There is no way a single integer like 3878 can hold all of that. Let's look at these issues more closely.
</Description>
</div>


<Step title="1. The Limits of Token IDs">
  <Description>
    Let's start with the magnitude problem. When a neuron processes input, it multiplies by a weight: `Output = Input √ó Weight`. If "red" has ID 87 and "yellow" has ID 892, then "yellow" produces about 10√ó more activation than "red" before the model has learned anything at all. The network is forced to treat certain words as inherently "bigger" than others, purely because of how IDs happened to be assigned. This is called **magnitude bias**, and it corrupts the learning signal from the very first training step.
  </Description>

  <Description>
    One common workaround is **one-hot encoding**. Instead of using a single integer, each token becomes a long vector of zeros with a single 1 at its index position. If our vocabulary has 50,000 tokens, then each word becomes a vector of 50,000 numbers, with exactly one of them set to 1. This eliminates magnitude bias because every token now has exactly the same "intensity." But it still does not solve the second problem: a one-hot vector gives us no room to encode meaning. "King" and "Queen" are orthogonal vectors, mathematically just as different from each other as "King" is from "Banana." Every word is equally distant from every other word, so we don't have any sense of similarity or semantic meaning.
  </Description>

  <Description>
    What we really need is a way to encode semantic meaning directly into how we represent words. Think about how you naturally make sense of words: you know that "King" and "Queen" are related, that "red" and "blue" belong to the same category, and that "Paris" connects to "France" the way "Tokyo" connects to "Japan." The representation should let us capture these kinds of relationships and operate on them, the way we do. Neither a raw integer nor a sparse one-hot vector can support this. We need something richer.
  </Description>
</Step>


<Step title="2. Representing Meaning with Multiple Numbers">
  <Description>
    What if instead of one number, we used a list of numbers to represent each word? Each number in the list could capture a different attribute of the word's meaning. Let's see how this would work.
  </Description>

  <Description>
    To make this concrete, let's design a simple system with just two attributes: **Royalty** and **Gender**. We will score each word on both of these dimensions, using a scale from **-1.0** to **+1.0**. For Royalty, +1 means strongly royal, -1 means strongly opposite, and 0 means unrelated. For Gender, +1 means masculine, -1 means feminine, and 0 means neutral.
  </Description>

  <Description>
    With these two dimensions, we can now represent words as pairs of numbers. Let's see how a few example words would look:
  </Description>

  <Table>
    <TableHead>
      <TableRow>
        <TableHeader>Word</TableHeader>
        <TableHeader>Royalty</TableHeader>
        <TableHeader>Gender</TableHeader>
        <TableHeader>Vector</TableHeader>
      </TableRow>
    </TableHead>
    <TableBody>
      <TableRow>
        <TableCell className="text-primary">ü§¥ King</TableCell>
        <TableCell className="text-emerald-400 font-mono">1.0</TableCell>
        <TableCell className="text-blue-400 font-mono">1.0</TableCell>
        <TableCell className="font-mono">[1.0, 1.0]</TableCell>
      </TableRow>
      <TableRow>
        <TableCell className="text-primary">üë∏ Queen</TableCell>
        <TableCell className="text-emerald-400 font-mono">1.0</TableCell>
        <TableCell className="text-pink-400 font-mono">-1.0</TableCell>
        <TableCell className="font-mono">[1.0, -1.0]</TableCell>
      </TableRow>
      <TableRow>
        <TableCell className="text-primary">üßî Man</TableCell>
        <TableCell className="text-muted font-mono">0.0</TableCell>
        <TableCell className="text-blue-400 font-mono">1.0</TableCell>
        <TableCell className="font-mono">[0.0, 1.0]</TableCell>
      </TableRow>
      <TableRow>
        <TableCell className="text-primary">üçé Apple</TableCell>
        <TableCell className="text-muted font-mono">0.0</TableCell>
        <TableCell className="text-muted font-mono">0.0</TableCell>
        <TableCell className="font-mono">[0.0, 0.0]</TableCell>
      </TableRow>
    </TableBody>
  </Table>

  <Description>
    By representing each word as a vector, we have encoded meaning directly into the numbers. King and Queen share the same Royalty score. King and Man share the same Gender score. And Apple sits at zero for both, since royalty and gender do not really apply to fruit.
  </Description>
</Step>


<Step title="3. Visualizing Word Vectors">
  <Description>
    Since we used only two dimensions in our example, we can plot these words on a standard X-Y coordinates with Royalty on the vertical axis and Gender on the horizontal axis.
  </Description>

  <EmbeddingSpace />

  <Description>
    Every word now sits at a specific coordinate in this 2D space. The space organizes words by meaning. Words with similar concepts group into specific regions. You can see King and Queen sit at the top, while King and Man sit on the right. Apple sits at the origin because neither dimension applies to it.
  </Description>

  <Description>
    Because each dimension represents an attribute, the direction you travel from one word to another captures their relationship. Notice the arrows in the graph below.
  </Description>

  <EmbeddingSpace showArrows />

  <Description>
    The arrow from King to Queen represents "flip gender while keeping royalty the same." The arrow from Man to Woman represents the same transformation. Both point the same direction because both represent the same concept.
  </Description>

  <Description attached>
    This geometry allows us to do arithmetic on meaning. If we take the concept of **King**, subtract the **Man** component, and add **Woman**, we should logically discover the female equivalent of royalty. Let's test this with our vectors:
  </Description>

  <div className="content-attached p-4 bg-surface rounded-lg border border-border">
    <div className="space-y-3 font-mono text-sm">
      <div className="flex items-center gap-3">
        <span className="text-muted w-28">King:</span>
        <span className="text-primary">[1.0, 1.0]</span>
      </div>
      <div className="flex items-center gap-3">
        <span className="text-muted w-28">Minus Man:</span>
        <span className="text-red-400">-[0.0, 1.0]</span>
      </div>
      <div className="flex items-center gap-3">
        <span className="text-muted w-28">Plus Woman:</span>
        <span className="text-emerald-400">+[0.0, -1.0]</span>
      </div>
      <div className="pt-3 border-t border-border flex items-center gap-3">
        <span className="text-muted w-28">Result:</span>
        <span className="text-emerald-400 font-bold text-lg">[1.0, -1.0]</span>
      </div>
    </div>
  </div>

  <Description>
    The result is exactly the vector for **Queen**. By taking King, removing the "maleness," and adding "femaleness," we mechanically arrived at Queen. This confirms that by expanding from a single number to a vector, we have successfully captured two things: the **semantic meaning** of each word, and the **logical relationships** between them, allowing us to manipulate relationships using simple math.
  </Description>
</Step>


<Step title="4. The Embedding Layer">
  <Description>
    The vectors we have been building are called **embeddings** in machine learning, and the component that stores and retrieves them is called the **embedding layer**. In the previous sections, we hand-picked two dimensions (Royalty and Gender) and manually assigned values to each word. In practice, we only decide how many dimensions to use and initialize all values randomly. The model then learns both what each dimension represents and the specific values for every word in the vocabulary during training.
  </Description>

  <Description>
    The embedding layer is implemented as a simple 2D array where each row stores the vector for one token. If our vocabulary has $V$ tokens and we choose $d_{model}$ dimensions, the array has shape `[V, d_model]`. To convert a token ID into its embedding, the layer simply looks up that row in the table.
  </Description>

  <Table>
    <TableHead>
      <TableRow>
        <TableHeader>Token ID</TableHeader>
        <TableHeader>Dim 1</TableHeader>
        <TableHeader>Dim 2</TableHeader>
        <TableHeader>Dim 3</TableHeader>
        <TableHeader>...</TableHeader>
        <TableHeader>Dim $d_{model}$</TableHeader>
      </TableRow>
    </TableHead>
    <TableBody>
      <TableRow>
        <TableCell>0 ("the")</TableCell>
        <TableCell className="font-mono text-primary">0.12</TableCell>
        <TableCell className="font-mono text-primary">-0.45</TableCell>
        <TableCell className="font-mono text-primary">0.78</TableCell>
        <TableCell className="text-muted">...</TableCell>
        <TableCell className="font-mono text-primary">0.33</TableCell>
      </TableRow>
      <TableRow>
        <TableCell>1 ("King")</TableCell>
        <TableCell className="font-mono text-primary">0.89</TableCell>
        <TableCell className="font-mono text-primary">0.56</TableCell>
        <TableCell className="font-mono text-primary">-0.21</TableCell>
        <TableCell className="text-muted">...</TableCell>
        <TableCell className="font-mono text-primary">0.67</TableCell>
      </TableRow>
      <TableRow>
        <TableCell className="text-muted">...</TableCell>
        <TableCell className="text-muted">...</TableCell>
        <TableCell className="text-muted">...</TableCell>
        <TableCell className="text-muted">...</TableCell>
        <TableCell className="text-muted">...</TableCell>
        <TableCell className="text-muted">...</TableCell>
      </TableRow>
    </TableBody>
  </Table>

  <Description>
    The two numbers that define the embedding layer are the **vocabulary size** $V$ (number of rows) and the **embedding dimension** $d_{model}$ (number of columns). A larger vocabulary lets more words exist as single tokens, so sentences become shorter sequences, but it also adds more rows to the table and increases memory. A larger embedding dimension gives the model more attributes to describe each word, but it adds more columns to every row and increases both memory and computation. In practice, vocabulary sizes range from around 32,000 to 100,000 tokens, while embedding dimensions range from a few hundred to several thousand depending on the model's scale.
  </Description>

  <Description>
    When training begins, every value in this table is initialized with small random numbers. "King" might start near "Sandwich" purely by chance. The structure we saw earlier does not exist yet, but it emerges as the model trains.
  </Description>

  <Description>
    During training, the model reads billions of sentences and tries to predict the next word. When it gets a prediction wrong, the error signal flows backward through the network and nudges the embedding values. If "King" often appears near "throne," "crown," and "palace," their vectors gradually move closer together. If "apple" appears near "orange" and "banana," those cluster together instead.
  </Description>

  <Description>
    After enough examples, the random numbers transform into the kind of organized space we visualized earlier. Unlike our hand-picked dimensions, the learned dimensions are abstract and often uninterpretable, but they capture whatever structure helps the model predict well.
  </Description>
</Step>


<Step title="6. Why Word Order Gets Lost">
  <Description>
    We have solved the meaning problem. We can translate "King" into a rich vector that captures its essence.
  </Description>

  <Description>
    Here's the issue. Traditional language models (like RNNs) read words one by one, left to right, so order is built in. But this sequential approach is painfully slow. You cannot process word #5 until you have finished words #1 through #4. This makes training on billions of sentences take forever.
  </Description>

  <Description>
    Transformer architectures (used in modern LLMs) solve the speed problem by processing all words **in parallel**, reading the entire sentence at once. This is massively faster and more scalable. But it creates a new problem: if you hand the model all words simultaneously, how does it know which came first?
  </Description>

  <Description>
    Think back to our preprocessing pipeline: Text ‚Üí Bytes ‚Üí Tokens ‚Üí Vectors. At no point did we encode **where** each word appears. The token ID for "Alice" is the same whether she appears first, third, or last in a sentence. And the embedding vector we just learned to look up? It only captures **what** the word means, not **where** it sits.
  </Description>

  <Description>
    This becomes a problem when words need to interact with each other. In a Transformer, every word looks at every other word simultaneously to build understanding. Let's visualize this with the sentence "Alice gave Bob a book":
  </Description>

  <Description>
    In the grid below, `compare(wordA, wordB)` is a placeholder for the actual mechanism (which we'll cover in a later chapter). For now, just think of it as "wordA examines wordB."
  </Description>

  <div className="my-6 overflow-x-auto">
    <table className="w-full text-sm border-collapse">
      <thead>
        <tr>
          <th className="p-2 bg-surface border border-border"></th>
          <th className="p-2 bg-blue-50 dark:bg-blue-900/30 border border-border font-mono text-blue-700 dark:text-blue-300">Alice</th>
          <th className="p-2 bg-emerald-500/10 border border-border font-mono text-emerald-400">gave</th>
          <th className="p-2 bg-amber-50 dark:bg-amber-900/30 border border-border font-mono text-amber-700 dark:text-amber-300">Bob</th>
          <th className="p-2 bg-surface border border-border font-mono text-secondary">a</th>
          <th className="p-2 bg-purple-50 dark:bg-purple-900/30 border border-border font-mono text-purple-700 dark:text-purple-300">book</th>
        </tr>
      </thead>
      <tbody className="text-xs">
        <tr>
          <td className="p-2 bg-blue-50 dark:bg-blue-900/30 border border-border font-mono font-medium text-blue-700 dark:text-blue-300">Alice</td>
          <td className="p-2 bg-surface border border-border text-center text-muted">‚Äî</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(Alice, gave)</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(Alice, Bob)</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(Alice, a)</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(Alice, book)</td>
        </tr>
        <tr>
          <td className="p-2 bg-emerald-500/10 border border-border font-mono font-medium text-emerald-400">gave</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(gave, Alice)</td>
          <td className="p-2 bg-surface border border-border text-center text-muted">‚Äî</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(gave, Bob)</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(gave, a)</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(gave, book)</td>
        </tr>
        <tr>
          <td className="p-2 bg-amber-50 dark:bg-amber-900/30 border border-border font-mono font-medium text-amber-700 dark:text-amber-300">Bob</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(Bob, Alice)</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(Bob, gave)</td>
          <td className="p-2 bg-surface border border-border text-center text-muted">‚Äî</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(Bob, a)</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(Bob, book)</td>
        </tr>
        <tr>
          <td className="p-2 bg-surface border border-border font-mono font-medium text-secondary">a</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(a, Alice)</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(a, gave)</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(a, Bob)</td>
          <td className="p-2 bg-surface border border-border text-center text-muted">‚Äî</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(a, book)</td>
        </tr>
        <tr>
          <td className="p-2 bg-purple-50 dark:bg-purple-900/30 border border-border font-mono font-medium text-purple-700 dark:text-purple-300">book</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(book, Alice)</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(book, gave)</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(book, Bob)</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(book, a)</td>
          <td className="p-2 bg-surface border border-border text-center text-muted">‚Äî</td>
        </tr>
      </tbody>
    </table>
    <div className="text-xs text-muted mt-2 text-center">
      Each cell shows one word examining another. All comparisons happen simultaneously.
    </div>
  </div>

  <Description>
    Here's the problem: `compare(Alice, Bob)` and `compare(Bob, Alice)` use the **exact same vectors**. The function only sees two meaning-vectors. It has no idea that in "Alice gave Bob," Alice comes before the verb (making her the giver) while Bob comes after (making him the receiver).
  </Description>

  <Description>
    Swap the sentence to "Bob gave Alice a book" and every `compare()` call produces identical results. The model cannot distinguish the giver from the receiver because position was never encoded.
  </Description>

  <Description>
    We need a way to stamp each vector with its position. That's exactly what we'll tackle in the next chapter.
  </Description>

  <Callout type="success" title="Summary">
    * **Raw Token IDs** cannot be used directly because the model would treat ID 500 as "5√ó more" than ID 100, which is meaningless
    * **Embeddings** are vectors (lists of numbers) that encode meaning. Similar words cluster together in vector space
    * The **Embedding Matrix** is learned during training: the model gradually nudges coordinates based on context
    * **Vector arithmetic** works on concepts: King ‚àí Man + Woman ‚âà Queen
    * **Parallel processing** is fast and scalable but loses word order, and we've identified this as a critical missing piece
  </Callout>

  <Description>
    We can now represent **what** each token means, but not **where** it appears. "Alice gave Bob" and "Bob gave Alice" produce identical embeddings despite meaning opposite things. In the next chapter, we'll tackle **Positional Encoding** to solve this.
  </Description>
</Step>
