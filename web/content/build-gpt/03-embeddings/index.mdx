---
title: "The Vector Space"
step: 3
description: "How to represent tokens as vectors to capture semantic meaning."
---

<div className="flex flex-col" style={{ gap: 'var(--space-flow)' }}>
<Description noMargin>
  In the previous chapter, we built a tokenizer that converts text into integers. A sentence like "I love cats" becomes `[40, 1842, 11875]`. These token IDs are just identifiers, they don't capture what words mean or how they relate to each other. This chapter introduces **embeddings**, vectors that can express meaning.
</Description>
</div>


<Step title="1. The Limits of Token IDs">
  <Description>
    Let's start with the magnitude problem. When a neuron processes input, it multiplies by a weight: `Output = Input √ó Weight`. If "red" has ID 87 and "yellow" has ID 892, then "yellow" produces about 10√ó more activation than "red" before the model has learned anything at all. The network is forced to treat certain words as inherently "bigger" than others, purely because of how IDs happened to be assigned. This is called **magnitude bias**, and it corrupts the learning signal from the very first training step.
  </Description>

  <Description>
    One common workaround is **one-hot encoding**. Instead of using a single integer, each token becomes a long vector of zeros with a single 1 at its index position. If our vocabulary has 50,000 tokens, then each word becomes a vector of 50,000 numbers, with exactly one of them set to 1. This eliminates magnitude bias because every token now has exactly the same "intensity." But it still does not solve the second problem: a one-hot vector gives us no room to encode meaning. "King" and "Queen" are orthogonal vectors, mathematically just as different from each other as "King" is from "Banana." Every word is equally distant from every other word, so we don't have any sense of similarity or semantic meaning.
  </Description>

  <Description>
    What we really need is a way to encode semantic meaning directly into how we represent words. Think about how you naturally make sense of words: you know that "King" and "Queen" are related, that "red" and "blue" belong to the same category, and that "Paris" connects to "France" the way "Tokyo" connects to "Japan." The representation should let us capture these kinds of relationships and operate on them, the way we do. Neither a raw integer nor a sparse one-hot vector can support this. We need something richer.
  </Description>
</Step>


<Step title="2. Representing Meaning with Multiple Numbers">
  <Description>
    What if instead of one number, we used a list of numbers to represent each word? Each number in the list could capture a different attribute of the word's meaning. Let's see how this would work.
  </Description>

  <Description>
    To make this concrete, let's design a simple system with just two attributes: **Royalty** and **Gender**. We will score each word on both of these dimensions, using a scale from **-1.0** to **+1.0**. For Royalty, +1 means strongly royal, -1 means strongly opposite, and 0 means unrelated. For Gender, +1 means masculine, -1 means feminine, and 0 means neutral.
  </Description>

  <Description>
    With these two dimensions, we can now represent words as pairs of numbers. Let's see how a few example words would look:
  </Description>

  <Table>
    <TableHead>
      <TableRow>
        <TableHeader>Word</TableHeader>
        <TableHeader>Royalty</TableHeader>
        <TableHeader>Gender</TableHeader>
        <TableHeader>Vector</TableHeader>
      </TableRow>
    </TableHead>
    <TableBody>
      <TableRow>
        <TableCell className="text-primary">ü§¥ King</TableCell>
        <TableCell className="text-emerald-400 font-mono">1.0</TableCell>
        <TableCell className="text-blue-400 font-mono">1.0</TableCell>
        <TableCell className="font-mono">[1.0, 1.0]</TableCell>
      </TableRow>
      <TableRow>
        <TableCell className="text-primary">üë∏ Queen</TableCell>
        <TableCell className="text-emerald-400 font-mono">1.0</TableCell>
        <TableCell className="text-pink-400 font-mono">-1.0</TableCell>
        <TableCell className="font-mono">[1.0, -1.0]</TableCell>
      </TableRow>
      <TableRow>
        <TableCell className="text-primary">üßî Man</TableCell>
        <TableCell className="text-muted font-mono">0.0</TableCell>
        <TableCell className="text-blue-400 font-mono">1.0</TableCell>
        <TableCell className="font-mono">[0.0, 1.0]</TableCell>
      </TableRow>
      <TableRow>
        <TableCell className="text-primary">üçé Apple</TableCell>
        <TableCell className="text-muted font-mono">0.0</TableCell>
        <TableCell className="text-muted font-mono">0.0</TableCell>
        <TableCell className="font-mono">[0.0, 0.0]</TableCell>
      </TableRow>
    </TableBody>
  </Table>

  <Description>
    By representing each word as a vector, we have encoded meaning directly into the numbers. King and Queen share the same Royalty score. King and Man share the same Gender score. And Apple sits at zero for both, since royalty and gender do not really apply to fruit.
  </Description>
</Step>


<Step title="3. Visualizing Word Vectors">
  <Description>
    Since we used only two dimensions in our example, we can plot these words on a standard X-Y coordinates with Royalty on the vertical axis and Gender on the horizontal axis.
  </Description>

  <EmbeddingSpace />

  <Description>
    Every word now sits at a specific coordinate in this 2D space. The space organizes words by meaning. Words with similar concepts group into specific regions. You can see King and Queen sit at the top, while King and Man sit on the right. Apple sits at the origin because neither dimension applies to it.
  </Description>

  <Description>
    Because each dimension represents an attribute, the direction you travel from one word to another captures their relationship. Notice the arrows in the graph below.
  </Description>

  <EmbeddingSpace showArrows />

  <Description>
    The arrow from King to Queen represents "flip gender while keeping royalty the same." The arrow from Man to Woman represents the same transformation. Both point the same direction because both represent the same concept.
  </Description>

  <Description attached>
    This geometry allows us to do arithmetic on meaning. If we take the concept of **King**, subtract the **Man** component, and add **Woman**, we should logically discover the female equivalent of royalty. Let's test this with our vectors:
  </Description>

  <div className="content-attached p-4 bg-surface rounded-lg border border-border">
    <div className="space-y-3 font-mono text-sm">
      <div className="flex items-center gap-3">
        <span className="text-muted w-28">King:</span>
        <span className="text-primary">[1.0, 1.0]</span>
      </div>
      <div className="flex items-center gap-3">
        <span className="text-muted w-28">Minus Man:</span>
        <span className="text-red-400">-[0.0, 1.0]</span>
      </div>
      <div className="flex items-center gap-3">
        <span className="text-muted w-28">Plus Woman:</span>
        <span className="text-emerald-400">+[0.0, -1.0]</span>
      </div>
      <div className="pt-3 border-t border-border flex items-center gap-3">
        <span className="text-muted w-28">Result:</span>
        <span className="text-emerald-400 font-bold text-lg">[1.0, -1.0]</span>
      </div>
    </div>
  </div>

  <Description>
    The result is exactly the vector for **Queen**. By taking King, removing the "maleness," and adding "femaleness," we mechanically arrived at Queen. This confirms that by expanding from a single number to a vector, we have successfully captured two things: the **semantic meaning** of each word, and the **logical relationships** between them, allowing us to manipulate relationships using simple math.
  </Description>
</Step>


<Step title="4. The Embedding Layer">
  <Description>
    The vectors we have been building are called **embeddings** in machine learning, and the component that stores and retrieves them is called the **embedding layer**. In the previous sections, we hand-picked two dimensions (Royalty and Gender) and manually assigned values to each word. In practice, we only decide how many dimensions to use and initialize all values randomly. The model then learns both what each dimension represents and the specific values for every word in the vocabulary during training.
  </Description>

  <Description>
    The embedding layer is implemented as a simple 2D array where each row stores the vector for one token. If our vocabulary has $V$ tokens and we choose $d_{model}$ dimensions, the array has shape `[V, d_model]`. To convert a token ID into its embedding, the layer simply looks up that row in the table.
  </Description>

  <Table>
    <TableHead>
      <TableRow>
        <TableHeader>Token ID</TableHeader>
        <TableHeader>Dim 1</TableHeader>
        <TableHeader>Dim 2</TableHeader>
        <TableHeader>Dim 3</TableHeader>
        <TableHeader>...</TableHeader>
        <TableHeader>Dim $d_{model}$</TableHeader>
      </TableRow>
    </TableHead>
    <TableBody>
      <TableRow>
        <TableCell>0 ("the")</TableCell>
        <TableCell className="font-mono text-primary">0.12</TableCell>
        <TableCell className="font-mono text-primary">-0.45</TableCell>
        <TableCell className="font-mono text-primary">0.78</TableCell>
        <TableCell className="text-muted">...</TableCell>
        <TableCell className="font-mono text-primary">0.33</TableCell>
      </TableRow>
      <TableRow>
        <TableCell>1 ("King")</TableCell>
        <TableCell className="font-mono text-primary">0.89</TableCell>
        <TableCell className="font-mono text-primary">0.56</TableCell>
        <TableCell className="font-mono text-primary">-0.21</TableCell>
        <TableCell className="text-muted">...</TableCell>
        <TableCell className="font-mono text-primary">0.67</TableCell>
      </TableRow>
      <TableRow>
        <TableCell className="text-muted">...</TableCell>
        <TableCell className="text-muted">...</TableCell>
        <TableCell className="text-muted">...</TableCell>
        <TableCell className="text-muted">...</TableCell>
        <TableCell className="text-muted">...</TableCell>
        <TableCell className="text-muted">...</TableCell>
      </TableRow>
    </TableBody>
  </Table>

  <Description>
    The two numbers that define the embedding layer are the **vocabulary size** $V$ (number of rows) and the **embedding dimension** $d_{model}$ (number of columns). A larger vocabulary lets more words exist as single tokens, so sentences become shorter sequences, but it also adds more rows to the table and increases memory. A larger embedding dimension gives the model more attributes to describe each word, but it adds more columns to every row and increases both memory and computation. In practice, vocabulary sizes range from around 32,000 to 100,000 tokens, while embedding dimensions range from a few hundred to several thousand depending on the model's scale.
  </Description>

  <Description>
    When training begins, every value in this table is initialized with small random numbers. "King" might start near "Sandwich" purely by chance. The structure we saw earlier does not exist yet, but it emerges as the model trains.
  </Description>

  <Description>
    During training, the model reads billions of sentences and tries to predict the next word. When it gets a prediction wrong, the error signal flows backward through the network and nudges the embedding values. If "King" often appears near "throne," "crown," and "palace," their vectors gradually move closer together. If "apple" appears near "orange" and "banana," those cluster together instead.
  </Description>

  <Description>
    After enough examples, the random numbers transform into the kind of organized space we visualized earlier. Unlike our hand-picked dimensions, the learned dimensions are abstract and often uninterpretable, but they capture whatever structure helps the model predict well.
  </Description>
</Step>


<Step title="5. What Embeddings Don't Capture">
  <Description>
    We now have a way to represent what each token means. The embedding layer converts arbitrary token IDs into rich vectors where similar words cluster together and relationships become geometric directions.
  </Description>

  <Description>
    But embeddings do not capture position. The embedding layer is just a lookup table, so the vector for "Alice" is the same whether she appears first, third, or last in a sentence.
  </Description>

  <Description>
    This matters because Transformers process all tokens in parallel, seeing the entire sentence at once rather than reading left to right like older models. This makes them fast, but it also means the model has no built-in sense of order. Without position information, "Alice helped Bob" and "Bob helped Alice" produce the same embeddings with no indication of word order. The next chapter introduces positional encoding to address this.
  </Description>

  <Callout type="success" title="Summary">
    * **Raw token IDs** fail because a single number cannot encode meaning, and magnitude bias creates false quantitative relationships
    * **Embeddings** represent tokens as vectors where each dimension captures an aspect of meaning
    * **Similar words** cluster together in vector space, and relationships become directions (King ‚àí Man + Woman ‚âà Queen)
    * **The embedding layer** is a learned lookup table with shape `[V, d_model]` that gets refined during training
    * **Embeddings do not capture position** since the embedding layer maps each token to the same vector regardless of where it appears
  </Callout>
</Step>
