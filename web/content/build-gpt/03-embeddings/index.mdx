---
title: "The Vector Space"
step: 3
description: "How to represent tokens as vectors to capture semantic meaning."
---

<div className="flex flex-col" style={{ gap: 'var(--space-flow)' }}>
<Description noMargin>
  In the previous chapter, we built a Tokenizer that converts text into integers. A sentence like "I love cats" becomes something like `[40, 1842, 9246]`. Since computers work with numbers, it might seem like we are ready to feed these integers directly into a neural network.
</Description>

<Description noMargin>
  But raw token IDs have some fundamental issues. For one, neural networks assume that the size of a number means something. If one word has an ID 10√ó larger than another, the network will mathematically treat it as "10√ó more" of something, even though these IDs are just arbitrary labels. Beyond that, a single number cannot capture the complexity of what a word means. Think about the word "Queen": to truly understand it, the model needs to know about gender, status, grammar, and more. There is no way a single integer like 3878 can hold all of that. Let's look at these issues more closely.
</Description>
</div>


<Step title="1. The Limits of Token IDs">
  <Description>
    Let's start with the magnitude problem. When a neuron processes input, it multiplies by a weight: `Output = Input √ó Weight`. If "red" has ID 87 and "yellow" has ID 892, then "yellow" produces about 10√ó more activation than "red" before the model has learned anything at all. The network is forced to treat certain words as inherently "bigger" than others, purely because of how IDs happened to be assigned. This is called **magnitude bias**, and it corrupts the learning signal from the very first training step.
  </Description>

  <Description>
    One common workaround is **one-hot encoding**. Instead of using a single integer, each token becomes a long vector of zeros with a single 1 at its index position. If our vocabulary has 50,000 tokens, then each word becomes a vector of 50,000 numbers, with exactly one of them set to 1. This eliminates magnitude bias because every token now has exactly the same "intensity." But it still does not solve the second problem: a one-hot vector gives us no room to encode meaning. "King" and "Queen" are orthogonal vectors, mathematically just as different from each other as "King" is from "Banana." Every word is equally distant from every other word, so we don't have any sense of similarity or semantic meaning.
  </Description>

  <Description>
    What we really need is a way to encode semantic meaning directly into how we represent words. Think about how you naturally make sense of words: you know that "King" and "Queen" are related, that "red" and "blue" belong to the same category, and that "Paris" connects to "France" the way "Tokyo" connects to "Japan." The representation should let us capture these kinds of relationships and operate on them, the way we do. Neither a raw integer nor a sparse one-hot vector can support this. We need something richer.
  </Description>
</Step>


<Step title="2. Representing Meaning with Multiple Numbers">
  <Description>
    What if instead of one number, we used a list of numbers to represent each word? Each number in the list could capture a different attribute of the word's meaning. Let's see how this would work.
  </Description>

  <Description>
    To make this concrete, let's design a simple system with just two attributes: **Royalty** and **Gender**. We will score each word on both of these dimensions, using a scale from **-1.0** to **+1.0**. For Royalty, +1 means strongly royal, -1 means strongly opposite, and 0 means unrelated. For Gender, +1 means masculine, -1 means feminine, and 0 means neutral.
  </Description>

  <Description>
    With these two dimensions, we can now represent words as pairs of numbers. Let's see how a few example words would look:
  </Description>

  <Table>
    <TableHead>
      <TableRow>
        <TableHeader>Word</TableHeader>
        <TableHeader>Royalty</TableHeader>
        <TableHeader>Gender</TableHeader>
        <TableHeader>Vector</TableHeader>
      </TableRow>
    </TableHead>
    <TableBody>
      <TableRow>
        <TableCell className="text-primary">ü§¥ King</TableCell>
        <TableCell className="text-emerald-400 font-mono">1.0</TableCell>
        <TableCell className="text-blue-400 font-mono">1.0</TableCell>
        <TableCell className="font-mono">[1.0, 1.0]</TableCell>
      </TableRow>
      <TableRow>
        <TableCell className="text-primary">üë∏ Queen</TableCell>
        <TableCell className="text-emerald-400 font-mono">1.0</TableCell>
        <TableCell className="text-pink-400 font-mono">-1.0</TableCell>
        <TableCell className="font-mono">[1.0, -1.0]</TableCell>
      </TableRow>
      <TableRow>
        <TableCell className="text-primary">üßî Man</TableCell>
        <TableCell className="text-muted font-mono">0.0</TableCell>
        <TableCell className="text-blue-400 font-mono">1.0</TableCell>
        <TableCell className="font-mono">[0.0, 1.0]</TableCell>
      </TableRow>
      <TableRow>
        <TableCell className="text-primary">üçé Apple</TableCell>
        <TableCell className="text-muted font-mono">0.0</TableCell>
        <TableCell className="text-muted font-mono">0.0</TableCell>
        <TableCell className="font-mono">[0.0, 0.0]</TableCell>
      </TableRow>
    </TableBody>
  </Table>

  <Description>
    By representing each word as a vector, we have encoded meaning directly into the numbers. King and Queen share the same Royalty score. King and Man share the same Gender score. And Apple sits at zero for both, since royalty and gender do not really apply to fruit.
  </Description>
</Step>


<Step title="3. Visualizing the Meaning Space">
  <Description>
    In our simple example we acted as human linguists and hand-picked clear categories like Royalty and Gender, but in real deep learning we do not define these labels beforehand. We instead give the model a fixed number of dimensions, known as the embedding size (often denoted as $d_{model}$), which effectively serves as a list of empty attribute slots waiting to be filled. We let the model figure out what they mean during training by allowing it to discover and assign its own abstract features to these slots, representing complex relationships like plurality, sentiment, or grammatical rules that humans might not even have names for.
  </Description>

  <Description>
    But because we stuck to 2 dimensions for our example, we can plot these words on a standard X-Y graph.
  </Description>

  <EmbeddingSpace />

  <Description>
    In this 2D space, every word lands at a specific coordinate. We have turned words into **Geometry**.
  </Description>

  <div className="my-6 space-y-4">
    <div>
      <h5 className="text-base font-semibold text-primary mb-1">Similarity = Closeness</h5>
      <p className="text-secondary">Words with similar meanings cluster together. King and Queen both sit in the upper region (high Royalty). Man and King share the right side (positive Gender). Meanwhile, Apple sits alone at the origin. It has nothing in common with royalty or gender.</p>
    </div>
    <div>
      <h5 className="text-base font-semibold text-primary mb-1">Region = Topic</h5>
      <p className="text-secondary">The top half of the graph is the "Royal Region." The right half is the "Male Region." Words naturally organize into neighborhoods of related concepts.</p>
    </div>
  </div>
</Step>


<Step title="4. Semantic Arithmetic: The King-Queen Analogy">
  <Description>
    Because we are now working with geometry, we can do something almost magical. We can perform **arithmetic on meaning itself**.
  </Description>

  <EmbeddingSpace showArrows />

  <Description>
    Look at the arrows on the map above. The arrow from **King** to **Queen** represents "Flipping Gender" while keeping Royalty constant. The arrow from **Man** to **Woman** represents the exact same transformation.
  </Description>

  <Description>
    In a well-trained model, those two arrows are almost identical. The model has learned that the concept of "Gender" isn't just a label. It is a **specific direction in space**.
  </Description>

  <div className="my-6 p-5 bg-surface rounded-lg border border-border">
    <div className="text-center mb-4">
      <span className="text-sm font-medium text-muted">The Famous Word Puzzle</span>
    </div>
    <div className="text-center text-lg font-mono text-primary">
      King - Man + Woman = <span className="text-emerald-400 font-bold">?</span>
    </div>
  </div>

  <Description attached>
    Let's plug in the numbers from Section 2:
  </Description>

  <div className="content-attached p-4 bg-surface rounded-lg border border-border">
    <div className="space-y-3 font-mono text-sm">
      <div className="flex items-center gap-3">
        <span className="text-muted w-28">King:</span>
        <span className="text-primary">[1.0, 1.0]</span>
      </div>
      <div className="flex items-center gap-3">
        <span className="text-muted w-28">Minus Man:</span>
        <span className="text-red-400">-[0.0, 1.0]</span>
      </div>
      <div className="flex items-center gap-3">
        <span className="text-muted w-28">Plus Woman:</span>
        <span className="text-emerald-400">+[0.0, -1.0]</span>
      </div>
      <div className="pt-3 border-t border-border">
        <div className="text-xs text-muted mb-2">Calculation:</div>
        <div className="space-y-1">
          <div className="text-secondary">First dimension (Royalty): $1.0 - 0.0 + 0.0 = $ <span className="text-emerald-400 font-bold">1.0</span></div>
          <div className="text-secondary">Second dimension (Gender): $1.0 - 1.0 + (-1.0) = $ <span className="text-emerald-400 font-bold">-1.0</span></div>
        </div>
      </div>
      <div className="pt-3 border-t border-border flex items-center gap-3">
        <span className="text-muted w-28">Result:</span>
        <span className="text-emerald-400 font-bold text-lg">[1.0, -1.0]</span>
      </div>
    </div>
  </div>

  <Description>
    Looking back at our definitions in Section 2, the vector `[1.0, -1.0]` belongs to:
  </Description>

  <div style={{ marginTop: "var(--space-connected)", marginBottom: "var(--space-connected)" }} className="pl-5 border-l-2 border-emerald-500/50">
    <div className="flex items-center gap-3">
      <span className="text-3xl">üë∏</span>
      <span className="text-xl font-semibold text-primary">Queen!</span>
    </div>
  </div>

  <Description>
    By taking the concept of a King, removing the "Man-ness", and adding "Woman-ness", we arrive mechanically at the coordinates for Queen. This demonstrates how the embedding space captures semantic relationships, showing that the model has organized language into a consistent geometric map where meaning can be manipulated mathematically.
  </Description>
</Step>


<Step title="5. Implementation: The Embedding Matrix">
  <Description>
    We know the model will learn these coordinates during training (remember those empty attribute slots from Section 3). But how exactly does this work in practice? Let's look at the actual data structure and the learning process.
  </Description>

  <h4 className="text-lg font-semibold text-primary" style={{ marginTop: "var(--space-section)", marginBottom: "var(--space-connected)" }}>The Embedding Layer: A Lookup Table</h4>

  <Description>
    In code, an **Embedding Layer** is a table of numbers. Think of it like a spreadsheet: each row corresponds to a token ID, and each column is one of those attribute slots. When the model sees a token, it simply looks up the corresponding row.
  </Description>

  <div className="my-6">
    <div className="text-xs font-medium text-muted uppercase tracking-wider mb-2">The Embedding Matrix Structure</div>
    <div className="p-4 bg-[#121212] rounded-lg border border-zinc-800">
    <div className="overflow-x-auto">
      <table className="w-full text-sm">
        <thead>
          <tr className="border-b border-border">
            <th className="text-left py-2 px-3 text-muted font-medium">Token ID</th>
            <th className="text-left py-2 px-3 text-muted font-medium">Dim 1</th>
            <th className="text-left py-2 px-3 text-muted font-medium">Dim 2</th>
            <th className="text-left py-2 px-3 text-muted font-medium">Dim 3</th>
            <th className="text-left py-2 px-3 text-muted font-medium">...</th>
            <th className="text-left py-2 px-3 text-muted font-medium">Dim N</th>
          </tr>
        </thead>
        <tbody className="font-mono text-xs">
          <tr className="border-b border-border">
            <td className="py-2 px-3 text-secondary">0 ("the")</td>
            <td className="py-2 px-3 text-primary">0.12</td>
            <td className="py-2 px-3 text-primary">-0.45</td>
            <td className="py-2 px-3 text-primary">0.78</td>
            <td className="py-2 px-3 text-muted">...</td>
            <td className="py-2 px-3 text-primary">0.33</td>
          </tr>
          <tr className="border-b border-border">
            <td className="py-2 px-3 text-secondary">1 ("King")</td>
            <td className="py-2 px-3 text-primary">0.89</td>
            <td className="py-2 px-3 text-primary">0.56</td>
            <td className="py-2 px-3 text-primary">-0.21</td>
            <td className="py-2 px-3 text-muted">...</td>
            <td className="py-2 px-3 text-primary">0.67</td>
          </tr>
          <tr>
            <td className="py-2 px-3 text-muted">...</td>
            <td className="py-2 px-3 text-muted">...</td>
            <td className="py-2 px-3 text-muted">...</td>
            <td className="py-2 px-3 text-muted">...</td>
            <td className="py-2 px-3 text-muted">...</td>
            <td className="py-2 px-3 text-muted">...</td>
          </tr>
        </tbody>
      </table>
    </div>
    </div>
  </div>

  <p className="text-sm text-muted mt-3">
    When the model sees token ID 1, it simply looks up row 1 and grabs that entire row as the vector. Just a table lookup.
  </p>

  <Description attached>
    The size of this table depends on two choices you make when designing your model:
  </Description>

  <div className="content-attached grid grid-cols-1 md:grid-cols-2 gap-8">
    <div>
      <h5 className="text-base font-semibold text-primary mb-1">Vocabulary Size</h5>
      <p className="text-sm text-secondary mb-3">Number of rows (one per token)</p>
      <ul className="text-sm text-muted space-y-1">
        <li>GPT-2: ~50,000 tokens</li>
        <li>GPT-4: ~100,000 tokens</li>
        <li>Llama 2: ~32,000 tokens</li>
      </ul>
    </div>
    <div>
      <h5 className="text-base font-semibold text-primary mb-1">Embedding Dimension</h5>
      <p className="text-sm text-secondary mb-3">Number of columns (attributes per token)</p>
      <ul className="text-sm text-muted space-y-1">
        <li>GPT-2 Small: 768 dimensions</li>
        <li>GPT-3: 12,288 dimensions</li>
        <li>Llama 2 70B: 8,192 dimensions</li>
      </ul>
    </div>
  </div>

  <Callout type="tip" title="Why Do Models Use Different Sizes?">
    <div className="space-y-2">
      <div><strong>Vocabulary size</strong> (rows): More tokens means fewer unknown words, but requires more memory.</div>
      <div><strong>Embedding dimension</strong> (columns): More dimensions allows richer meaning representation, but increases memory usage and computation time.</div>
      <div className="pt-2 text-secondary">When designing a model, you balance these based on your constraints: available compute, memory budget, target latency, and how much training data you have.</div>
    </div>
  </Callout>

  <h4 className="text-lg font-semibold text-primary" style={{ marginTop: "var(--space-section)", marginBottom: "var(--space-connected)" }}>The Training Process</h4>

  <Description>
    When we first initialize the embedding table, every cell is filled with random numbers. "King" might start at coordinates right next to "Sandwich." The map has no structure at all.
  </Description>

  <Description>
    Training is how the model figures out what each dimension should represent, and it happens **end-to-end** with the rest of the model. We don't build embeddings separately; the embedding table is the first learned layer that feeds into the transformer stack.
  </Description>

  <Description>
    When the model tries to predict the next word and makes a mistake, the "correction signal" (gradients) flows backwards through the entire network, updating the transformer blocks and finally the embedding vectors themselves. That reshaping teaches which coordinates should pull related tokens closer and push unrelated ones apart. It's how meaning gets baked into those embedding vectors *at the same time* the model learns how to use the words in context.
  </Description>

  <ProcessTimeline steps={[
    {
      title: "Model sees a sentence with a gap:",
      data: '"The King sat on the ___"',
      description: "The training sentence."
    },
    {
      title: "It makes a guess:",
      data: '"Banana" (totally wrong, but it\'s early in training!)',
      description: "Random prediction early in training."
    },
    {
      title: "We reveal the correct answer:",
      data: '"The King sat on the Throne"',
      description: "The correct word is compared against the prediction."
    },
    {
      title: "The model adjusts its internal map:",
      description: '"King" appeared near "Throne," so the model nudges "King" a little closer to "Throne" in its coordinate space, and a little further from "Banana."'
    }
  ]} />

  <Description>
    This process repeats many times with different sentences. Every time "King" appears near "Queen," "Crown," or "Palace," the model nudges their coordinates closer. Every time "Apple" appears near "Orange" and "Banana," those fruit words cluster together.
  </Description>

  <Description>
    After enough examples, the random noise transforms into the structured semantic space we visualized in Section 3. The model has discovered concepts like Royalty and Gender on its own, encoded them into its dimensions, and organized all the words accordingly.
  </Description>
</Step>


<Step title="6. Why Word Order Gets Lost">
  <Description>
    We have solved the meaning problem. We can translate "King" into a rich vector that captures its essence.
  </Description>

  <Description>
    Here's the issue. Traditional language models (like RNNs) read words one by one, left to right, so order is built in. But this sequential approach is painfully slow. You cannot process word #5 until you have finished words #1 through #4. This makes training on billions of sentences take forever.
  </Description>

  <Description>
    Transformer architectures (used in modern LLMs) solve the speed problem by processing all words **in parallel**, reading the entire sentence at once. This is massively faster and more scalable. But it creates a new problem: if you hand the model all words simultaneously, how does it know which came first?
  </Description>

  <Description>
    Think back to our preprocessing pipeline: Text ‚Üí Bytes ‚Üí Tokens ‚Üí Vectors. At no point did we encode **where** each word appears. The token ID for "Alice" is the same whether she appears first, third, or last in a sentence. And the embedding vector we just learned to look up? It only captures **what** the word means, not **where** it sits.
  </Description>

  <Description>
    This becomes a problem when words need to interact with each other. In a Transformer, every word looks at every other word simultaneously to build understanding. Let's visualize this with the sentence "Alice gave Bob a book":
  </Description>

  <Description>
    In the grid below, `compare(wordA, wordB)` is a placeholder for the actual mechanism (which we'll cover in a later chapter). For now, just think of it as "wordA examines wordB."
  </Description>

  <div className="my-6 overflow-x-auto">
    <table className="w-full text-sm border-collapse">
      <thead>
        <tr>
          <th className="p-2 bg-surface border border-border"></th>
          <th className="p-2 bg-blue-50 dark:bg-blue-900/30 border border-border font-mono text-blue-700 dark:text-blue-300">Alice</th>
          <th className="p-2 bg-emerald-500/10 border border-border font-mono text-emerald-400">gave</th>
          <th className="p-2 bg-amber-50 dark:bg-amber-900/30 border border-border font-mono text-amber-700 dark:text-amber-300">Bob</th>
          <th className="p-2 bg-surface border border-border font-mono text-secondary">a</th>
          <th className="p-2 bg-purple-50 dark:bg-purple-900/30 border border-border font-mono text-purple-700 dark:text-purple-300">book</th>
        </tr>
      </thead>
      <tbody className="text-xs">
        <tr>
          <td className="p-2 bg-blue-50 dark:bg-blue-900/30 border border-border font-mono font-medium text-blue-700 dark:text-blue-300">Alice</td>
          <td className="p-2 bg-surface border border-border text-center text-muted">‚Äî</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(Alice, gave)</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(Alice, Bob)</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(Alice, a)</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(Alice, book)</td>
        </tr>
        <tr>
          <td className="p-2 bg-emerald-500/10 border border-border font-mono font-medium text-emerald-400">gave</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(gave, Alice)</td>
          <td className="p-2 bg-surface border border-border text-center text-muted">‚Äî</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(gave, Bob)</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(gave, a)</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(gave, book)</td>
        </tr>
        <tr>
          <td className="p-2 bg-amber-50 dark:bg-amber-900/30 border border-border font-mono font-medium text-amber-700 dark:text-amber-300">Bob</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(Bob, Alice)</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(Bob, gave)</td>
          <td className="p-2 bg-surface border border-border text-center text-muted">‚Äî</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(Bob, a)</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(Bob, book)</td>
        </tr>
        <tr>
          <td className="p-2 bg-surface border border-border font-mono font-medium text-secondary">a</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(a, Alice)</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(a, gave)</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(a, Bob)</td>
          <td className="p-2 bg-surface border border-border text-center text-muted">‚Äî</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(a, book)</td>
        </tr>
        <tr>
          <td className="p-2 bg-purple-50 dark:bg-purple-900/30 border border-border font-mono font-medium text-purple-700 dark:text-purple-300">book</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(book, Alice)</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(book, gave)</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(book, Bob)</td>
          <td className="p-2 bg-surface border border-border text-center text-secondary">compare(book, a)</td>
          <td className="p-2 bg-surface border border-border text-center text-muted">‚Äî</td>
        </tr>
      </tbody>
    </table>
    <div className="text-xs text-muted mt-2 text-center">
      Each cell shows one word examining another. All comparisons happen simultaneously.
    </div>
  </div>

  <Description>
    Here's the problem: `compare(Alice, Bob)` and `compare(Bob, Alice)` use the **exact same vectors**. The function only sees two meaning-vectors. It has no idea that in "Alice gave Bob," Alice comes before the verb (making her the giver) while Bob comes after (making him the receiver).
  </Description>

  <Description>
    Swap the sentence to "Bob gave Alice a book" and every `compare()` call produces identical results. The model cannot distinguish the giver from the receiver because position was never encoded.
  </Description>

  <Description>
    We need a way to stamp each vector with its position. That's exactly what we'll tackle in the next chapter.
  </Description>

  <Callout type="success" title="Summary">
    * **Raw Token IDs** cannot be used directly because the model would treat ID 500 as "5√ó more" than ID 100, which is meaningless
    * **Embeddings** are vectors (lists of numbers) that encode meaning. Similar words cluster together in vector space
    * The **Embedding Matrix** is learned during training: the model gradually nudges coordinates based on context
    * **Vector arithmetic** works on concepts: King ‚àí Man + Woman ‚âà Queen
    * **Parallel processing** is fast and scalable but loses word order, and we've identified this as a critical missing piece
  </Callout>

  <Description>
    We can now represent **what** each token means, but not **where** it appears. "Alice gave Bob" and "Bob gave Alice" produce identical embeddings despite meaning opposite things. In the next chapter, we'll tackle **Positional Encoding** to solve this.
  </Description>
</Step>
