---
title: "Feed-Forward Networks"
step: 7
description: "How the feed-forward network transforms each token independently."
---

<Description>
  In the previous chapters, attention allowed each token to gather relevant information from other positions in the sequence, producing a context-aware representation. This chapter introduces the feed-forward network (FFN), which takes each token's enriched vector and rewrites its features through a learned transformation applied independently at every position. While attention handles the flow of information between positions, the FFN processes that information within each position.
</Description>


<Step title="The FFN">
  <Description>
    The FFN takes a token's vector of size `d_model` and passes it through two linear transformations with a nonlinear activation function in between. The first linear projection expands the vector from `d_model` to a larger intermediate size called `d_ff`. Then a nonlinear activation function is applied element-wise. Finally, a second linear projection contracts the result back from `d_ff` to `d_model`.
  </Description>

  <div className="my-6 flex justify-center overflow-x-auto">
    <div className="inline-flex items-center gap-2 font-mono text-sm p-4">
      <div className="px-3 py-2 bg-surface rounded border border-border text-center">
        <div className="text-muted text-xs mb-0.5">input</div>
        <div className="text-primary font-semibold">d_model</div>
      </div>
      <div className="text-muted text-lg">→</div>
      <div className="px-3 py-2 bg-blue-500/10 rounded border border-blue-500/30 text-center">
        <div className="text-blue-400 text-xs mb-0.5">W<sub>1</sub></div>
        <div className="text-blue-400">expand</div>
      </div>
      <div className="text-muted text-lg">→</div>
      <div className="px-3 py-2 bg-amber-500/10 rounded border border-amber-500/30 text-center">
        <div className="text-amber-400 text-xs mb-0.5">activation</div>
        <div className="text-amber-400 font-semibold">d_ff</div>
      </div>
      <div className="text-muted text-lg">→</div>
      <div className="px-3 py-2 bg-blue-500/10 rounded border border-blue-500/30 text-center">
        <div className="text-blue-400 text-xs mb-0.5">W<sub>2</sub></div>
        <div className="text-blue-400">contract</div>
      </div>
      <div className="text-muted text-lg">→</div>
      <div className="px-3 py-2 bg-surface rounded border border-border text-center">
        <div className="text-muted text-xs mb-0.5">output</div>
        <div className="text-primary font-semibold">d_model</div>
      </div>
    </div>
  </div>

  <Description>
    In equation form, the complete operation is:
  </Description>

  <div className="my-6 p-5 bg-surface rounded-lg border border-border">
    <div className="text-center font-mono text-lg text-primary">
      FFN(x) = W<sub>2</sub> · activation(W<sub>1</sub> · x + b<sub>1</sub>) + b<sub>2</sub>
    </div>
  </div>

  <Description>
    The expansion through W<sub>1</sub> produces a vector of `d_ff` values, where each value is a dot product between the token's vector and one of W<sub>1</sub>'s learned weight vectors. Each dot product measures how much the token's representation aligns with what that weight vector has learned to respond to. A large positive result means strong alignment, a value near zero means weak alignment, and a negative value means the representation points away from what that weight vector has learned.
  </Description>

  <Description>
    The activation function then filters the expanded vector element-wise. With ReLU for example, every negative value becomes zero and every positive value passes through unchanged. Since different tokens produce different values after W<sub>1</sub>, the activation zeros out or suppresses different neurons for each one, so each token passes a different combination of active and suppressed neurons forward to W<sub>2</sub>.
  </Description>

  <Description>
    The contraction through W<sub>2</sub> then maps these filtered values back from `d_ff` to `d_model` dimensions. Each of the `d_ff` neurons has a corresponding column in W<sub>2</sub> that defines how much that neuron contributes to each of the `d_model` output dimensions. Since different tokens activate different neurons, a different set of W<sub>2</sub> columns drives the output for each token, so the same weight matrices effectively apply a different transformation depending on the token's representation. This is what allows the FFN to capture nonlinear relationships rather than applying a single fixed transformation to every token.
  </Description>

  <Description>
    In GPT-2's base configuration, `d_model = 768` and `d_ff = 4 × d_model = 3072`, giving W<sub>1</sub> the shape `768 × 3072` and W<sub>2</sub> the shape `3072 × 768`. This 4× expansion ratio is a common default across many Transformer architectures based on empirical findings.
  </Description>
</Step>


<Step title="Why the Activation Matters">
  <Description>
    Without the activation, the FFN is just two linear projections applied in sequence, and since composing two linear transformations always produces another single linear transformation, the product W<sub>2</sub>·W<sub>1</sub> collapses into one matrix. That reduces the entire FFN to a single `d_model × d_model` multiplication where the expansion to `d_ff` adds nothing and the network can no longer capture nonlinear relationships in the data.
  </Description>

  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800 font-mono text-sm text-center space-y-2">
    <div className="text-muted">Without activation:</div>
    <div>W<sub>2</sub> · (W<sub>1</sub> · x) = (W<sub>2</sub> · W<sub>1</sub>) · x = W<sub>combined</sub> · x</div>
  </div>

  <Description>
    The activation function prevents this collapse by inserting a nonlinear operation between the two projections. As covered above, it selectively zeros out or suppresses different neurons for each token, so W<sub>2</sub> receives a different filtered signal each time rather than a plain linear combination of W<sub>1</sub>'s output. This is what keeps the two matrices from collapsing into a single linear operation and what makes the expansion to `d_ff` meaningful.
  </Description>
</Step>
