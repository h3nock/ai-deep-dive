---
title: "Feed-Forward Networks"
step: 7
description: "How the feed-forward network transforms each token independently."
---

<Description>
  In the previous chapters, attention allowed each token to gather relevant information from other positions in the sequence, producing a context-aware representation. This chapter introduces the feed-forward network (FFN), which takes each token's enriched vector and rewrites its features through a learned transformation applied independently at every position. While attention handles the flow of information between positions, the FFN processes that information within each position.
</Description>


<Step title="The FFN">
  <Description>
    The FFN takes a token's vector of size `d_model` and passes it through two linear transformations with a nonlinear activation function in between. The first linear projection expands the vector from `d_model` to a larger intermediate size called `d_ff`. Then a nonlinear activation function is applied element-wise. Finally, a second linear projection contracts the result back from `d_ff` to `d_model`.
  </Description>

  <div className="my-6 flex justify-center overflow-x-auto">
    <svg viewBox="0 0 520 160" className="w-full" style={{ maxWidth: "520px", display: "block" }}>
      {/* Input box - d_model (small) */}
      <rect x="10" y="55" width="60" height="50" rx="4" fill="#18181B" stroke="#27272A" strokeWidth="1" />
      <text x="40" y="72" textAnchor="middle" fill="#71717A" fontSize="10" fontFamily="var(--font-mono)">input</text>
      <text x="40" y="90" textAnchor="middle" fill="#D4D4D8" fontSize="12" fontWeight="600" fontFamily="var(--font-mono)">d_model</text>

      {/* Arrow */}
      <text x="85" y="84" textAnchor="middle" fill="#71717A" fontSize="16">→</text>

      {/* W₁ - trapezoid expanding from small to large */}
      <polygon points="100,55 160,20 160,140 100,105" rx="4" fill="rgba(59,130,246,0.08)" stroke="rgba(59,130,246,0.3)" strokeWidth="1" />
      <text x="125" y="76" textAnchor="middle" fill="#60a5fa" fontSize="10" fontFamily="var(--font-mono)">W₁</text>
      <text x="125" y="92" textAnchor="middle" fill="#60a5fa" fontSize="11" fontFamily="var(--font-mono)">expand</text>

      {/* Arrow */}
      <text x="175" y="84" textAnchor="middle" fill="#71717A" fontSize="16">→</text>

      {/* Activation box - d_ff (4x tall) */}
      <rect x="190" y="20" width="70" height="120" rx="4" fill="rgba(245,158,11,0.08)" stroke="rgba(245,158,11,0.3)" strokeWidth="1" />
      <text x="225" y="72" textAnchor="middle" fill="#f59e0b" fontSize="10" fontFamily="var(--font-mono)">activation</text>
      <text x="225" y="90" textAnchor="middle" fill="#f59e0b" fontSize="12" fontWeight="600" fontFamily="var(--font-mono)">d_ff</text>

      {/* Arrow */}
      <text x="275" y="84" textAnchor="middle" fill="#71717A" fontSize="16">→</text>

      {/* W₂ - trapezoid contracting from large to small */}
      <polygon points="290,20 350,55 350,105 290,140" rx="4" fill="rgba(59,130,246,0.08)" stroke="rgba(59,130,246,0.3)" strokeWidth="1" />
      <text x="320" y="76" textAnchor="middle" fill="#60a5fa" fontSize="10" fontFamily="var(--font-mono)">W₂</text>
      <text x="320" y="92" textAnchor="middle" fill="#60a5fa" fontSize="11" fontFamily="var(--font-mono)">contract</text>

      {/* Arrow */}
      <text x="365" y="84" textAnchor="middle" fill="#71717A" fontSize="16">→</text>

      {/* Output box - d_model (small) */}
      <rect x="380" y="55" width="60" height="50" rx="4" fill="#18181B" stroke="#27272A" strokeWidth="1" />
      <text x="410" y="72" textAnchor="middle" fill="#71717A" fontSize="10" fontFamily="var(--font-mono)">output</text>
      <text x="410" y="90" textAnchor="middle" fill="#D4D4D8" fontSize="12" fontWeight="600" fontFamily="var(--font-mono)">d_model</text>
    </svg>
  </div>

  <Description>
    In equation form, the complete operation is:
  </Description>

  <div className="my-6 p-5 bg-surface rounded-lg border border-border">
    <div className="text-center font-mono text-lg text-primary">
      FFN(x) = W<sub>2</sub> · activation(W<sub>1</sub> · x + b<sub>1</sub>) + b<sub>2</sub>
    </div>
  </div>

  <Description>
    The expansion through W<sub>1</sub> produces a vector of `d_ff` values, where each value is a dot product between the token's vector and one of W<sub>1</sub>'s learned weight vectors. Each dot product measures how much the token's representation aligns with what that weight vector has learned to respond to. A large positive result means strong alignment, a value near zero means weak alignment, and a negative value means the representation points away from what that weight vector has learned.
  </Description>

  <Description>
    The activation function then filters the expanded vector element-wise. With ReLU for example, every negative value becomes zero and every positive value passes through unchanged. Since different tokens produce different values after W<sub>1</sub>, the activation zeros out or suppresses different neurons for each one, so each token passes a different combination of active and suppressed neurons forward to W<sub>2</sub>.
  </Description>

  <Description>
    The contraction through W<sub>2</sub> then maps these filtered values back from `d_ff` to `d_model` dimensions. Each of the `d_ff` neurons has a corresponding column in W<sub>2</sub> that defines how much that neuron contributes to each of the `d_model` output dimensions. Since different tokens activate different neurons, a different set of W<sub>2</sub> columns drives the output for each token, so the same weight matrices effectively apply a different transformation depending on the token's representation. This is what allows the FFN to capture nonlinear relationships rather than applying a single fixed transformation to every token.
  </Description>

  <Description>
    In GPT-2's base configuration, `d_model = 768` and `d_ff = 4 × d_model = 3072`, giving W<sub>1</sub> the shape `768 × 3072` and W<sub>2</sub> the shape `3072 × 768`. This 4× expansion ratio is a common default across many Transformer architectures based on empirical findings.
  </Description>
</Step>


<Step title="Why the Activation Matters">
  <Description>
    Without the activation, the FFN is just two linear projections applied in sequence, and since composing two linear transformations always produces another single linear transformation, the product W<sub>2</sub>·W<sub>1</sub> collapses into one matrix. That reduces the entire FFN to a single `d_model × d_model` multiplication where the expansion to `d_ff` adds nothing and the network can no longer capture nonlinear relationships in the data.
  </Description>

  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800 font-mono text-sm text-center space-y-2">
    <div className="text-muted">Without activation:</div>
    <div>W<sub>2</sub> · (W<sub>1</sub> · x) = (W<sub>2</sub> · W<sub>1</sub>) · x = W<sub>combined</sub> · x</div>
  </div>

  <Description>
    The activation function prevents this collapse by inserting a nonlinear operation between the two projections. As covered above, it selectively zeros out or suppresses different neurons for each token, so W<sub>2</sub> receives a different filtered signal each time rather than a plain linear combination of W<sub>1</sub>'s output. This is what keeps the two matrices from collapsing into a single linear operation and what makes the expansion to `d_ff` meaningful.
  </Description>
</Step>
