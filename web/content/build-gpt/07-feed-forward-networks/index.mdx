---
title: "Feed-Forward Networks"
step: 7
description: "How the feed-forward network transforms each token independently."
---

<Description>
  In the previous chapters, attention allowed each token to gather relevant information from other positions in the sequence, producing a context-aware representation. This chapter introduces the feed-forward network (FFN), which takes each token's enriched vector and rewrites its features through a learned transformation applied independently at every position. While attention handles the flow of information between positions, the FFN processes that information within each position.
</Description>


<Step title="FFN Anatomy">
  <Description>
    The FFN takes a token's vector of size `d_model` and passes it through two linear transformations with a nonlinear activation function in between. The first linear projection expands it from `d_model` to a larger intermediate size called `d_ff`. Then a nonlinear activation function is applied element-wise. Finally, a second linear projection contracts the result back from `d_ff` to `d_model`. The output has the same size as the input, but its contents have been transformed.
  </Description>

  <div className="my-6 flex justify-center overflow-x-auto">
    <div className="inline-flex items-center gap-2 font-mono text-sm p-4">
      <div className="px-3 py-2 bg-surface rounded border border-border text-center">
        <div className="text-muted text-xs mb-0.5">input</div>
        <div className="text-primary font-semibold">d_model</div>
      </div>
      <div className="text-muted text-lg">→</div>
      <div className="px-3 py-2 bg-blue-500/10 rounded border border-blue-500/30 text-center">
        <div className="text-blue-400 text-xs mb-0.5">W<sub>1</sub></div>
        <div className="text-blue-400">expand</div>
      </div>
      <div className="text-muted text-lg">→</div>
      <div className="px-3 py-2 bg-amber-500/10 rounded border border-amber-500/30 text-center">
        <div className="text-amber-400 text-xs mb-0.5">activation</div>
        <div className="text-amber-400 font-semibold">d_ff</div>
      </div>
      <div className="text-muted text-lg">→</div>
      <div className="px-3 py-2 bg-blue-500/10 rounded border border-blue-500/30 text-center">
        <div className="text-blue-400 text-xs mb-0.5">W<sub>2</sub></div>
        <div className="text-blue-400">contract</div>
      </div>
      <div className="text-muted text-lg">→</div>
      <div className="px-3 py-2 bg-surface rounded border border-border text-center">
        <div className="text-muted text-xs mb-0.5">output</div>
        <div className="text-primary font-semibold">d_model</div>
      </div>
    </div>
  </div>

  <Description>
    In equation form, the complete operation is:
  </Description>

  <div className="my-6 p-5 bg-surface rounded-lg border border-border">
    <div className="text-center font-mono text-lg text-primary">
      FFN(x) = W<sub>2</sub> · activation(W<sub>1</sub> · x + b<sub>1</sub>) + b<sub>2</sub>
    </div>
  </div>

  <Description>
    In GPT-2's base configuration, `d_model = 768` and `d_ff = 4 × d_model = 3072`, giving W<sub>1</sub> the shape `768 × 3072` and W<sub>2</sub> the shape `3072 × 768`. This 4× expansion ratio is a common default across many Transformer architectures based on emprical findings.
  </Description>

</Step>


<Step title="Why the Activation Matters">
  <Description>
    Without the activation, the FFN is just two linear projections applied in sequence, and since composing two linear transformations always produces another single linear transformation, the product W<sub>2</sub>·W<sub>1</sub> collapses into one matrix. That reduces the entire FFN to a single `d_model × d_model` multiplication where the expansion to `d_ff` adds nothing and the network can no longer capture nonlinear relationships in the data.
  </Description>

  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800 font-mono text-sm text-center space-y-2">
    <div className="text-muted">Without activation:</div>
    <div>W<sub>2</sub> · (W<sub>1</sub> · x) = (W<sub>2</sub> · W<sub>1</sub>) · x = W<sub>combined</sub> · x</div>
  </div>

  <Description>
      The activation function breaks this linearity by applying a nonlinear operation between the two projections. After W<sub>1</sub> expands the input into `d_ff` values, the activation decides which of those values pass through. With ReLU for example, one of the most widely used activation functions in deep learning, any negative value becomes zero while positive values pass through unchanged. This means after applying the activation function, different inputs produce different patterns of active (on) and inactive (off) neurons, so W<sub>2</sub> receives a selectively filtered signal rather than a plain linear combination. This nonlinear filtering is what prevents the two weight matrices from collapsing into one and what gives the FFN the ability to model nonlinear relationships in the data.
  </Description>
</Step>
