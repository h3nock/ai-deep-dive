---
title: "Tokenization"
step: 2
description: "How BPE compresses text into efficient tokens."
---

<div className="flex flex-col" style={{ gap: 'var(--space-flow)' }}>
<Description noMargin>
  In the previous chapter, we've established that raw bytes create sequences that are too long, and word-level tokenization creates vocabularies that are too large. The solution is **subword tokenization**: chunking text into efficient pieces that balance sequence length with vocabulary size.
</Description>

<Description noMargin>
  The industry-standard algorithm for this is **Byte Pair Encoding (BPE)**. In this chapter, we'll understand how it works and build it from scratch.
</Description>
</div>


<Step title="1. The Key Insight">
  <Description>
    BPE is a **compression algorithm**. It finds patterns that appear frequently in the training data and replaces them with shorter symbols.
  </Description>

  <Description>
    Think about how you text. You don't type "Laughing Out Loud" every time. That's 17 characters. Instead, you type "LOL" (3 characters). You've mentally agreed that this sequence appears so frequently it deserves to be a single unit.
  </Description>

  <div className="my-6 flex items-center justify-center gap-8">
    <div className="text-center">
      <div className="font-mono text-sm text-secondary">"Laughing Out Loud"</div>
      <div className="text-xs text-muted mt-1">17 characters</div>
    </div>
    <div className="text-xl text-muted">→</div>
    <div className="text-center">
      <div className="font-mono text-sm font-bold text-primary">"LOL"</div>
      <div className="text-xs text-muted mt-1">3 characters</div>
    </div>
  </div>

  <Description>
    BPE does exactly this, but **automatically**. It reads through massive amounts of text (like Wikipedia) and asks: *"Which sequences of characters appear together most often?"*
  </Description>

  <div className="my-6">
    <div className="text-xs font-medium text-muted uppercase tracking-wider mb-2">Common Patterns Learned</div>
    <div className="bg-terminal rounded-lg border border-border divide-y divide-border">
      <div className="flex items-center gap-3 px-4 py-3">
        <span className="font-mono text-secondary">t + h</span>
        <span className="text-muted">→</span>
        <span className="font-mono font-bold text-primary">th</span>
        <span className="text-sm text-muted ml-auto">appears constantly together</span>
      </div>
      <div className="flex items-center gap-3 px-4 py-3">
        <span className="font-mono text-secondary">th + e</span>
        <span className="text-muted">→</span>
        <span className="font-mono font-bold text-primary">the</span>
        <span className="text-sm text-muted ml-auto">one of the most common words</span>
      </div>
      <div className="flex items-center gap-3 px-4 py-3">
        <span className="font-mono text-secondary">in + g</span>
        <span className="text-muted">→</span>
        <span className="font-mono font-bold text-primary">ing</span>
        <span className="text-sm text-muted ml-auto">common suffix</span>
      </div>
    </div>
  </div>

  <Description>
    By the end of this process, common words like "apple" become single tokens, while rare words are split into multiple smaller tokens. This allows the model to process text more efficiently.
  </Description>
</Step>


<Step title="2. BPE Walkthrough">
  <Description>
    To understand BPE, we need to run the algorithm by hand. We'll use a short sequence to see the mechanics clearly before scaling up to real text.
  </Description>

  <h4 className="text-lg font-semibold text-primary" style={{ marginTop: 'var(--space-flow)', marginBottom: 'var(--space-atomic)' }}>The Dataset</h4>

  <Description attached>
    We'll train BPE on this 11-character string:
  </Description>

  <div className="content-attached flex flex-wrap justify-center gap-2">
    {['a', 'a', 'a', 'b', 'd', 'a', 'a', 'a', 'b', 'a', 'c'].map((char, i) => (
      <span key={i} className="w-8 h-8 flex items-center justify-center font-mono text-sm text-primary bg-surface rounded">{char}</span>
    ))}
  </div>

  <h4 className="text-lg font-semibold text-primary" style={{ marginTop: 'var(--space-flow)', marginBottom: 'var(--space-atomic)' }}>Starting Point</h4>

  <Description>
    Before any merges, each character is its own token. Our sequence is **11 tokens** long.
  </Description>

  <div style={{ marginTop: "var(--space-connected)", marginBottom: "var(--space-connected)" }} className="flex flex-wrap gap-2">
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">a</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">a</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">a</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">b</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">d</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">a</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">a</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">a</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">b</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">a</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">c</div>
  </div>

  <h4 className="text-lg font-semibold text-primary" style={{ marginTop: 'var(--space-flow)', marginBottom: 'var(--space-atomic)' }}>Step 1: Count the Pairs</h4>

  <Description attached>
    BPE scans every **adjacent pair** of tokens and counts how often each appears:
  </Description>

  <div className="content-attached">
    <div className="text-xs font-medium text-muted uppercase tracking-wider mb-2">Adjacent pairs in our sequence (same color = same pair type)</div>
    <div className="p-4 bg-terminal rounded-lg border border-border">
    <div className="flex flex-wrap gap-x-1 gap-y-4 font-mono text-lg">
      {/* (a,a) - amber */}
      <div className="flex items-end">
        <span className="px-2 py-1 bg-emerald-100 dark:bg-emerald-900/40 rounded-t border-b-2 border-emerald-500">a</span>
        <span className="px-2 py-1 bg-emerald-100 dark:bg-emerald-900/40 rounded-t border-b-2 border-emerald-500">a</span>
      </div>
      {/* (a,a) - amber */}
      <div className="flex items-end">
        <span className="px-2 py-1 bg-emerald-100 dark:bg-emerald-900/40 rounded-t border-b-2 border-emerald-500">a</span>
        <span className="px-2 py-1 bg-emerald-100 dark:bg-emerald-900/40 rounded-t border-b-2 border-emerald-500">a</span>
      </div>
      {/* (a,b) - blue */}
      <div className="flex items-end">
        <span className="px-2 py-1 bg-blue-100 dark:bg-blue-900/40 rounded-t border-b-2 border-blue-500">a</span>
        <span className="px-2 py-1 bg-blue-100 dark:bg-blue-900/40 rounded-t border-b-2 border-blue-500">b</span>
      </div>
      {/* (b,d) - red */}
      <div className="flex items-end">
        <span className="px-2 py-1 bg-red-100 dark:bg-red-900/40 rounded-t border-b-2 border-red-500">b</span>
        <span className="px-2 py-1 bg-red-100 dark:bg-red-900/40 rounded-t border-b-2 border-red-500">d</span>
      </div>
      {/* (d,a) - emerald */}
      <div className="flex items-end">
        <span className="px-2 py-1 bg-amber-100 dark:bg-amber-900/40 rounded-t border-b-2 border-amber-500">d</span>
        <span className="px-2 py-1 bg-amber-100 dark:bg-amber-900/40 rounded-t border-b-2 border-amber-500">a</span>
      </div>
      {/* (a,a) - amber */}
      <div className="flex items-end">
        <span className="px-2 py-1 bg-emerald-100 dark:bg-emerald-900/40 rounded-t border-b-2 border-emerald-500">a</span>
        <span className="px-2 py-1 bg-emerald-100 dark:bg-emerald-900/40 rounded-t border-b-2 border-emerald-500">a</span>
      </div>
      {/* (a,a) - amber */}
      <div className="flex items-end">
        <span className="px-2 py-1 bg-emerald-100 dark:bg-emerald-900/40 rounded-t border-b-2 border-emerald-500">a</span>
        <span className="px-2 py-1 bg-emerald-100 dark:bg-emerald-900/40 rounded-t border-b-2 border-emerald-500">a</span>
      </div>
      {/* (a,b) - blue */}
      <div className="flex items-end">
        <span className="px-2 py-1 bg-blue-100 dark:bg-blue-900/40 rounded-t border-b-2 border-blue-500">a</span>
        <span className="px-2 py-1 bg-blue-100 dark:bg-blue-900/40 rounded-t border-b-2 border-blue-500">b</span>
      </div>
      {/* (b,a) - purple */}
      <div className="flex items-end">
        <span className="px-2 py-1 bg-purple-100 dark:bg-purple-900/40 rounded-t border-b-2 border-purple-500">b</span>
        <span className="px-2 py-1 bg-purple-100 dark:bg-purple-900/40 rounded-t border-b-2 border-purple-500">a</span>
      </div>
      {/* (a,c) - cyan */}
      <div className="flex items-end">
        <span className="px-2 py-1 bg-cyan-100 dark:bg-cyan-900/40 rounded-t border-b-2 border-cyan-500">a</span>
        <span className="px-2 py-1 bg-cyan-100 dark:bg-cyan-900/40 rounded-t border-b-2 border-cyan-500">c</span>
      </div>
    </div>
    <div className="mt-4 flex flex-wrap gap-x-4 gap-y-2 text-sm text-secondary">
      <span className="flex items-center gap-1.5"><span className="w-3 h-0.5 bg-emerald-500"></span>(a,a) × 4</span>
      <span className="flex items-center gap-1.5"><span className="w-3 h-0.5 bg-blue-500"></span>(a,b) × 2</span>
      <span className="flex items-center gap-1.5"><span className="w-3 h-0.5 bg-red-500"></span>(b,d) × 1</span>
      <span className="flex items-center gap-1.5"><span className="w-3 h-0.5 bg-amber-500"></span>(d,a) × 1</span>
      <span className="flex items-center gap-1.5"><span className="w-3 h-0.5 bg-purple-500"></span>(b,a) × 1</span>
      <span className="flex items-center gap-1.5"><span className="w-3 h-0.5 bg-cyan-500"></span>(a,c) × 1</span>
    </div>
    </div>
  </div>

  <div style={{ marginTop: "var(--space-connected)", marginBottom: "var(--space-connected)" }} className="flex flex-wrap gap-2">
    <div className="px-3 py-2 bg-emerald-500/10 rounded border-2 border-emerald-400 text-center">
      <div className="font-mono text-sm font-bold text-emerald-400">(a,a)</div>
      <div className="text-lg font-bold text-emerald-400">4</div>
    </div>
    <div className="px-3 py-2 bg-surface rounded border border-border text-center">
      <div className="font-mono text-sm text-secondary">(a,b)</div>
      <div className="text-lg font-bold text-muted">2</div>
    </div>
    <div className="px-3 py-2 bg-surface rounded border border-border text-center">
      <div className="font-mono text-sm text-secondary">(b,d)</div>
      <div className="text-lg font-bold text-muted">1</div>
    </div>
    <div className="px-3 py-2 bg-surface rounded border border-border text-center">
      <div className="font-mono text-sm text-secondary">(d,a)</div>
      <div className="text-lg font-bold text-muted">1</div>
    </div>
    <div className="px-3 py-2 bg-surface rounded border border-border text-center">
      <div className="font-mono text-sm text-secondary">(b,a)</div>
      <div className="text-lg font-bold text-muted">1</div>
    </div>
    <div className="px-3 py-2 bg-surface rounded border border-border text-center">
      <div className="font-mono text-sm text-secondary">(a,c)</div>
      <div className="text-lg font-bold text-muted">1</div>
    </div>
  </div>

  <h4 className="text-lg font-semibold text-primary" style={{ marginTop: 'var(--space-flow)', marginBottom: 'var(--space-atomic)' }}>Merge</h4>

  <Description>
    The pair `(a, a)` appears most often (4 times), so we merge it into a new token `Z`. Every `aa` in the original sequence becomes `Z`:
  </Description>

  <div className="my-6">
    <div className="text-xs font-medium text-muted uppercase tracking-wider mb-2">After Merge #1: (a, a) → Z</div>
    <div className="p-4 bg-terminal rounded-lg border border-border">
    <div className="flex flex-wrap gap-2">
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-emerald-500 text-primary font-mono font-bold">Z</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">a</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">b</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">d</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-emerald-500 text-primary font-mono font-bold">Z</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">a</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">b</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">a</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">c</div>
    </div>
    <div className="mt-3 text-sm text-secondary">
      <strong>Length: 9 tokens</strong> (down from 11)
    </div>
    </div>
  </div>

  <Callout type="tip" title="Key Insight">
    This is **compression**. The information is preserved, but the sequence is shorter. This is the core of BPE.
  </Callout>

  <h4 className="text-lg font-semibold text-primary" style={{ marginTop: 'var(--space-flow)', marginBottom: 'var(--space-atomic)' }}>Repeat</h4>

  <Description>
    We count pairs again on the updated sequence. Both `(Z, a)` and `(a, b)` now appear twice. When pairs tie in frequency, the algorithm picks one arbitrarily. Here we choose `(a, b)` and merge it into a new token `Y`:
  </Description>

  <div className="my-6">
    <div className="text-xs font-medium text-muted uppercase tracking-wider mb-2">After Merge #2: (a, b) → Y</div>
    <div className="p-4 bg-terminal rounded-lg border border-border">
    <div className="flex flex-wrap gap-2">
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-emerald-500 text-primary font-mono font-bold">Z</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-emerald-400/20 border-2 border-emerald-400 text-emerald-400 font-mono font-bold">Y</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">d</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-emerald-500 text-primary font-mono font-bold">Z</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-emerald-400/20 border-2 border-emerald-400 text-emerald-400 font-mono font-bold">Y</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">a</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">c</div>
    </div>
    <div className="mt-3 text-sm text-secondary">
      <strong>Length: 7 tokens</strong> (down from 9)
    </div>
    </div>
  </div>

  <h4 className="text-lg font-semibold text-primary" style={{ marginTop: 'var(--space-flow)', marginBottom: 'var(--space-atomic)' }}>Result</h4>

  <div style={{ marginTop: "var(--space-connected)", marginBottom: "var(--space-connected)" }} className="p-5 bg-surface rounded-lg border border-border">
    <div className="grid grid-cols-3 gap-4 text-center">
      <div>
        <div className="text-3xl font-bold text-muted">11</div>
        <div className="text-sm text-muted">tokens (start)</div>
      </div>
      <div className="flex items-center justify-center">
        <div className="text-2xl text-muted">→</div>
      </div>
      <div>
        <div className="text-3xl font-bold text-primary">7</div>
        <div className="text-sm text-muted">tokens (after 2 merges)</div>
      </div>
    </div>
    <div className="mt-4 pt-4 border-t border-border text-center text-sm text-secondary">
      On large text corpora, BPE typically achieves <strong>50-60% compression</strong> compared to raw bytes.
    </div>
  </div>
</Step>


<Step title="3. Implementing BPE">
  <Description>
    Now we translate the walkthrough into code.
  </Description>

  <Description>
    In the walkthrough, we used letters like `a`, `b`, `c` for clarity. But recall from Chapter 1: text is already a sequence of integers. When you encode `"hello"` as UTF-8, you get `[104, 101, 108, 108, 111]`. These byte values (0-255) form our **base vocabulary of 256 tokens**. Merged tokens get new IDs starting at **256**, then 257, 258, and so on.
  </Description>

  <h4 className="text-lg font-semibold text-primary" style={{ marginTop: 'var(--space-flow)', marginBottom: 'var(--space-atomic)' }}>Breaking Down the Algorithm</h4>

  <Description attached>
    In the walkthrough, we repeated three operations: **count** pairs, **pick** the most frequent, and **merge** it. We kept going until we decided to stop (after 2 merges). In practice, we continue until we reach a target vocabulary size. To implement the algorithm, we need three functions:
  </Description>

  <ProcessTimeline steps={[
    {
      title: "get_stats",
      description: "Counts how often each adjacent pair appears"
    },
    {
      title: "merge",
      description: "Replaces all occurrences of a pair with a new token"
    },
    {
      title: "train_bpe",
      description: "The main loop: count pairs, pick the most frequent, merge it, repeat until target vocab size"
    }
  ]} />

  <Description>
    Here's each function with its signature. You'll implement them in the challenges.
  </Description>

  <h4 className="text-lg font-semibold text-primary" style={{ marginTop: 'var(--space-flow)', marginBottom: 'var(--space-atomic)' }}>Counting Pairs</h4>

  <Description>
    `get_stats` scans a list of token IDs and counts how often each adjacent pair occurs. For example, `[1, 2, 3, 1, 2]` returns `{(1, 2): 2, (2, 3): 1, (3, 1): 1}`.
  </Description>

```python
def get_stats(ids: list[int]) -> dict[tuple, int]:
    """
    Given a list of integers, return a dictionary of counts of consecutive pairs.
    Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}
    """
    counts = {}
    # Loop through the list and look at each element and its neighbor
    # Your implementation here...
    return counts
```

  <Callout type="note" title="Challenge: The Pair Counter">
    Try implementing this yourself! Switch to the **Challenge** tab to test your solution.
  </Callout>

  <h4 className="text-lg font-semibold text-primary" style={{ marginTop: 'var(--space-flow)', marginBottom: 'var(--space-atomic)' }}>Merging a Pair</h4>

  <Description>
    `merge` takes the token list and replaces every occurrence of a target pair with a new token ID.
  </Description>

```python
def merge(ids: list[int], pair: tuple[int, int], new_id: int) -> list[int]:
    """
    In the list of integers (ids), replace all consecutive occurrences 
    of pair with the new token new_id.
    """
    newids = []
    i = 0
    while i < len(ids):
        # Check if we found the pair (and aren't at the end)
        # If match: append new_id and skip ahead by 2 (we consumed both elements)
        # If no match: append current element and move ahead by 1
        pass
    return newids
```

  <Callout type="note" title="Challenge: The Token Merger">
    This one requires careful index management. Switch to the **Challenge** tab to try it!
  </Callout>

  <h4 className="text-lg font-semibold text-primary" style={{ marginTop: 'var(--space-flow)', marginBottom: 'var(--space-atomic)' }}>The Training Loop</h4>

  <Description>
    `train_bpe` combines everything into a training loop. We start with UTF-8 bytes (vocab size 256) and merge until we reach a target size. Large models use **50,000 to 100,000 tokens**, balancing efficiency (shorter sequences) vs. memory (larger embedding table).
  </Description>

```python
def train_bpe(text: str, num_merges: int) -> tuple[list[int], dict]:
    ids = list(text.encode("utf-8"))  # Start with raw bytes
    merges = {}  # Will store our learned merge rules
    
    for i in range(num_merges):
        # Find the most frequent pair, merge it, record the rule
        # New token IDs start at 256 (after the 0-255 byte range)
        pass
    
    return ids, merges
```

  <Callout type="note" title="Challenge: The BPE Trainer">
    Put it all together! Switch to the **Challenge** tab to implement the complete training function.
  </Callout>

  <Description>
    After training, `merges` contains all the rules needed to tokenize new text, and `ids` contains your compressed training data.
  </Description>
</Step>


<Step title="4. Regex Pre-splitting">
  <Description attached>
    BPE merges pairs across the entire text without understanding word boundaries. This means punctuation can merge with words: `dog.` might become one token, while `dog ` (with space) becomes another. The same word gets tokenized differently based on what comes after it:
  </Description>

  <div className="content-attached my-6">
    <div className="text-xs font-medium text-muted uppercase tracking-wider mb-2">The Problem</div>
    <div className="p-4 bg-terminal rounded-lg border border-border">
    <div className="font-mono text-sm space-y-1 text-secondary">
      <div>"dog" + space → one pattern</div>
      <div>"dog" + period → different pattern</div>
      <div>"dog" + exclamation → yet another pattern</div>
    </div>
    <div className="mt-2 text-sm text-secondary">
      The word "dog" has no consistent representation. Its tokenization depends on surrounding characters.
    </div>
    </div>
  </div>

  <h4 className="text-lg font-semibold text-primary" style={{ marginTop: 'var(--space-flow)', marginBottom: 'var(--space-atomic)' }}>The Fix</h4>

  <Description attached>
    The fix is to split text into chunks *before* BPE runs, forcing boundaries between different character types (words, punctuation, numbers, whitespace). Modern tokenizers use **Regular Expressions** for this:
  </Description>

  <div className="my-6">
    <div className="text-xs font-medium text-muted uppercase tracking-wider mb-2">The Solution</div>
    <div className="p-4 bg-terminal rounded-lg border border-border">
    <div className="space-y-2">
      <div className="flex items-center gap-3">
        <span className="text-sm text-secondary">Input:</span>
        <span className="font-mono bg-background px-2 py-1 rounded border border-border">"dog. dog!"</span>
      </div>
      <div className="flex items-center gap-3 flex-wrap">
        <span className="text-sm text-secondary">After regex:</span>
        <div className="flex gap-1 flex-wrap">
          <span className="font-mono bg-surface px-2 py-1 rounded text-primary">"dog"</span>
          <span className="font-mono bg-surface px-2 py-1 rounded text-muted">"."</span>
          <span className="font-mono bg-surface px-2 py-1 rounded text-primary">" dog"</span>
          <span className="font-mono bg-surface px-2 py-1 rounded text-muted">"!"</span>
        </div>
      </div>
    </div>
    <div className="mt-3 text-sm text-secondary">
      Now "dog" and "." are separate chunks, so they can't merge into a single token.
    </div>
    </div>
  </div>

  <Description>
    By forcing boundaries before BPE runs, words, punctuation, and numbers stay in separate chunks. BPE then merges only within these chunks, so it can't create tokens that mix different character types.
  </Description>
</Step>


<Step title="5. Encoding & Decoding">
  <Description>
    After training, we have a set of merge rules, but we still need a way to use them. We need to **encode** new text into tokens (so the model can process it) and **decode** tokens back into text (so we can read the model's output).
  </Description>

  <h4 className="text-lg font-semibold text-primary" style={{ marginTop: 'var(--space-flow)', marginBottom: 'var(--space-atomic)' }}>Decoding: Tokens → Text</h4>

  <Description attached>
    To decode tokens back into text, we need a **vocabulary table** that maps each token ID to its byte sequence. For the base vocabulary (0-255), each token ID equals its byte value, so token 97 maps to `[97]`. For merged tokens, we combine the bytes of the two tokens that were merged: if token 256 represents the merge of `a` (97) and `t` (116), it maps to `[97, 116]`:
  </Description>

  <div className="content-attached">
    <div className="text-xs font-medium text-muted uppercase tracking-wider mb-2">The Vocabulary: Token ID → Bytes</div>
    <div className="p-4 bg-terminal rounded-lg border border-border">
    <div className="space-y-3 font-mono text-sm">
      <div>
        <div className="text-muted mb-1"># Base vocabulary: single bytes (0-255)</div>
        <div className="text-primary">vocab[97] = [97] <span className="text-muted"># "a"</span></div>
        <div className="text-primary">vocab[98] = [98] <span className="text-muted"># "b"</span></div>
        <div className="text-primary">vocab[99] = [99] <span className="text-muted"># "c"</span></div>
        <div className="text-muted">...</div>
      </div>
      <div className="pt-2 border-t border-border">
        <div className="text-muted mb-1"># Merged tokens: pre-computed byte sequences</div>
        <div className="text-primary">vocab[256] = [97, 97] <span className="text-muted"># "aa"</span></div>
        <div className="text-primary">vocab[257] = [97, 98] <span className="text-muted"># "ab"</span></div>
        <div className="text-primary">vocab[258] = [97, 97, 98] <span className="text-muted"># "aab"</span></div>
        <div className="text-muted">...</div>
      </div>
    </div>
    </div>
  </div>


  <Callout type="tip" title="Building the Vocabulary During Training">
    When you create a new merged token, immediately store its bytes by concatenating the bytes of the first token and the bytes of the second token: `vocab[new_id] = vocab[pair[0]] + vocab[pair[1]]`. After training, every token’s bytes are ready for instant lookup.
  </Callout>

  <Description attached>
    With this vocabulary, decoding becomes a dictionary lookup:
  </Description>

```python
def decode(ids: list[int], vocab: dict[int, list[int]]) -> str:
    """Convert token IDs back to text."""
    # Look up each ID in vocab, concatenate the bytes, decode as UTF-8
    pass
```

  <Callout type="note" title="Challenge: The Decoder">
    Try implementing this in the **Challenge** tab.
  </Callout>

  <h4 className="text-lg font-semibold text-primary" style={{ marginTop: 'var(--space-flow)', marginBottom: 'var(--space-atomic)' }}>Decoding Example</h4>

  <div className="my-6">
    <div className="text-xs font-medium text-muted uppercase tracking-wider mb-2">Decoding [258, 99] back to text</div>
    <div className="p-4 bg-terminal rounded-lg border border-border">
    <div className="space-y-4">
      <div className="flex items-center gap-3">
        <div className="text-sm text-muted w-28">Token IDs:</div>
        <div className="flex gap-2">
          <div className="px-3 py-2 bg-surface rounded font-mono text-sm">258</div>
          <div className="px-3 py-2 bg-surface rounded font-mono text-sm">99</div>
        </div>
      </div>
      <div className="flex items-center gap-3">
        <div className="text-sm text-muted w-28">Look up bytes:</div>
        <div className="flex gap-3 items-center">
          <div className="flex flex-col items-center">
            <div className="text-xs text-muted mb-1">vocab[258]</div>
            <div className="px-3 py-2 bg-emerald-500/10 rounded font-mono text-sm border border-emerald-500/30">[97, 97, 98]</div>
          </div>
          <div className="flex flex-col items-center">
            <div className="text-xs text-muted mb-1">vocab[99]</div>
            <div className="px-3 py-2 bg-emerald-500/10 rounded font-mono text-sm border border-emerald-500/30">[99]</div>
          </div>
        </div>
      </div>
      <div className="flex items-center gap-3">
        <div className="text-sm text-muted w-28">Concatenate:</div>
        <div className="px-3 py-2 bg-emerald-500/10 rounded font-mono text-sm border border-emerald-500/30">[97, 97, 98, 99]</div>
      </div>
      <div className="flex items-center gap-3">
        <div className="text-sm text-muted w-28">Decode UTF-8:</div>
        <div className="px-3 py-2 bg-emerald-500 text-primary rounded font-mono text-sm font-bold">"aabc"</div>
      </div>
    </div>
    </div>
  </div>

  <h4 className="text-lg font-semibold text-primary" style={{ marginTop: 'var(--space-flow)', marginBottom: 'var(--space-atomic)' }}>Encoding: Text → Tokens</h4>

  <Description>
    To encode new text, we start with raw bytes and apply each merge rule in the order it was learned, since later merge rules may depend on tokens created by earlier ones.
  </Description>

```python
def encode(text: str, merges: dict[tuple[int, int], int]) -> list[int]:
    """Tokenize text using learned merge rules."""
    ids = list(text.encode("utf-8"))  # Start with raw bytes
    
    # Apply each merge rule in the order it was learned
    # Hint: iterate through merges.items() and use your merge function
    pass
```
  <Description>
    Since Python dictionaries preserve insertion order, we can iterate through `merges` and apply each rule in the order it was learned. To see why order matters, consider: if we learned `(116,104)→256` first and `(256,101)→257` second, the second rule looks for the pair `(256, 101)` in the token sequence, but token 256 doesn't exist yet. The rule finds no match and gets skipped.
  </Description>

  {/* Order comparison visual */}
  <div className="my-8">
    <p className="text-sm text-secondary mb-5">
      For example, encoding "the" with merge rules <code className="font-mono text-xs bg-surface px-1.5 py-0.5 rounded">(116,104)→256</code> <span className="text-muted text-xs">(t,h→th)</span> and <code className="font-mono text-xs bg-surface px-1.5 py-0.5 rounded">(256,101)→257</code> <span className="text-muted text-xs">(th,e→the)</span>:
    </p>
    
    <div className="grid grid-cols-1 md:grid-cols-2 gap-4">
      {/* Valid Path */}
      <div className="p-4 bg-background rounded-lg border border-emerald-500/20">
        <div className="flex items-center gap-2 mb-4">
          <span className="text-emerald-400">✓</span>
          <span className="text-sm font-semibold text-primary">Correct Order: #1 then #2</span>
        </div>
        
        <div className="space-y-4">
          {/* Step 1 */}
          <div className="pb-3 border-b border-border">
            <div className="text-xs text-muted mb-1.5 font-medium">Step 1 · Apply merge #1: (116,104)→256</div>
            <div className="font-mono text-sm text-secondary">
              [116, 104, 101] <span className="text-muted">→</span> [<span className="text-emerald-400 font-bold">256</span>, 101] <span className="text-muted text-xs">th, e</span>
            </div>
          </div>
          
          {/* Step 2 */}
          <div>
            <div className="text-xs text-muted mb-1.5 font-medium">Step 2 · Apply merge #2: (256,101)→257</div>
            <div className="font-mono text-sm text-secondary">
              [256, 101] <span className="text-muted">→</span> [<span className="text-emerald-400 font-bold">257</span>] <span className="text-muted text-xs">the</span>
            </div>
          </div>
        </div>
        
        {/* Result */}
        <div className="mt-4 pt-3 border-t border-border">
          <span className="text-xs font-semibold">Result: [257]</span>
          <span className="text-xs text-muted ml-2">(1 token)</span>
        </div>
      </div>
      
      {/* Invalid Path */}
      <div className="p-4 bg-background rounded-lg border border-rose-500/20">
        <div className="flex items-center gap-2 mb-4">
          <span className="text-rose-400">✗</span>
          <span className="text-sm font-semibold text-primary">Wrong Order: #2 then #1</span>
        </div>
        
        <div className="space-y-4">
          {/* Step 1 */}
          <div className="pb-3 border-b border-border">
            <div className="text-xs text-muted mb-1.5 font-medium">Step 1 · Apply merge #2: (256,101)→257</div>
            <div className="font-mono text-sm text-secondary">
              [116, 104, 101] <span className="text-muted">→</span> [116, 104, 101] <span className="text-muted text-xs">(no 256 yet, skipped!)</span>
            </div>
          </div>
          
          {/* Step 2 */}
          <div>
            <div className="text-xs text-muted mb-1.5 font-medium">Step 2 · Apply merge #1: (116,104)→256</div>
            <div className="font-mono text-sm text-secondary">
              [116, 104, 101] <span className="text-muted">→</span> [<span className="text-rose-400 font-bold">256</span>, 101] <span className="text-muted text-xs">th, e</span>
            </div>
          </div>
        </div>
        
        {/* Result */}
        <div className="mt-4 pt-3 border-t border-border">
          <span className="text-xs font-semibold">Result: [256, 101]</span>
          <span className="text-xs text-muted ml-2">(2 tokens, missed a merge)</span>
        </div>
      </div>
    </div>
  </div>

  <h4 className="text-lg font-semibold text-primary" style={{ marginTop: 'var(--space-flow)', marginBottom: 'var(--space-atomic)' }}>Encoding Example</h4>

  <div className="my-6">
    <div className="text-xs font-medium text-muted uppercase tracking-wider mb-2">Encoding "the" to tokens</div>
    <div className="p-4 bg-terminal rounded-lg border border-border">
    <div className="space-y-3 font-mono text-sm">
      <div className="flex items-center gap-3">
        <span className="text-muted w-32">Start:</span>
        <span className="text-primary">[116, 104, 101]</span>
        <span className="text-muted text-xs">t, h, e</span>
      </div>
      <div className="flex items-center gap-3">
        <span className="text-muted w-32">Merge (116,104):</span>
        <span className="text-primary">[<span className="text-emerald-400 font-bold">256</span>, 101]</span>
        <span className="text-muted text-xs">th, e</span>
      </div>
      <div className="flex items-center gap-3">
        <span className="text-muted w-32">Merge (256,101):</span>
        <span className="text-primary">[<span className="text-emerald-400 font-bold">257</span>]</span>
        <span className="text-muted text-xs">the</span>
      </div>
      <div className="flex items-center gap-3 pt-2 border-t border-border">
        <span className="text-muted w-32">Result:</span>
        <span className="text-emerald-400 font-bold">[257]</span>
        <span className="text-muted text-xs">← 3 bytes compressed to 1 token</span>
      </div>
    </div>
    </div>
  </div>

  <Callout type="note" title="Challenge: The Encoder">
    Try implementing this in the **Challenge** tab.
  </Callout>
</Step>

<Step title="6. Recap">
  <Description>
    Let's step back and see the complete flow from text to token IDs.
  </Description>

  <ProcessTimeline steps={[
    {
      title: "Raw Text",
      data: '"Hello, World!"',
      description: "The original input string."
    },
    {
      title: "Regex Split",
      data: '["Hello", ",", " World", "!"]',
      description: "Pre-split into chunks. Punctuation stays separate."
    },
    {
      title: "UTF-8 Encoding",
      data: "[72, 101, 108, 108, 111], [44], [32, 87, 111, 114, 108, 100], [33]",
      description: "Each chunk becomes a list of bytes."
    },
    {
      title: "BPE Merging",
      description: "Apply learned merge rules to compress bytes into tokens."
    },
    {
      title: "Token IDs",
      data: "[15496, 11, 2159, 0]",
      description: "The final sequence of integers."
    }
  ]} />

  <Callout type="success" title="Summary">
    * **Tokenization** converts text into a sequence of integer IDs
    * **BPE** learns merge rules by iteratively combining frequent byte pairs
    * **Regex pre-splitting** keeps words and punctuation in separate chunks
    * **Decoding** converts tokens back to text by looking up each token's bytes
    * **Encoding** converts text to tokens by applying merge rules in order they were learned
  </Callout>

  <Description>
    We now have a sequence of token IDs. But these are just identifiers. They don't tell us anything about what the tokens mean. In the next chapter, we'll see how **Embeddings** turn each token ID into a vector that captures meaning.
  </Description>
</Step>
