---
title: "Residuals & Normalization"
step: 8
description: "How residual connections and layer normalization enable deep networks."
---

<div className="flex flex-col" style={{ gap: 'var(--space-flow)' }}>
<Description noMargin>
  We now have all the core components, from embeddings and positional encoding to multi-head attention and feed-forward networks. But a single pass through one attention layer and one FFN isn't enough to capture the full complexity of language. So, the model needs to refine its representations through multiple rounds of processing, and the way to do this is **stacking**. By repeating the same attention-then-FFN block many times in sequence, the model develops increasingly abstract patterns at each level.
</Description>

<Description>
  GPT-2 stacks this block **12 times**, and GPT-3 stacks it **96 times**. That kind of depth is what gives these models their power, but it also creates serious training problems. This chapter explores what goes wrong and introduces the two techniques that fix it, **residual connections** and **layer normalization**.
</Description>
</div>


<Step title="Why Deep Stacks Break">
  <Description>
    Each layer applies its own transformation, and stacking means those transformations compound through every subsequent one.
  </Description>

  <Description>
    During backpropagation, the gradient flows backward through every layer, multiplied at each one by that layer's local derivatives. In GPT-2, that's twelve multiplications in sequence. If those multipliers average below 1.0, the gradient shrinks at every step until early layers receive almost no learning signal, a problem known as **vanishing gradients**. If the multipliers average above 1.0, the gradient grows instead, eventually destabilizing training until it diverges. This is called **exploding gradients**.
  </Description>

  <div className="my-6 p-4 bg-[#121212] rounded-lg border border-zinc-800 font-mono text-sm space-y-3">
    <div>
      <div className="text-zinc-400">0.7 × 0.7 × 0.7 × ... = 0.7<sup>12</sup> = <strong className="text-zinc-200">0.014</strong></div>
      <div className="text-xs text-zinc-500 mt-1">A multiplier of 0.7 leaves only 1.4% of the gradient after 12 layers</div>
    </div>
    <div className="border-t border-zinc-800" />
    <div>
      <div className="text-zinc-400">1.3 × 1.3 × 1.3 × ... = 1.3<sup>12</sup> = <strong className="text-zinc-200">23.3</strong></div>
      <div className="text-xs text-zinc-500 mt-1">A multiplier of 1.3 amplifies the gradient 23× over 12 layers</div>
    </div>
  </div>

  <Description>
    The forward pass also suffers from a related problem. Information from early layers must pass through every subsequent transformation to reach the output, and each layer's transformation can distort or overwrite this information. Over enough layers, the representations that earlier layers produced are increasingly likely to be degraded or lost entirely.
  </Description>
</Step>


<Step title="Residual Connections">
  <Description>
    Both problems trace back to the same structural cause. The layers form a sequential chain where each one receives the previous layer's output, transforms it into a new vector, and passes that vector as the next layer's entire input. This means whatever an early layer produces gets transformed by the next layer, then again by the layer after that, and so on through the rest of the stack. The information an early layer encodes can only reach the final output by surviving this entire sequence of transformations, and any transformation that distorts something useful passes that distortion forward to every layer that follows.
  </Description>

  <Description>
    The gradient during the backward pass travels this same chain in reverse. For an early layer to update its weights, the loss signal must flow backward from the output through every intermediate layer, getting multiplied at each one by that layer's local derivatives. Over enough layers, these repeated multiplications might cause gradients to vanish or explode.
  </Description>

  <Description>
    Residual connections solve both problems by adding each layer's output to its input instead of replacing it:
  </Description>

  <div className="my-6 p-5 bg-surface rounded-lg border border-border">
    <div className="text-center font-mono text-lg text-primary">
      output = sublayer(x) + x
    </div>
  </div>

  <div className="my-6 flex justify-center">
    <svg viewBox="0 0 420 78" className="w-full" style={{ maxWidth: "420px", display: "block" }}>
      <line x1="42" y1="18" x2="301" y2="18" stroke="#3F3F46" strokeWidth="1.5" />

      <rect x="8" y="5" width="34" height="26" rx="4" fill="#18181B" stroke="#27272A" strokeWidth="1" />
      <text x="25" y="22" textAnchor="middle" fill="#A1A1AA" fontSize="11" fontFamily="var(--font-mono)">x</text>

      <path d="M 75 18 L 75 54 L 105 54" fill="none" stroke="#71717A" strokeWidth="1.5" />
      <rect x="105" y="41" width="120" height="26" rx="4" fill="rgba(161,161,170,0.08)" stroke="rgba(161,161,170,0.2)" strokeWidth="1" />
      <text x="165" y="58" textAnchor="middle" fill="#A1A1AA" fontSize="10" fontFamily="var(--font-mono)">sublayer(x)</text>
      <path d="M 225 54 L 310 54 L 310 34" fill="none" stroke="#71717A" strokeWidth="1.5" />
      <polygon points="306.5,34 310,27 313.5,34" fill="#71717A" />

      <circle cx="310" cy="18" r="9" fill="#18181B" stroke="#71717A" strokeWidth="1.5" />
      <text x="310" y="22" textAnchor="middle" fill="#D4D4D8" fontSize="12" fontWeight="bold">+</text>

      <line x1="319" y1="18" x2="370" y2="18" stroke="#3F3F46" strokeWidth="1.5" />
      <polygon points="364,14.5 372,18 364,21.5" fill="#3F3F46" />
      <text x="382" y="22" textAnchor="start" fill="#A1A1AA" fontSize="10" fontFamily="var(--font-mono)">output</text>
    </svg>
  </div>

  <Description>
    The input x bypasses the sublayer entirely while the sublayer's output gets added to it. So the sublayer only needs to learn what to add rather than producing the full output from scratch. The change the sublayer learns to produce is called the **residual**, which is where the name comes from.
  </Description>

  <Description>
    Starting from the token embeddings, information now flows along a pathway that runs through every layer in the model. Each layer taps into this pathway and adds its contribution, and because contributions are added rather than replaced, information from earlier layers can persist all the way to the end. This pathway is called the **residual stream**.
  </Description>

  <div className="my-6 flex justify-center overflow-x-auto">
    <svg viewBox="0 0 580 115" className="w-full" style={{ maxWidth: "580px", display: "block" }}>
      <line x1="30" y1="84" x2="550" y2="84" stroke="#3F3F46" strokeWidth="2" />

      <rect x="8" y="69" width="44" height="30" rx="4" fill="#18181B" stroke="#27272A" strokeWidth="1" />
      <text x="30" y="88" textAnchor="middle" fill="#A1A1AA" fontSize="11" fontFamily="var(--font-mono)">x</text>

      {/* attn₁ */}
      <line x1="86" y1="84" x2="86" y2="35" stroke="#3B82F6" strokeWidth="1.5" />
      <polygon points="82.5,35 86,28 89.5,35" fill="#3B82F6" />
      <rect x="75" y="4" width="56" height="24" rx="4" fill="rgba(59,130,246,0.08)" stroke="rgba(59,130,246,0.3)" strokeWidth="1" />
      <text x="103" y="20" textAnchor="middle" fill="#60A5FA" fontSize="10" fontFamily="var(--font-mono)">attn₁</text>
      <line x1="122" y1="28" x2="122" y2="67" stroke="#3B82F6" strokeWidth="1.5" />
      <polygon points="118.5,67 122,74 125.5,67" fill="#3B82F6" />
      <circle cx="122" cy="84" r="9" fill="#18181B" stroke="#3B82F6" strokeWidth="1.5" />
      <text x="122" y="88" textAnchor="middle" fill="#60A5FA" fontSize="12" fontWeight="bold">+</text>

      {/* ffn₁ */}
      <line x1="183" y1="84" x2="183" y2="35" stroke="#F59E0B" strokeWidth="1.5" />
      <polygon points="179.5,35 183,28 186.5,35" fill="#F59E0B" />
      <rect x="172" y="4" width="56" height="24" rx="4" fill="rgba(245,158,11,0.08)" stroke="rgba(245,158,11,0.3)" strokeWidth="1" />
      <text x="200" y="20" textAnchor="middle" fill="#F59E0B" fontSize="10" fontFamily="var(--font-mono)">ffn₁</text>
      <line x1="219" y1="28" x2="219" y2="67" stroke="#F59E0B" strokeWidth="1.5" />
      <polygon points="215.5,67 219,74 222.5,67" fill="#F59E0B" />
      <circle cx="219" cy="84" r="9" fill="#18181B" stroke="#F59E0B" strokeWidth="1.5" />
      <text x="219" y="88" textAnchor="middle" fill="#F59E0B" fontSize="12" fontWeight="bold">+</text>

      {/* attn₂ */}
      <line x1="275" y1="84" x2="275" y2="35" stroke="#3B82F6" strokeWidth="1.5" />
      <polygon points="271.5,35 275,28 278.5,35" fill="#3B82F6" />
      <rect x="264" y="4" width="56" height="24" rx="4" fill="rgba(59,130,246,0.08)" stroke="rgba(59,130,246,0.3)" strokeWidth="1" />
      <text x="292" y="20" textAnchor="middle" fill="#60A5FA" fontSize="10" fontFamily="var(--font-mono)">attn₂</text>
      <line x1="311" y1="28" x2="311" y2="67" stroke="#3B82F6" strokeWidth="1.5" />
      <polygon points="307.5,67 311,74 314.5,67" fill="#3B82F6" />
      <circle cx="311" cy="84" r="9" fill="#18181B" stroke="#3B82F6" strokeWidth="1.5" />
      <text x="311" y="88" textAnchor="middle" fill="#60A5FA" fontSize="12" fontWeight="bold">+</text>

      <text x="365" y="89" textAnchor="middle" fill="#71717A" fontSize="15">···</text>

      {/* ffnₙ */}
      <line x1="417" y1="84" x2="417" y2="35" stroke="#F59E0B" strokeWidth="1.5" />
      <polygon points="413.5,35 417,28 420.5,35" fill="#F59E0B" />
      <rect x="406" y="4" width="56" height="24" rx="4" fill="rgba(245,158,11,0.08)" stroke="rgba(245,158,11,0.3)" strokeWidth="1" />
      <text x="434" y="20" textAnchor="middle" fill="#F59E0B" fontSize="10" fontFamily="var(--font-mono)">ffnₙ</text>
      <line x1="453" y1="28" x2="453" y2="67" stroke="#F59E0B" strokeWidth="1.5" />
      <polygon points="449.5,67 453,74 456.5,67" fill="#F59E0B" />
      <circle cx="453" cy="84" r="9" fill="#18181B" stroke="#F59E0B" strokeWidth="1.5" />
      <text x="453" y="88" textAnchor="middle" fill="#F59E0B" fontSize="12" fontWeight="bold">+</text>

      <rect x="504" y="69" width="48" height="30" rx="4" fill="#18181B" stroke="#27272A" strokeWidth="1" />
      <text x="528" y="88" textAnchor="middle" fill="#A1A1AA" fontSize="11" fontFamily="var(--font-mono)">out</text>

      <text x="290" y="110" textAnchor="middle" fill="#52525B" fontSize="10" fontFamily="var(--font-mono)">residual stream</text>
    </svg>
  </div>

  <Description>
    Residual connections also provide a direct path for gradients in the backward direction. At each addition point, the gradient flows to both inputs of the sum. One path carries it back through the layer. The other bypasses the layer entirely, and the gradient passes through with a multiplier of exactly 1. This guarantees that every layer receives a usable learning signal from the output, regardless of what happens inside the layers themselves. These bypass paths also smooth the loss landscape (Li et al., 2018), making optimization reliable even at considerable depth.
  </Description>

</Step>


<Step title="Layer Normalization">
  <Description>
    In a deep network, the distribution of each layer's inputs changes during training as the parameters of earlier layers update. This forces each layer to continuously readjust to new input distributions rather than learning a stable mapping. Layer normalization addresses this by standardizing each layer's inputs to a consistent scale, neutralizing the distribution shifts that earlier layers introduce.
  </Description>

  <Description>
    The core idea is to reduce each token's vector to just its underlying structure by subtracting the mean across the `d_model` values to remove any overall offset, and dividing by the standard deviation to remove differences in scale.
  </Description>

  <div className="my-6 p-5 bg-surface rounded-lg border border-border space-y-3">
    <div className="text-center font-mono text-lg text-primary">
      LayerNorm(x) = γ ⊙ <span className="inline-block" style={{ verticalAlign: "middle" }}><span className="inline-flex flex-col items-center"><span className="border-b border-current px-2">x − μ</span><span className="px-2">√(σ² + ε)</span></span></span> + β
    </div>
    <div className="text-center text-sm text-muted">
      ⊙ = element-wise multiply&ensp;&ensp;ε = small constant for numerical stability&ensp;&ensp;γ = learned scale&ensp;&ensp;β = learned shift
    </div>
  </div>

  <Description>
    The small constant ε (typically 10⁻⁵) prevents division by zero when the variance is near zero. To make this concrete, consider a token whose vector has `d_model` = 4, with values [2, 4, 6, 8]:
  </Description>

  <div className="my-6 p-4 bg-[#121212] rounded-lg border border-zinc-800 font-mono text-sm space-y-3">
    <div>
      <span className="text-zinc-500">Step 1: Mean</span>
      <div className="text-zinc-500 mt-2 text-xs flex items-center gap-1.5">
        μ = <span className="inline-flex flex-col items-center"><span className="border-b border-zinc-600 px-1.5">x₁ + x₂ + ... + x_d</span><span className="px-1.5">d</span></span>
      </div>
      <div className="text-zinc-300 mt-2">μ = (2 + 4 + 6 + 8) / 4 = <strong>5.0</strong></div>
    </div>
    <div className="border-t border-zinc-800" />
    <div>
      <span className="text-zinc-500">Step 2: Variance</span>
      <div className="text-zinc-500 mt-2 text-xs flex items-center gap-1.5">
        σ² = <span className="inline-flex flex-col items-center"><span className="border-b border-zinc-600 px-1.5">(x₁ − μ)² + (x₂ − μ)² + ... + (x_d − μ)²</span><span className="px-1.5">d</span></span>
      </div>
      <div className="text-zinc-300 mt-2">σ² = ((2−5)² + (4−5)² + (6−5)² + (8−5)²) / 4 = (9+1+1+9) / 4 = <strong>5.0</strong></div>
    </div>
    <div className="border-t border-zinc-800" />
    <div>
      <span className="text-zinc-500">Step 3: Normalize</span>
      <div className="text-zinc-500 mt-2 text-xs flex items-center gap-1.5">
        x̂ = <span className="inline-flex flex-col items-center"><span className="border-b border-zinc-600 px-1.5">x − μ</span><span className="px-1.5">√(σ² + ε)</span></span>
      </div>
      <div className="text-zinc-300 mt-2">x̂ = [(2−5), (4−5), (6−5), (8−5)] / √5.0</div>
      <div className="text-zinc-300 mt-1">= [−3, −1, 1, 3] / 2.236 = [<strong>−1.342, −0.447, 0.447, 1.342</strong>]</div>
    </div>
    <div className="border-t border-zinc-800" />
    <div>
      <span className="text-zinc-500">Step 4: Scale and shift</span>
      <div className="text-zinc-500 mt-2 text-xs">LayerNorm(x) = γ ⊙ x̂ + β</div>
      <div className="text-zinc-300 mt-2">[γ₁, γ₂, γ₃, γ₄] ⊙ [−1.342, −0.447, 0.447, 1.342] + [β₁, β₂, β₃, β₄]</div>
      <div className="text-xs text-zinc-500 mt-1">γ = [γ₁, γ₂, ..., γ_d] and β = [β₁, β₂, ..., β_d] are learned vectors, one value per dimension</div>
    </div>
  </div>

  <Description>
    The learned parameters γ (scale) and β (shift), each of size `d_model`, let the network adjust or even undo the normalization for each dimension independently. Since different dimensions encode different features, they may need different scales. The difference from having no normalization at all is that the final output range becomes a deliberate, learned choice rather than uncontrolled drift from earlier layers.
  </Description>

</Step>


<Step title="Pre-Norm vs Post-Norm">
  <Description>
    We now have residual connections to preserve information across depth and layer normalization to stabilize magnitudes. The final step is determining how to arrange these two mechanisms around each sublayer. The specific ordering of these operations directly affects how stably the model trains.
  </Description>

  <Description>
    The original Transformer architecture (Vaswani et al., 2017) applies normalization **after** the residual addition. This arrangement is known as **Post-Norm**:
  </Description>

  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800 font-mono text-sm space-y-1">
    <div className="text-zinc-500"># Post-Norm</div>
    <div className="text-zinc-300">x = LayerNorm(x + sublayer(x))</div>
  </div>

  <Description>
    Most modern language models use the alternative approach of applying normalization **before** the sublayer. By placing LayerNorm directly inside the residual branch, we get what is called **Pre-Norm**:
  </Description>

  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800 font-mono text-sm space-y-1">
    <div className="text-zinc-500"># Pre-Norm</div>
    <div className="text-zinc-300">x = x + sublayer(LayerNorm(x))</div>
  </div>

  <Description>
    The fundamental difference between these approaches is how they affect the identity path. In Post-Norm, the combined output passes through LayerNorm before moving to the next layer. This introduces a nonlinear operation directly on the identity path, which modifies the gradient and disrupts the clean information flow that residual connections are designed to provide.
  </Description>

  <Description>
    Pre-Norm avoids this issue by keeping the residual stream entirely clean. Since LayerNorm is isolated within the sublayer branch, the identity path remains an uninterrupted chain of addition operations. The input vector flows straight through without any additional transformation, ensuring that gradients can flow backward with a consistent multiplier of 1 at every step.
  </Description>

  {/* Pre-Norm vs Post-Norm identity path visual */}
  <div className="my-6 grid grid-cols-1 sm:grid-cols-2 gap-4">
    <div className="p-4 bg-[#121212] rounded-lg border border-zinc-800">
      <div className="text-center text-sm font-semibold text-zinc-400 mb-3">Post-Norm</div>
      <div className="flex flex-col items-center font-mono text-xs space-y-2">
        <div className="text-zinc-500">x</div>
        <div className="text-zinc-600">↓ ─────────╮</div>
        <div className="text-zinc-400">sublayer&ensp;&ensp;&ensp;│</div>
        <div className="text-zinc-600">↓ ─────────╯</div>
        <div className="text-zinc-400">+ (add)</div>
        <div className="text-zinc-600">↓</div>
        <div className="px-2 py-1 rounded border border-red-900/50 bg-red-950/30 text-red-400">LayerNorm</div>
        <div className="text-zinc-600">↓</div>
        <div className="text-zinc-500">out</div>
      </div>
      <div className="text-center text-xs text-red-400/70 mt-3">identity path passes through LayerNorm</div>
    </div>
    <div className="p-4 bg-[#121212] rounded-lg border border-zinc-800">
      <div className="text-center text-sm font-semibold text-zinc-400 mb-3">Pre-Norm</div>
      <div className="flex flex-col items-center font-mono text-xs space-y-2">
        <div className="text-zinc-500">x</div>
        <div className="text-zinc-600">↓ ─────────╮</div>
        <div className="px-2 py-1 rounded border border-emerald-900/50 bg-emerald-950/30 text-emerald-400">LayerNorm</div>
        <div className="text-zinc-600">↓&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;│</div>
        <div className="text-zinc-400">sublayer&ensp;&ensp;&ensp;│</div>
        <div className="text-zinc-600">↓ ─────────╯</div>
        <div className="text-zinc-400">+ (add)</div>
        <div className="text-zinc-600">↓</div>
        <div className="text-zinc-500">out</div>
      </div>
      <div className="text-center text-xs text-emerald-400/70 mt-3">identity path is completely clean</div>
    </div>
  </div>

  <Description>
    As a result, Post-Norm models can be difficult to train at scale. They typically require careful learning rate warmup schedules and can diverge entirely in deeper networks. Pre-Norm avoids these complications and trains much more stably. This is why it quickly became the standard approach for nearly all large language models.
  </Description>

  <Description>
    Because Pre-Norm applies normalization before each sublayer, the final sublayer in the very last transformer block adds its output into the residual stream unnormalized. To account for this, Pre-Norm architectures include one final layer normalization step at the end of the model to stabilize these values. In many codebases, including GPT-2, this is referred to as `ln_f`.
  </Description>
</Step>

<Description>
  With residual connections preserving our information and layer normalization stabilizing the scale, we finally have all the pieces we need. In the next chapter, we will combine embeddings, attention, feed-forward networks, and these two stabilization mechanisms to build the repeatable **Transformer block** that forms the core of the architecture.
</Description>

