---
title: Embeddings & Positional Encoding
step: 1
description: Representing words as vectors and injecting order into the sequence.
---

<SplitLayout>
  <Step>
    <Description>
      ## The Language of Vectors

      Before a neural network can process text, we must convert discrete words into continuous vectors. In this step, we will build the `InputEmbedding` layer.

      ### What we'll build
      - A lookup table mapping token IDs to vectors.
      - A positional encoding module to give the model a sense of order.
      
      > [!NOTE]
      > This content is under construction.
    </Description>
  </Step>
</SplitLayout>
