---
title: Multi-Head Attention
step: 3
description: Parallelizing attention to capture different types of relationships.
---

<SplitLayout>
  <Step>
    <Description>
      ## Parallel Perspectives

      One attention head isn't enough. By running multiple heads in parallel, the model can focus on different aspects of the language (syntax, semantics, etc.) simultaneously.

      ### What we'll build
      - The `MultiHeadAttention` module.
      - Logic to split and concatenate heads.
      
      > [!NOTE]
      > This content is under construction.
    </Description>
  </Step>
</SplitLayout>
