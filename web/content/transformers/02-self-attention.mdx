---
title: The Self-Attention Mechanism
step: 2
description: The heart of the Transformer. Query, Key, and Value matrices explained.
---

<SplitLayout>
  <Step>
    <Description>
      ## The Heart of the Transformer

      Self-attention allows the model to weigh the importance of different words in the sequence relative to each other.

      ### What we'll build
      - The `Query`, `Key`, and `Value` linear projections.
      - The scaled dot-product attention mechanism.
      
      > [!NOTE]
      > This content is under construction.
    </Description>
  </Step>
</SplitLayout>
