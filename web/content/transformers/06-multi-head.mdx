---
title: "Multi-Head Attention"
description: "Parallelizing attention to capture different types of relationships."
step: 6
challenge: "Implement the MultiHeadAttention module."
---

# Multi-Head Attention

One head is good, but multiple heads are better.
Maybe one head focuses on grammar ("is" follows "he"), while another focuses on context ("bank" relates to "money").

## How to Split
We don't just make $h$ copies of the attention. We **split** the embedding dimension.
If $d_{model} = 512$ and we want 8 heads, each head gets $d_k = 64$ dimensions.

## The Process
1.  Linear projection to get $Q, K, V$.
2.  Split into $h$ heads.
3.  Apply Scaled Dot-Product Attention to each head in parallel.
4.  Concatenate the results.
5.  Final linear projection.

## Your Task
Implement `MultiHeadAttention` class.
