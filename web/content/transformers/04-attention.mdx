---
title: "Single-Head Attention"
description: "The core mechanism: Query, Key, and Value."
step: 4
challenge: "Implement scaled dot-product attention."
---

# Single-Head Attention

This is the engine of the Transformer. It allows tokens to "talk" to each other.

## The Mechanism
Every token produces three vectors:
1.  **Query ($Q$)**: "What am I looking for?"
2.  **Key ($K$)**: "What do I contain?"
3.  **Value ($V$)**: "If you match me, here's my information."

## The Math

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

We calculate the dot product between every Query and every Key to get a similarity score. We scale it, softmax it (to get probabilities), and then use those probabilities to weight the Values.

## Your Task
Implement the `attention` function from scratch using `torch.matmul`.
