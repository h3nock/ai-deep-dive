---
title: Residual Connections
step: 8
description: Enabling deep networks to learn by letting gradients flow.
---

<SplitLayout>
  <Step>
    <Description>
      ## The Highway for Gradients

      As networks get deeper, gradients vanish. Residual connections (skip connections) allow gradients to flow through the network unimpeded.

      ### The Concept
      Instead of learning $y = f(x)$, we learn $y = f(x) + x$. In the Transformer, we apply this around the Attention and FFN blocks, followed by LayerNorm.

      ### The Architecture
      $$ x = \text{LayerNorm}(x + \text{Attention}(x)) $$
      $$ x = \text{LayerNorm}(x + \text{FFN}(x)) $$
      *(Note: This is the "Post-LN" variant. We will also discuss "Pre-LN".)*

      ### Challenge
      Implement a `ResidualBlock` wrapper or integrate the add-and-norm logic into your main block class.
    </Description>
  </Step>
</SplitLayout>
