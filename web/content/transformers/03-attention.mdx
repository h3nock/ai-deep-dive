---
title: Scaled Dot-Product Attention
step: 3
description: The core mathematical operation of the Transformer.
---

<SplitLayout>
  <Step>
    <Description>
      ## The Core Mechanism

      Attention allows the model to "focus" on relevant parts of the input sequence.

      ### The Concept
      We compute a weighted sum of Values ($V$), where weights are determined by the compatibility of Queries ($Q$) and Keys ($K$).

      ### The Math
      $$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$

      ### Challenge
      Implement the `attention` function. It takes Q, K, V tensors and returns the weighted sum. Don't forget the scaling factor $\frac{1}{\sqrt{d_k}}$!
    </Description>
  </Step>
</SplitLayout>
