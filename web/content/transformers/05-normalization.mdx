---
title: Layer Norm & Residuals
step: 5
description: The stabilization techniques that make deep learning possible.
---

<SplitLayout>
  <Step>
    <Description>
      ## Stability at Scale

      Deep networks are notoriously hard to train. Residual connections (skip connections) and Layer Normalization are the secret sauce that allows Transformers to be deep.

      ### What we'll build
      - The `LayerNorm` module from scratch.
      - The `Add & Norm` block structure.
      
      > [!NOTE]
      > This content is under construction.
    </Description>
  </Step>
</SplitLayout>
