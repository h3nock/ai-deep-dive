---
title: "Feed-Forward Networks"
description: "The 'brain' of the block: adding non-linearity and computation."
step: 8
challenge: "Implement the Position-wise Feed-Forward Network."
---

# Feed-Forward Networks (FFN)

Attention mixes information *between* tokens. The FFN processes information *within* each token.
This is where the model "thinks" about what it has gathered.

## Structure
It's a simple MLP (Multi-Layer Perceptron) applied to every token independently.

1.  Linear: $d_{model} \to 4 \times d_{model}$
2.  Activation: GELU (Gaussian Error Linear Unit)
3.  Linear: $4 \times d_{model} \to d_{model}$

## Your Task
Implement the `FeedForward` module.
