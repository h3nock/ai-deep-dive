---
title: "Layer Normalization"
description: "Stabilizing training by normalizing across the feature dimension."
step: 7
challenge: "Implement LayerNorm."
---

# Layer Normalization

To train deep networks, we need to keep the numbers stable.
**Layer Normalization** ensures that for every token, the features have a mean of 0 and a variance of 1.

## The Formula

$$
y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta
$$

We also learn two parameters, $\gamma$ (scale) and $\beta$ (shift), so the model can "undo" the normalization if it needs to.

## Your Task
Implement `LayerNorm` (or use `nn.LayerNorm` and understand what it does).
