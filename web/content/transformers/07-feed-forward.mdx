---
title: Feed-Forward Networks
step: 7
description: The processing engine of the Transformer.
---

<SplitLayout>
  <Step>
    <Description>
      ## The MLP Block

      Attention mixes information between tokens. The Feed-Forward Network (FFN) processes information *within* each token independently.

      ### The Concept
      It's a simple two-layer MLP that expands the dimension (usually $4 \times d_{model}$) and then projects it back.

      ### The Math
      $$ \text{FFN}(x) = \text{GELU}(xW_1 + b_1)W_2 + b_2 $$

      ### Challenge
      Implement `FeedForward`. Use `nn.Linear` for the projections and `GELU` (Gaussian Error Linear Unit) as the activation function.
    </Description>
  </Step>
</SplitLayout>
