---
title: "Residual Connections"
description: "The gradient superhighway: enabling deep network training."
step: 9
challenge: "Add skip connections to your architecture."
---

# Residual Connections

Deep networks suffer from vanishing gradients. The signal gets lost as it travels back through many layers.
**Residual Connections** (Skip Connections) provide a direct path for the gradient to flow.

$$
\text{Output} = x + \text{Layer}(x)
$$

## In the Transformer
We apply this around every sub-layer:
1.  $x = x + \text{Attention}(x)$
2.  $x = x + \text{FFN}(x)$

(Usually with LayerNorm applied before or after).

## Your Task
Update your modules to include these addition operations.
