---
title: "Encoder & Decoder Blocks"
description: "Building the two fundamental units: Self-Attention vs. Cross-Attention."
step: 10
challenge: "Implement both EncoderBlock and DecoderBlock."
---

# Encoder & Decoder Blocks

In a full Transformer (like the original "Attention Is All You Need"), the architecture is composed of two distinct types of processing blocks.

## 1. The Encoder Block
Used for understanding the input (e.g., the Python code).
- **Structure**: `Self-Attention` -> `LayerNorm` -> `FFN` -> `LayerNorm`.
- **Masking**: None (it can see the whole input).

## 2. The Decoder Block
Used for generating the output (e.g., the JavaScript code).
- **Structure**: `Masked Self-Attention` -> `LayerNorm` -> **`Cross-Attention`** -> `LayerNorm` -> `FFN` -> `LayerNorm`.
- **Masking**: Causal (can't see future output tokens).

## Cross-Attention?
This is new! In Cross-Attention:
- **Queries** come from the **Decoder** (what we are generating).
- **Keys & Values** come from the **Encoder** (what we are translating).

This allows the decoder to "look back" at the source code to decide what to write next.

## Your Task
Implement both `EncoderBlock` and `DecoderBlock`.
