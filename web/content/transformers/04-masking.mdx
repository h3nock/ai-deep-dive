---
title: Causal Masking
step: 4
description: Preventing the model from peeking into the future.
---

<SplitLayout>
  <Step>
    <Description>
      ## Preserving Causality

      In a GPT (Decoder-only) model, we are predicting the *next* token. The model cannot see future tokens during training.

      ### The Concept
      We apply a mask to the attention scores (before softmax). We set the scores of future positions to $-\infty$, so their probability becomes 0 after softmax.

      ### The Implementation
      Create a lower-triangular matrix of 1s. Use `masked_fill` to apply $-\infty$ where the mask is 0.

      ### Challenge
      Modify your attention function to accept a `mask` argument. Create a `causal_mask` function that generates the triangular mask.
    </Description>
  </Step>
</SplitLayout>
