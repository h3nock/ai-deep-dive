---
title: "Tokenization"
description: "Turn raw text into a sequence of integers: the first step of understanding."
step: 1
challenge: "Implement a simple character-level tokenizer."
---

# Tokenization

Before we can do any math, we need to turn text into numbers.

## The Problem
Computers don't understand "Hello". They understand `01001000...`.
In Deep Learning, we want to map meaningful units of text to unique integers (IDs).

## Approaches

1.  **Character Level**: 'a' -> 1, 'b' -> 2.
    *   *Pros*: Small vocabulary, no unknown words.
    *   *Cons*: Long sequences, less meaning per token.
2.  **Word Level**: "apple" -> 1, "banana" -> 2.
    *   *Pros*: High meaning per token.
    *   *Cons*: Huge vocabulary, struggle with rare words.
3.  **Subword (BPE)**: "ing" -> 1, "ed" -> 2.
    *   *Pros*: Best of both worlds. Used by GPT (Byte-Pair Encoding).

## Your Task
Implement a simple `Tokenizer` class that can `encode` (string -> list of ints) and `decode` (list of ints -> string).
