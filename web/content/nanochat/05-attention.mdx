---
title: Implementing Attention
step: 5
description: Coding the self-attention mechanism in raw PyTorch.
---

<SplitLayout>
  <Step>
    <Description>
      ## Coding Attention

      Now we replace the Bigram model with Self-Attention. We will implement the `Head` and `MultiHeadAttention` classes.

      ### What we'll build
      - The `Head` class.
      - Masked self-attention (causal attention).
      
      > [!NOTE]
      > This content is under construction.
    </Description>
  </Step>
</SplitLayout>
