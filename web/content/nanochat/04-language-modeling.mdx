---
title: Bigram Language Model
step: 4
description: Building a simple probabilistic model to understand next-token prediction.
---

<SplitLayout>
  <Step>
    <Description>
      ## The Baseline

      Before we build a Transformer, let's build the simplest possible language model: a Bigram model. This will establish our training loop and evaluation metrics.

      ### What we'll build
      - A simple lookup-table model.
      - The training loop skeleton.
      
      > [!NOTE]
      > This content is under construction.
    </Description>
  </Step>
</SplitLayout>
