---
title: "Positional Encoding"
step: 4
description: "Why giving AI a sense of order requires re-inventing how we count."
---

<div className="flex flex-col" style={{ gap: 'var(--space-flow)' }}>
<Description noMargin>
  In the previous chapter, we solved the meaning problem. Each token now has an embedding vector that captures *what* it means. But we left one question unanswered: how does the model know *where* each word appears?
</Description>

<Description>
  Recall that Transformers process all words **in parallel** rather than one-by-one. This is what makes them fast. But it also means the model receives all embeddings at once, with no inherent notion of "first" or "last."
</Description>

<Description>
  Consider two sentences: **"Alice gave Bob a book"** and **"Bob gave Alice a book"**. They contain the exact same words. If we look up their embeddings, both sentences produce the same set of vectors. The only difference is the order.
</Description>

<div className="my-6 grid grid-cols-1 md:grid-cols-2 gap-4">
  <div className="p-4 bg-surface rounded-lg border border-border">
    <div className="text-xs font-semibold text-muted uppercase tracking-wider mb-2">Sentence A</div>
    <div className="text-primary font-medium mb-3">"Alice gave Bob a book"</div>
    <div className="font-mono text-xs text-secondary">
      [<span className="text-emerald-400">E(Alice)</span>, E(gave), <span className="text-sky-400">E(Bob)</span>, ...]
    </div>
  </div>
  <div className="p-4 bg-surface rounded-lg border border-border">
    <div className="text-xs font-semibold text-muted uppercase tracking-wider mb-2">Sentence B</div>
    <div className="text-primary font-medium mb-3">"Bob gave Alice a book"</div>
    <div className="font-mono text-xs text-secondary">
      [<span className="text-sky-400">E(Bob)</span>, E(gave), <span className="text-emerald-400">E(Alice)</span>, ...]
    </div>
  </div>
</div>

<Description>
  The Transformer's core mechanism, **Self-Attention**, determines how words relate by computing a dot product between their vectors. This dot product produces a **single number** (a score) representing how strongly two words are connected. The problem? E(Alice) · E(Bob) returns the exact same score regardless of which sentence they came from.
</Description>

<Description>
  The model sees "Alice" and "Bob" are related. It cannot tell which one is the Subject (giver) and which is the Object (receiver). Order matters for meaning, but there is nothing in these vectors that encodes order. We need to inject position information.
</Description>
</div>



<Step title="1. Attempt 1: Counting Up">
  <Description>
    The most obvious idea: number the positions 1, 2, 3... and add this integer to each embedding dimension.
  </Description>

  <Description attached>
    Let's see what happens at position 1000:
  </Description>

  <div className="content-attached">
    <div className="p-4 bg-[#121212] rounded-lg border border-zinc-800 font-mono text-sm">
      <div className="flex items-center gap-4">
        <span className="text-muted w-28 shrink-0">E("Apple")</span>
        <span className="text-primary">[0.05, -0.02, 0.11]</span>
      </div>
      <div className="flex items-center gap-4 mt-1">
        <span className="text-muted w-28 shrink-0">+ Position 1000</span>
        <span className="text-secondary">[1000, 1000, 1000]</span>
      </div>
      <div className="border-t border-zinc-700 mt-2 pt-2 flex items-center gap-4">
        <span className="text-muted w-28 shrink-0">= Result</span>
        <span className="text-amber-400 font-bold">[1000.05, 999.98, 1000.11]</span>
      </div>
    </div>
  </div>

  <Description>
    Look at the result. The values that encoded "Apple" (0.05, -0.02, 0.11) are now invisible. Whether the original word was "Apple", "Banana", or "King", the output is essentially [1000, 1000, 1000]. The meaning of the word is **drowned** by the position.
  </Description>

  <Description>
    There is a second problem. Recall from Chapter 3 that neural networks learn by finding the right **weights** to multiply inputs. But a single weight cannot work well for both small and large numbers. A weight tuned to work for position 1 will produce wildly different results for position 1000. The model has no way to learn a consistent relationship between position 1 and position 1000 because they live at completely different scales.
  </Description>

  <Description>
    Both problems stem from the same root cause: **unbounded values**. As sequences get longer, the position numbers grow without limit.
  </Description>
</Step>

<Step title="2. Attempt 2: Scaling to [0, 1]">
  <Description>
    Okay, let's fix the magnitude problem. We can divide by the total sequence length so every position stays between 0 and 1.
  </Description>

  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800 font-mono text-sm">
    Position = index / total_length
  </div>

  <Description>
    This keeps values bounded. But it introduces **two new problems**.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">Problem 1: Variable Step Size</h4>

  <Description>
    The model needs to learn relative relationships, like "pay attention to the word immediately before me." But what does "one position back" mean in this scheme?
  </Description>

  <Description>
    Since the encoding divides by length, the answer changes. In a 10-word sentence, one step back is -0.10. In a 50-word sentence, the same step is only -0.02. Play the animation or drag the slider to explore:
  </Description>

  <NormalizedStepSize />

  <Description>
    The model cannot learn a stable rule for "neighbor" because its mathematical meaning keeps changing.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">Problem 2: Unstable Absolute Positions</h4>

  <Description>
    Consider the sentence **"The cat sat."** The word "cat" is at index 1. With normalization, its position value is 1/3 ≈ 0.33.
  </Description>

  <Description>
    Now consider **"The cat sat on the mat."** The word "cat" is still at index 1. But now its position value is 1/6 ≈ 0.17.
  </Description>

  <Description>
    Same word, same index, completely different encoding. The model cannot learn that "the second word is often the subject" because the value for "second word" keeps changing based on how long the sentence happens to be.
  </Description>

  <Description>
    Neither relative positions nor absolute positions are stable. This approach is fundamentally broken.
  </Description>
</Step>

<Step title="3. Defining the Perfect Encoding">
  <Description>
    We've tried two obvious approaches and both failed. Before we try again, let's step back and define exactly what properties a positional encoding must have.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">1. Bounded Values</h4>

  <Description>
    We learned this from Attempt 1. The position signal must stay small (like -1 to 1). If position values grow unbounded, they drown the semantic embedding. The model should hear "Apple at position 5," not just "POSITION 5."
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">2. Unique and Deterministic</h4>

  <Description>
    Every position must produce a distinct encoding, and that encoding must be the same every time. Position 5 must always map to the exact same vector, whether during training or at inference. If two positions share an encoding, the model cannot distinguish them. If the same position produces different encodings, the model cannot learn stable patterns.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">3. Consistent Relative Distances</h4>

  <Description>
    We learned this from Attempt 2. The relationship between any position P and position P+k should work the same regardless of what P is. "Three words ahead" should mean the same mathematical operation whether you're at position 10 or position 1000. This lets the model learn portable attention patterns.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">4. Extrapolation</h4>

  <Description>
    The encoding should work for sequence lengths the model has never seen. If training uses sequences up to 1000 tokens, the encoding should still produce meaningful values at position 2000. We don't want the model to break just because a user sends a longer prompt.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">5. Smooth and Continuous</h4>

  <Description>
    Neural networks learn by making small adjustments. If the model's prediction is slightly wrong, it nudges its weights a little bit in the right direction. This only works if small changes in input lead to small changes in output. In mathematical terms, the function must be **differentiable**.
  </Description>

  <Description>
    Imagine an encoding where position 5 is [0.12, 0.99] and position 6 is [-0.76, 0.42]. These vectors are wildly different. The model cannot slightly nudge its weights to learn "5 and 6 are adjacent" because their values share no predictable pattern. It would have to memorize every position as a separate case, which does not scale.
  </Description>

  <Description>
    Now consider a smooth encoding where nearby positions have similar values. Position 5 might be [0.12, 0.99], position 6 is [0.14, 0.98], and position 7 is [0.16, 0.97]. There are no sudden jumps between neighbors. This gradual change means the model can learn via small weight adjustments rather than memorizing each position separately.
  </Description>

  <Description>
    Now we have our requirements: **bounded**, **unique**, **consistent**, **extrapolatable**, and **smooth**. The tension is clear: how do you represent infinitely many positions with bounded values while keeping everything smooth and consistent?
  </Description>

  <Description>
    The answer comes from something we use every day.
  </Description>
</Step>



<Step title="4. Borrowing from Number Systems">
  <Description>
    We already use systems that satisfy most of our requirements every day. Consider how we write the number **459**:
  </Description>

  <Description>
    - Each digit is **bounded** (0 to 9)
    - The combination is **unique** (no other number looks like 459)
    - Moving by +1 follows a **consistent pattern** (459 → 460 → 461)
  </Description>

  <Description>
    The trick is using **multiple columns that cycle at different speeds**. The ones column cycles fastest (0→9, then wraps). The tens column cycles slower. The hundreds column cycles slower still. This multi-speed structure is how you represent infinitely many values with bounded digits.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">What About Decimal?</h4>

  <Description>
    Each digit is bounded (0-9), the combination is unique, and the multi-column structure handles infinite positions. But decimal has two problems. First, **magnitude**: embedding values are typically -1 to 1, but decimal digits range from 0 to 9. When added together, the digit dominates the embedding information. Second, **smoothness**: digits are discrete. When the ones column wraps from 9 to 0, there is no gradual transition. It jumps suddenly.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">What About Binary?</h4>

  <Description>
    Binary is closer to embedding scale. Each bit is just 0 or 1, which does not dominate embedding values the way 0-9 does. The multi-column structure still gives us uniqueness and consistency. Position 5 is 101, position 6 is 110, position 7 is 111.
  </Description>

  <Description>
    But binary still fails smoothness. Look at the transition from position 3 (011) to position 4 (100). Every single bit flips at once. There is no gradual change, just a sudden jump. If we can fix this one remaining issue, we have our encoding.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">Making Binary Smooth</h4>

  <Description>
    Binary's problem is that 0 and 1 are just two disconnected points. There are no values in between. To go from 0 to 1, you jump. To go from 1 back to 0, you jump again.
  </Description>

  <Description>
    What we need is a **continuous function** that cycles through values smoothly. Instead of jumping between two fixed points, we need something that rises from 0 to 1, then falls back to 0, touching every value in between.
  </Description>

  <BinaryVsSmooth />

  <Description>
    With this smooth oscillation, we keep everything binary gave us: bounded values, unique combinations, and the ability to represent infinitely many positions using multiple dimensions cycling at different speeds. We just need to find a mathematical function that produces this smooth curve.
  </Description>
</Step>
