---
title: "Positional Encoding"
step: 4
description: "Why giving AI a sense of order requires re-inventing how we count."
---

<div className="flex flex-col" style={{ gap: 'var(--space-flow)' }}>
<Description noMargin>
  In the previous chapter, we solved the "meaning representation" problem by mapping each token to an embedding vector. Each token now has an embedding vector that the model can use to capture *semantic* features. But we left one question unanswered: how does the model know *where* each word appears?
</Description>

<Description>
  Recall that Transformers process all words **in parallel** rather than one-by-one. This is what makes them fast. But it also means the model receives all embeddings at once, with no inherent notion of "first" or "last."
</Description>

<Description>
  Consider two sentences: **"Alice gave Bob a book"** and **"Bob gave Alice a book"**. They contain the exact same words. If we look up their embeddings, both sentences produce the same set of vectors. The only difference is the order.
</Description>

<div className="my-6 grid grid-cols-1 md:grid-cols-2 gap-4">
  <div className="p-4 bg-surface rounded-lg border border-border">
    <div className="text-xs font-semibold text-muted uppercase tracking-wider mb-2">Sentence A</div>
    <div className="text-primary font-medium mb-3">"Alice gave Bob a book"</div>
    <div className="font-mono text-xs text-secondary">
      [<span className="text-emerald-400">E(Alice)</span>, E(gave), <span className="text-sky-400">E(Bob)</span>, ...]
    </div>
  </div>
  <div className="p-4 bg-surface rounded-lg border border-border">
    <div className="text-xs font-semibold text-muted uppercase tracking-wider mb-2">Sentence B</div>
    <div className="text-primary font-medium mb-3">"Bob gave Alice a book"</div>
    <div className="font-mono text-xs text-secondary">
      [<span className="text-sky-400">E(Bob)</span>, E(gave), <span className="text-emerald-400">E(Alice)</span>, ...]
    </div>
  </div>
</div>

<Description>
  The Transformer's core mechanism, **Self-Attention**, determines how words relate by computing a **compatibility score** for each pair of tokens. Internally, it's a (scaled) dot product between their embedding vectors, producing a **single number** representing connection strength.
</Description>

<Description>
  The problem: if we only feed word embeddings (no position info), the compatibility between "Alice" and "Bob" depends on *what* they are, not *where* they appear. So "Alice gave Bob..." and "Bob gave Alice..." produce identical scores. The model sees that "Alice" and "Bob" are related, but it cannot tell which one is the Subject (giver) and which is the Object (receiver). Order matters for meaning, but nothing in these vectors encodes order. We need to inject position information.
</Description>
</div>



<Step title="1. Attempt 1: Counting Up">
  <Description>
    The most naive idea is to number the positions 1, 2, 3... and inject the raw index into the token embedding by adding the number to every dimension. Let's trace through what happens.
  </Description>

  <Description attached>
    Imagine the word "Apple" appears at position 1000 in a long document:
  </Description>

  <div className="content-attached">
    <div className="p-4 bg-[#121212] rounded-lg border border-zinc-800 font-mono text-sm">
      <div className="flex items-center gap-4">
        <span className="text-muted w-28 shrink-0">E("Apple")</span>
        <span className="text-primary">[0.05, -0.02, 0.11]</span>
      </div>
      <div className="flex items-center gap-4 mt-1">
        <span className="text-muted w-28 shrink-0">+ Position 1000</span>
        <span className="text-secondary">[1000, 1000, 1000]</span>
      </div>
      <div className="border-t border-zinc-700 mt-2 pt-2 flex items-center gap-4">
        <span className="text-muted w-28 shrink-0">= Result</span>
        <span className="text-amber-400 font-bold">[1000.05, 999.98, 1000.11]</span>
      </div>
    </div>
  </div>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">Problem 1: Drowned Meaning</h4>

  <Description>
    Look at the result. The values that encoded "Apple" (0.05, -0.02, 0.11) are now invisible. Whether the original word was "Apple", "Banana", or "King", the output is essentially [1000, 1000, 1000]. The semantic meaning is completely **drowned** by the position signal.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">Problem 2: Scale Mismatch</h4>

  <Description>
    Recall from Chapter 3 that neural networks learn by applying **weights** to their inputs. If we used raw position indices like 1, 1000, and 10,000, the model has to handle a huge dynamic range. That makes learning harder becuase the same weight has to work for all scales. A weight tuned for position 1 can behave very differently at position 1000.
  </Description>


  <Description>
    Both problems stem from the same root cause: **unbounded values**. As sequences get longer, the position numbers grow without limit.
  </Description>
</Step>

<Step title="2. Attempt 2: Scaling to [0, 1]">
  <Description>
    Okay, let's fix the magnitude problem. We can divide by the total sequence length so every position stays between 0 and 1.
  </Description>

  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800 font-mono text-sm">
    Position = index / total_length
  </div>

  <Description>
    This keeps values bounded. But it introduces **two new problems**.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">Problem 1: Variable Step Size</h4>

  <Description>
    The model often benefits from learning relative patterns, like "pay attention to the word immediately before me." But what does "one position back" mean in this scheme?
  </Description>

  <Description>
    Becuase we divide by the sequence length, the numeric step size changes with length. In a 10-word sentence, one step back is -0.10. In a 50-word sentence, the same step is only -0.02. So the same relative idea ("one token away") corresponds to different numeric values depending on how long the sentence is. Play the animation or drag the slider to explore:
  </Description>

  <NormalizedStepSize />

  <Description>
    That makes it harder for the model to learn a single, length-independent rule for "neighbor" or relative patterns in general.   
  </Description>


  <h4 className="text-base font-semibold text-primary mt-8 mb-3">Problem 2: Length-Dependent Absolute Positions</h4>

  <Description>
    Consider the sentence **"The cat sat."** The word "cat" is at index 1. With normalization, its position value is 1/3 ≈ 0.33.
  </Description>

  <Description>
    Now consider **"The cat sat on the mat."** The word "cat" is still at index 1. But now its position value is 1/6 ≈ 0.17.
  </Description>

  <Description>
    Same word, same index, completely different encoding. This means "second word" does not correspond to a consistent numeric value across examples, which weakens any attempt to learn stable absolute position patterns.
  </Description>

  <Description>
  So while this normalization trick keeps magnitudes small, neither relative positions nor absolute positions are stable, which makes generalization less clean.
  </Description>
</Step>

<Step title="3. Defining the Perfect Encoding">
  <Description>
    We've tried two obvious approaches and both failed. Before we try again, let's step back and define exactly what properties a positional encoding must have.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">1. Bounded Values</h4>

  <Description>
    We learned this from Attempt 1. The position signal must stay small (like -1 to 1). If position values grow unbounded, they drown the semantic embedding. The model should hear "Apple at position 5," not just "POSITION 5."
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">2. Unique and Deterministic</h4>

  <Description>
    Every position must produce a distinct encoding, and that encoding must be the same every time. Position 5 must always map to the exact same vector, whether during training or at inference. If two positions share an encoding, the model cannot distinguish them. If the same position produces different encodings, the model cannot learn stable patterns.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">3. Consistent Relative Distances</h4>

  <Description>
    We learned this from Attempt 2. The relationship between any position P and position P+k should work the same regardless of what P is. "Three words ahead" should mean the same mathematical operation whether you're at position 10 or position 1000. This lets the model learn portable attention patterns.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">4. Extrapolation</h4>

  <Description>
    The encoding should work for sequence lengths the model has never seen. If training uses sequences up to 1000 tokens, the encoding should still produce meaningful values at position 2000. We don't want the model to break just because a user sends a longer prompt.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">5. Smooth and Continuous</h4>

  <Description>
    Neural networks learn by making small adjustments. If the model's prediction is slightly wrong, it nudges its weights a little bit in the right direction. This only works if small changes in input lead to small changes in output. In mathematical terms, the function must be **differentiable**.
  </Description>

  <Description>
    Imagine an encoding where position 5 is [0.12, 0.99] and position 6 is [-0.76, 0.42]. These vectors are wildly different. The model cannot slightly nudge its weights to learn "5 and 6 are adjacent" because their values share no predictable pattern. It would have to memorize every position as a separate case, which does not scale.
  </Description>

  <Description>
    Now consider a smooth encoding where nearby positions have similar values. Position 5 might be [0.12, 0.99], position 6 is [0.14, 0.98], and position 7 is [0.16, 0.97]. There are no sudden jumps between neighbors. This gradual change means the model can learn via small weight adjustments rather than memorizing each position separately.
  </Description>

  <Description>
    Now we have our requirements: **bounded**, **unique**, **consistent**, **extrapolatable**, and **smooth**. The tension is clear: how do you represent infinitely many positions with bounded values while keeping everything smooth and consistent?
  </Description>

  <Description>
    The answer comes from something we use every day.
  </Description>
</Step>



<Step title="4. Borrowing from Number Systems">
  <Description>
    We already use systems that satisfy most of our requirements every day. Consider how we write the number **459**:
  </Description>

  <Description>
    - Each digit is **bounded** (0 to 9)
    - The combination is **unique** (no other number looks like 459)
    - Moving by +1 follows a **consistent pattern** (459 → 460 → 461)
  </Description>

  <Description>
    The trick is using **multiple columns that cycle at different speeds**. The ones column cycles fastest (0→9, then wraps). The tens column cycles slower. The hundreds column cycles slower still. This multi-speed structure is how you represent infinitely many values with bounded digits.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">What About Decimal?</h4>

  <Description>
    Each digit is bounded (0-9), the combination is unique, and the multi-column structure handles infinite positions. But decimal has two problems. First, **magnitude**: embedding values are typically -1 to 1, but decimal digits range from 0 to 9. When added together, the digit dominates the embedding information. Second, **smoothness**: digits are discrete. When the ones column wraps from 9 to 0, there is no gradual transition. It jumps suddenly.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">What About Binary?</h4>

  <Description>
    Binary is closer to embedding scale. Each bit is just 0 or 1, which does not dominate embedding values the way 0-9 does. The multi-column structure still gives us uniqueness and consistency. Position 5 is 101, position 6 is 110, position 7 is 111.
  </Description>

  <Description>
    But binary still fails smoothness. Look at the transition from position 3 (011) to position 4 (100). Every single bit flips at once. There is no gradual change, just a sudden jump. If we can fix this one remaining issue, we have our encoding.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">Making Binary Smooth</h4>

  <Description>
    Binary's problem is that 0 and 1 are just two disconnected points. There are no values in between. To go from 0 to 1, you jump. To go from 1 back to 0, you jump again.
  </Description>

  <Description>
    What we need is a **continuous function** that cycles through values smoothly. Instead of jumping between two fixed points, we need something that rises from 0 to 1, then falls back to 0, touching every value in between.
  </Description>

  <BinaryVsSmooth />

  <Description>
    This smooth oscillation keeps what binary gave us: bounded values, unique combinations, and the ability to handle infinitely many positions. But it also adds what binary couldn't: gradual transitions between neighbors, so the model can learn through small weight adjustments. We just need to find a mathematical function that produces this curve.
  </Description>
</Step>

<Step title="5. Finding the Function">
  <Description>
    In Section 4, we visualized what each dimension needs: a function that smoothly oscillates, rising and falling without jumps. If you have taken trigonometry, you might recognize this shape. It looks like a **sine wave**.
  </Description>

  <Description>
    Our visualization used a shifted version: (sin(x) + 1) / 2. This kept values between 0 and 1. But the raw sine function naturally oscillates between -1 and +1.
  </Description>

<Description>
This raises a design choice: should we artifically shift the wave to keep it positive, or simply use the raw values that dip into negatives?
</Description>

<Description>
It turns out the original range -1 to +1 is better. Embedding values are initialized centered around 0, and techniques like **Layer Normalization** (covered later) keep them there throughout the network. A positional encoding around 0 blends naturally with this distribution. Shifting to 0-1 would add a constant positive bias to every dimension for no added benefit.
</Description>

  <Description>
    Now we have the function for each dimension. Each dimension gets its own sine wave, cycling at a different frequency. Let's see all the dimensions in action:
  </Description>

  <FrequencyWaves />

  <Description>
    The fast waves distinguish nearby positions. The slow waves distinguish distant positions. Together, they create a unique fingerprint for every position, exactly as we designed in Section 4.
  </Description>
</Step>

  </Description>
</Step>
