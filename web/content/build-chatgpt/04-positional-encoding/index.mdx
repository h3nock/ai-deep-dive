---
title: "Positional Encoding"
step: 4
description: "Why giving AI a sense of order requires re-inventing how we count."
---

<div className="flex flex-col" style={{ gap: 'var(--space-flow)' }}>
<Description noMargin>
  In the previous chapter, we solved the meaning problem. Each token now has an embedding vector that captures *what* it means. But we left one question unanswered: how does the model know *where* each word appears?
</Description>

<Description>
  Recall that Transformers process all words **in parallel** rather than one-by-one. This is what makes them fast. But it also means the model receives all embeddings at once, with no inherent notion of "first" or "last."
</Description>

<Description>
  Consider two sentences: **"Alice gave Bob a book"** and **"Bob gave Alice a book"**. They contain the exact same words. If we look up their embeddings, both sentences produce the same set of vectors. The only difference is the order.
</Description>

<div className="my-6 grid grid-cols-1 md:grid-cols-2 gap-4">
  <div className="p-4 bg-surface rounded-lg border border-border">
    <div className="text-xs font-semibold text-muted uppercase tracking-wider mb-2">Sentence A</div>
    <div className="text-primary font-medium mb-3">"Alice gave Bob a book"</div>
    <div className="font-mono text-xs text-secondary">
      [<span className="text-emerald-400">E(Alice)</span>, E(gave), <span className="text-sky-400">E(Bob)</span>, ...]
    </div>
  </div>
  <div className="p-4 bg-surface rounded-lg border border-border">
    <div className="text-xs font-semibold text-muted uppercase tracking-wider mb-2">Sentence B</div>
    <div className="text-primary font-medium mb-3">"Bob gave Alice a book"</div>
    <div className="font-mono text-xs text-secondary">
      [<span className="text-sky-400">E(Bob)</span>, E(gave), <span className="text-emerald-400">E(Alice)</span>, ...]
    </div>
  </div>
</div>

<Description>
  The Transformer's core mechanism, **Self-Attention**, determines how words relate by computing a dot product between their vectors. This dot product produces a **single number** (a score) representing how strongly two words are connected. The problem? E(Alice) · E(Bob) returns the exact same score regardless of which sentence they came from.
</Description>

<Description>
  The model sees "Alice" and "Bob" are related. It cannot tell which one is the Subject (giver) and which is the Object (receiver). Order matters for meaning, but there is nothing in these vectors that encodes order. We need to inject position information.
</Description>
</div>



<Step title="1. Attempt 1: Counting Up">
  <Description>
    The most obvious idea: number the positions 1, 2, 3... and add this integer to each embedding dimension.
  </Description>

  <Description attached>
    Let's see what happens at position 1000:
  </Description>

  <div className="content-attached">
    <div className="p-4 bg-[#121212] rounded-lg border border-zinc-800 font-mono text-sm">
      <div className="flex items-center gap-4">
        <span className="text-muted w-28 shrink-0">E("Apple")</span>
        <span className="text-primary">[0.05, -0.02, 0.11]</span>
      </div>
      <div className="flex items-center gap-4 mt-1">
        <span className="text-muted w-28 shrink-0">+ Position 1000</span>
        <span className="text-secondary">[1000, 1000, 1000]</span>
      </div>
      <div className="border-t border-zinc-700 mt-2 pt-2 flex items-center gap-4">
        <span className="text-muted w-28 shrink-0">= Result</span>
        <span className="text-amber-400 font-bold">[1000.05, 999.98, 1000.11]</span>
      </div>
    </div>
  </div>

  <Description>
    Look at the result. The values that encoded "Apple" (0.05, -0.02, 0.11) are now invisible. Whether the original word was "Apple", "Banana", or "King", the output is essentially [1000, 1000, 1000]. The meaning of the word is **drowned** by the position.
  </Description>

  <Description>
    There is a second problem. Recall from Chapter 3 that neural networks learn by finding the right **weights** to multiply inputs. But a single weight cannot work well for both small and large numbers. A weight tuned to work for position 1 will produce wildly different results for position 1000. The model has no way to learn a consistent relationship between position 1 and position 1000 because they live at completely different scales.
  </Description>

  <Description>
    Both problems stem from the same root cause: **unbounded values**. As sequences get longer, the position numbers grow without limit.
  </Description>
</Step>

<Step title="2. Attempt 2: Scaling to [0, 1]">
  <Description>
    Okay, let's fix the magnitude problem. We can divide by the total sequence length so every position stays between 0 and 1.
  </Description>

  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800 font-mono text-sm">
    Position = index / total_length
  </div>

  <Description>
    This keeps values bounded. But it introduces **two new problems**.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">Problem 1: Variable Step Size</h4>

  <Description>
    The model needs to learn relative relationships, like "pay attention to the word immediately before me." But what does "one position back" mean in this scheme?
  </Description>

  <Description>
    Since the encoding divides by length, the answer changes. In a 10-word sentence, one step back is -0.10. In a 50-word sentence, the same step is only -0.02. Play the animation or drag the slider to explore:
  </Description>

  <NormalizedStepSize />

  <Description>
    The model cannot learn a stable rule for "neighbor" because its mathematical meaning keeps changing.
  </Description>

  <h4 className="text-base font-semibold text-primary mt-8 mb-3">Problem 2: Unstable Absolute Positions</h4>

  <Description>
    Consider the sentence **"The cat sat."** The word "cat" is at index 1. With normalization, its position value is 1/3 ≈ 0.33.
  </Description>

  <Description>
    Now consider **"The cat sat on the mat."** The word "cat" is still at index 1. But now its position value is 1/6 ≈ 0.17.
  </Description>

  <Description>
    Same word, same index, completely different encoding. The model cannot learn that "the second word is often the subject" because the value for "second word" keeps changing based on how long the sentence happens to be.
  </Description>

  <Description>
    Neither relative positions nor absolute positions are stable. This approach is fundamentally broken.
  </Description>
</Step>
