---
title: "Positional Encoding"
step: 4
description: "Teaching the model where each word lives in a sentence and why it's harder than you'd think."
---

<div className="flex flex-col gap-4">
<Description>
  In the previous chapter, we built rich embedding vectors that capture the **meaning** of each token. We can now represent "King" as a point in a 768-dimensional space where similar concepts cluster together. The embedding for "Queen" sits nearby, while "Banana" lives in a distant region.
</Description>

<Description>
  But there's a critical piece missing. Our embeddings tell us **what** each token means, but not **where** it appears. The word "Alice" gets the exact same vector whether she's the first word or the tenth. For language understanding, position matters enormously.
</Description>
</div>


<Step title="1. Why Position Matters">
  <Description>
    Consider these two sentences:
  </Description>

  <div className="my-6 grid grid-cols-1 md:grid-cols-2 gap-4">
    <div className="p-4 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
      <div className="flex items-center gap-3 mb-3">
        <div className="w-8 h-8 rounded-full bg-slate-200 dark:bg-slate-700 flex items-center justify-center text-slate-600 dark:text-slate-400 font-bold text-sm">1</div>
        <span className="font-bold text-lg text-slate-800 dark:text-slate-200">"Alice gave Bob a book"</span>
      </div>
      <div className="text-sm text-slate-600 dark:text-slate-400">
        Alice = the giver, Bob = the receiver
      </div>
    </div>

    <div className="p-4 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
      <div className="flex items-center gap-3 mb-3">
        <div className="w-8 h-8 rounded-full bg-slate-200 dark:bg-slate-700 flex items-center justify-center text-slate-600 dark:text-slate-400 font-bold text-sm">2</div>
        <span className="font-bold text-lg text-slate-800 dark:text-slate-200">"Bob gave Alice a book"</span>
      </div>
      <div className="text-sm text-slate-600 dark:text-slate-400">
        Bob = the giver, Alice = the receiver
      </div>
    </div>
  </div>

  <Description>
    The exact same words, the exact same embeddings. But the meaning is completely different! In the first sentence, Alice is generous. In the second, she's the recipient. The only difference is **position**.
  </Description>

  <Description>
    Traditional models like RNNs (Recurrent Neural Networks) process words one at a time, left to right. Position is built into the architecture - the model literally sees word #1 before word #2. But this sequential approach is painfully slow.
  </Description>

  <Description>
    Transformers revolutionized NLP by processing all words **in parallel**. The entire sentence goes in at once. This is massively faster, but creates a problem: if you hand the model all words simultaneously, how does it know which came first?
  </Description>

  <div className="my-4 p-4 bg-slate-100 dark:bg-slate-800/60 rounded-lg border border-slate-200 dark:border-slate-700">
    <div className="text-sm font-semibold text-slate-700 dark:text-slate-300 mb-2">The Core Problem</div>
    <div className="text-base text-slate-600 dark:text-slate-400 leading-relaxed">
      Without positional information, "Alice gave Bob" and "Bob gave Alice" look identical to the model. It sees the same bag of embeddings with no ordering.
    </div>
  </div>

  <Description>
    We need to somehow **inject** position information into our embeddings. The question is: how? Let's explore this from first principles, failing our way to the elegant solution that modern transformers use.
  </Description>
</Step>


<Step title="2. First Attempt: Integer Indexing">
  <Description>
    The most obvious approach: just use the position number directly! Add the index (0, 1, 2, 3...) to each embedding.
  </Description>

  <div className="my-4 p-4 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
    <div className="text-sm font-medium text-slate-700 dark:text-slate-300 mb-3">Integer Indexing Approach</div>
    <div className="overflow-x-auto">
      <table className="w-full text-sm">
        <thead>
          <tr className="border-b border-slate-200 dark:border-slate-700">
            <th className="text-left py-2 px-3 text-slate-500 dark:text-slate-400 font-medium">Word</th>
            <th className="text-left py-2 px-3 text-slate-500 dark:text-slate-400 font-medium">Position</th>
            <th className="text-left py-2 px-3 text-slate-500 dark:text-slate-400 font-medium">Position Encoding</th>
          </tr>
        </thead>
        <tbody className="font-mono text-xs">
          <tr className="border-b border-slate-100 dark:border-slate-800">
            <td className="py-2 px-3 text-slate-700 dark:text-slate-300">"Alice"</td>
            <td className="py-2 px-3 text-slate-600 dark:text-slate-400">0</td>
            <td className="py-2 px-3 text-slate-700 dark:text-slate-300">[0, 0, 0, ...]</td>
          </tr>
          <tr className="border-b border-slate-100 dark:border-slate-800">
            <td className="py-2 px-3 text-slate-700 dark:text-slate-300">"gave"</td>
            <td className="py-2 px-3 text-slate-600 dark:text-slate-400">1</td>
            <td className="py-2 px-3 text-slate-700 dark:text-slate-300">[1, 1, 1, ...]</td>
          </tr>
          <tr className="border-b border-slate-100 dark:border-slate-800">
            <td className="py-2 px-3 text-slate-700 dark:text-slate-300">"Bob"</td>
            <td className="py-2 px-3 text-slate-600 dark:text-slate-400">2</td>
            <td className="py-2 px-3 text-slate-700 dark:text-slate-300">[2, 2, 2, ...]</td>
          </tr>
          <tr>
            <td className="py-2 px-3 text-slate-400">...</td>
            <td className="py-2 px-3 text-slate-400">n</td>
            <td className="py-2 px-3 text-slate-400">[n, n, n, ...]</td>
          </tr>
        </tbody>
      </table>
    </div>
  </div>

  <Description>
    Simple, right? Just add the position number to every dimension of the embedding. Position 0 adds zeros, position 1 adds ones everywhere, position 100 adds 100 everywhere.
  </Description>

  <h4 className="text-lg font-semibold text-slate-800 dark:text-slate-200 mt-6 mb-3">Why This Fails</h4>

  <div className="my-4 p-4 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
    <div className="flex items-start gap-4">
      <div className="w-8 h-8 rounded-full bg-slate-200 dark:bg-slate-700 flex items-center justify-center text-slate-600 dark:text-slate-400 font-bold text-sm shrink-0">1</div>
      <div>
        <div className="font-bold text-slate-800 dark:text-slate-200 mb-2">Magnitude Explosion</div>
        <div className="text-sm text-slate-600 dark:text-slate-400 mb-3">
          Embeddings typically have values in the range of -1 to +1. Adding raw position numbers destroys this balance.
        </div>
        <div className="p-3 bg-white dark:bg-slate-900 rounded border border-slate-200 dark:border-slate-700 font-mono text-sm">
          <div className="space-y-1">
            <div className="text-slate-600 dark:text-slate-400">Position 0: embedding + [0, 0, ...] ‚Üí <span className="text-slate-500">OK</span></div>
            <div className="text-slate-600 dark:text-slate-400">Position 100: embedding + [100, 100, ...] ‚Üí <span className="text-slate-500">Values ~100</span></div>
            <div className="text-slate-600 dark:text-slate-400">Position 1000: embedding + [1000, 1000, ...] ‚Üí <span className="text-slate-500">Values ~1000</span></div>
          </div>
        </div>
        <div className="mt-3 text-sm text-slate-600 dark:text-slate-400">
          Words at position 1000 would have vectors 1000√ó larger than words at position 0. The neural network's math breaks down completely - later positions dominate all calculations.
        </div>
      </div>
    </div>
  </div>

  <div className="my-4 p-4 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
    <div className="flex items-start gap-4">
      <div className="w-8 h-8 rounded-full bg-slate-200 dark:bg-slate-700 flex items-center justify-center text-slate-600 dark:text-slate-400 font-bold text-sm shrink-0">2</div>
      <div>
        <div className="font-bold text-slate-800 dark:text-slate-200 mb-2">Unbounded Range</div>
        <div className="text-sm text-slate-600 dark:text-slate-400 mb-3">
          What happens when we see a sequence longer than anything in training?
        </div>
        <div className="p-3 bg-white dark:bg-slate-900 rounded border border-slate-200 dark:border-slate-700 text-sm">
          <div className="text-slate-700 dark:text-slate-300">
            If training only saw sequences up to length 512, position 513 produces a value the model has <strong>never seen before</strong>. The model can't generalize to longer sequences.
          </div>
        </div>
      </div>
    </div>
  </div>

  <div className="my-4 p-4 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
    <div className="flex items-start gap-4">
      <div className="w-8 h-8 rounded-full bg-slate-200 dark:bg-slate-700 flex items-center justify-center text-slate-600 dark:text-slate-400 font-bold text-sm shrink-0">3</div>
      <div>
        <div className="font-bold text-slate-800 dark:text-slate-200 mb-2">No Relative Information</div>
        <div className="text-sm text-slate-600 dark:text-slate-400 mb-3">
          The gap between positions 1 and 2 is +1. The gap between positions 999 and 1000 is also +1. But their encodings are wildly different scales.
        </div>
        <div className="p-3 bg-white dark:bg-slate-900 rounded border border-slate-200 dark:border-slate-700 font-mono text-sm">
          <div className="space-y-1">
            <div className="text-slate-600 dark:text-slate-400">|pos(2) - pos(1)| = |2 - 1| = 1</div>
            <div className="text-slate-600 dark:text-slate-400">|pos(1000) - pos(999)| = |1000 - 999| = 1</div>
          </div>
        </div>
        <div className="mt-3 text-sm text-slate-600 dark:text-slate-400">
          Numerically the same, but the model sees completely different patterns. "Adjacent" doesn't look the same at position 5 vs position 500.
        </div>
      </div>
    </div>
  </div>

  <Callout type="warning" title="Verdict: Integer Indexing">
    Raw integers create magnitude problems, don't generalize to new lengths, and destroy the notion of relative distance.
  </Callout>
</Step>


<Step title="3. Second Attempt: 0-1 Normalization">
  <Description>
    Okay, raw integers are too big. What if we normalize them to a nice 0-1 range?
  </Description>

  <div className="my-4 p-4 bg-slate-900 dark:bg-slate-950 rounded-lg border border-slate-700">
    <div className="space-y-2 font-mono text-sm">
      <div className="text-slate-400"># Normalize position to [0, 1]</div>
      <div className="text-white">position_encoding = position / max_sequence_length</div>
    </div>
  </div>

  <Description>
    If our maximum sequence length is 100, then:
  </Description>

  <div className="my-4 p-4 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
    <div className="text-sm font-medium text-slate-700 dark:text-slate-300 mb-3">0-1 Normalized Approach</div>
    <div className="overflow-x-auto">
      <table className="w-full text-sm">
        <thead>
          <tr className="border-b border-slate-200 dark:border-slate-700">
            <th className="text-left py-2 px-3 text-slate-500 dark:text-slate-400 font-medium">Position</th>
            <th className="text-left py-2 px-3 text-slate-500 dark:text-slate-400 font-medium">Encoding (max=100)</th>
          </tr>
        </thead>
        <tbody className="font-mono text-xs">
          <tr className="border-b border-slate-100 dark:border-slate-800">
            <td className="py-2 px-3 text-slate-700 dark:text-slate-300">0</td>
            <td className="py-2 px-3 text-emerald-600 dark:text-emerald-400">0.00</td>
          </tr>
          <tr className="border-b border-slate-100 dark:border-slate-800">
            <td className="py-2 px-3 text-slate-700 dark:text-slate-300">1</td>
            <td className="py-2 px-3 text-emerald-600 dark:text-emerald-400">0.01</td>
          </tr>
          <tr className="border-b border-slate-100 dark:border-slate-800">
            <td className="py-2 px-3 text-slate-700 dark:text-slate-300">50</td>
            <td className="py-2 px-3 text-emerald-600 dark:text-emerald-400">0.50</td>
          </tr>
          <tr>
            <td className="py-2 px-3 text-slate-700 dark:text-slate-300">100</td>
            <td className="py-2 px-3 text-emerald-600 dark:text-emerald-400">1.00</td>
          </tr>
        </tbody>
      </table>
    </div>
  </div>

  <Description>
    Nice! Values are bounded between 0 and 1. No more magnitude explosions. Problem solved?
  </Description>

  <h4 className="text-lg font-semibold text-slate-800 dark:text-slate-200 mt-6 mb-3">Why This Also Fails</h4>

  <div className="my-4 p-4 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
    <div className="flex items-start gap-4">
      <div className="w-8 h-8 rounded-full bg-slate-200 dark:bg-slate-700 flex items-center justify-center text-slate-600 dark:text-slate-400 font-bold text-sm shrink-0">1</div>
      <div>
        <div className="font-bold text-slate-800 dark:text-slate-200 mb-2">Resolution Depends on Sequence Length</div>
        <div className="text-sm text-slate-600 dark:text-slate-400 mb-3">
          The "step size" between adjacent positions changes based on max length.
        </div>
        <div className="p-3 bg-white dark:bg-slate-900 rounded border border-slate-200 dark:border-slate-700">
          <div className="grid grid-cols-2 gap-4 text-sm">
            <div>
              <div className="font-medium text-slate-700 dark:text-slate-300 mb-2">Short sequence (max=10)</div>
              <div className="font-mono text-xs text-slate-600 dark:text-slate-400">
                pos 0 ‚Üí 0.0<br/>
                pos 1 ‚Üí 0.1<br/>
                pos 2 ‚Üí 0.2<br/>
                <span className="text-amber-600 dark:text-amber-400">Step = 0.1</span>
              </div>
            </div>
            <div>
              <div className="font-medium text-slate-700 dark:text-slate-300 mb-2">Long sequence (max=1000)</div>
              <div className="font-mono text-xs text-slate-600 dark:text-slate-400">
                pos 0 ‚Üí 0.000<br/>
                pos 1 ‚Üí 0.001<br/>
                pos 2 ‚Üí 0.002<br/>
                <span className="text-amber-600 dark:text-amber-400">Step = 0.001</span>
              </div>
            </div>
          </div>
        </div>
        <div className="mt-3 text-sm text-slate-600 dark:text-slate-400">
          Position 1 means something different depending on sequence length. The model can't learn a consistent notion of "next to" or "3 words apart."
        </div>
      </div>
    </div>
  </div>

  <div className="my-4 p-4 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
    <div className="flex items-start gap-4">
      <div className="w-8 h-8 rounded-full bg-slate-200 dark:bg-slate-700 flex items-center justify-center text-slate-600 dark:text-slate-400 font-bold text-sm shrink-0">2</div>
      <div>
        <div className="font-bold text-slate-800 dark:text-slate-200 mb-2">Requires Knowing Sequence Length in Advance</div>
        <div className="text-sm text-slate-600 dark:text-slate-400 mb-3">
          To compute position/max, you need to know the final length. But during generation, you're creating the sequence one token at a time!
        </div>
        <div className="p-3 bg-white dark:bg-slate-900 rounded border border-slate-200 dark:border-slate-700 text-sm">
          <div className="text-slate-700 dark:text-slate-300">
            When generating "The cat sat on the ___", you don't know if the final answer will be 6 words or 60 words. You can't normalize properly.
          </div>
        </div>
      </div>
    </div>
  </div>

  <div className="my-4 p-4 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
    <div className="flex items-start gap-4">
      <div className="w-8 h-8 rounded-full bg-slate-200 dark:bg-slate-700 flex items-center justify-center text-slate-600 dark:text-slate-400 font-bold text-sm shrink-0">3</div>
      <div>
        <div className="font-bold text-slate-800 dark:text-slate-200 mb-2">Low Precision for Long Sequences</div>
        <div className="text-sm text-slate-600 dark:text-slate-400 mb-3">
          With 32-bit floating point numbers, tiny differences get lost.
        </div>
        <div className="p-3 bg-white dark:bg-slate-900 rounded border border-slate-200 dark:border-slate-700 font-mono text-sm">
          <div className="space-y-1">
            <div className="text-slate-600 dark:text-slate-400">max_length = 10,000</div>
            <div className="text-slate-600 dark:text-slate-400">pos 5000 = 0.5000</div>
            <div className="text-slate-600 dark:text-slate-400">pos 5001 = 0.5001</div>
            <div className="text-slate-500 dark:text-slate-400">Difference: 0.0001 ‚Üí Nearly indistinguishable!</div>
          </div>
        </div>
      </div>
    </div>
  </div>

  <Callout type="warning" title="Verdict: 0-1 Normalization">
    Resolution varies with length, requires future knowledge, and loses precision for long sequences.
  </Callout>
</Step>


<Step title="4. What We Actually Need">
  <Description>
    Let's step back and define our requirements explicitly. A good positional encoding should:
  </Description>

  <div className="my-6 space-y-3">
    <div className="flex items-start gap-4 p-3 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
      <div className="w-8 h-8 rounded-full bg-slate-200 dark:bg-slate-700 flex items-center justify-center text-slate-600 dark:text-slate-400 font-bold text-sm shrink-0">1</div>
      <div>
        <div className="font-bold text-slate-800 dark:text-slate-200">Bounded Values</div>
        <div className="text-sm text-slate-600 dark:text-slate-400 mt-1">
          Output should stay in a fixed range (like -1 to +1) regardless of position. No magnitude explosions at position 10,000.
        </div>
      </div>
    </div>

    <div className="flex items-start gap-4 p-3 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
      <div className="w-8 h-8 rounded-full bg-slate-200 dark:bg-slate-700 flex items-center justify-center text-slate-600 dark:text-slate-400 font-bold text-sm shrink-0">2</div>
      <div>
        <div className="font-bold text-slate-800 dark:text-slate-200">Unique for Each Position</div>
        <div className="text-sm text-slate-600 dark:text-slate-400 mt-1">
          Every position should have a distinct "fingerprint" so the model can tell them apart.
        </div>
      </div>
    </div>

    <div className="flex items-start gap-4 p-3 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
      <div className="w-8 h-8 rounded-full bg-slate-200 dark:bg-slate-700 flex items-center justify-center text-slate-600 dark:text-slate-400 font-bold text-sm shrink-0">3</div>
      <div>
        <div className="font-bold text-slate-800 dark:text-slate-200">Consistent Relative Distances</div>
        <div className="text-sm text-slate-600 dark:text-slate-400 mt-1">
          "3 positions apart" should look similar whether we're at the start or middle of the sequence. The model should easily learn patterns like "the word 2 slots ago."
        </div>
      </div>
    </div>

    <div className="flex items-start gap-4 p-3 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
      <div className="w-8 h-8 rounded-full bg-slate-200 dark:bg-slate-700 flex items-center justify-center text-slate-600 dark:text-slate-400 font-bold text-sm shrink-0">4</div>
      <div>
        <div className="font-bold text-slate-800 dark:text-slate-200">Generalizes to Unseen Lengths</div>
        <div className="text-sm text-slate-600 dark:text-slate-400 mt-1">
          If trained on sequences up to 512 tokens, it should still work reasonably at position 600.
        </div>
      </div>
    </div>

    <div className="flex items-start gap-4 p-3 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
      <div className="w-8 h-8 rounded-full bg-slate-200 dark:bg-slate-700 flex items-center justify-center text-slate-600 dark:text-slate-400 font-bold text-sm shrink-0">5</div>
      <div>
        <div className="font-bold text-slate-800 dark:text-slate-200">Deterministic (No Learning Required)</div>
        <div className="text-sm text-slate-600 dark:text-slate-400 mt-1">
          Ideally, we'd compute it with a formula rather than learning it. Less parameters = simpler model.
        </div>
      </div>
    </div>
  </div>

  <Description>
    Integer indexing fails requirements 1, 3, and 4. Normalized indexing fails 2, 3, and 4. We need something fundamentally different.
  </Description>

  <Description>
    What mathematical function is bounded, periodic, and naturally encodes patterns? **Sinusoids!**
  </Description>
</Step>


<Step title="5. The Elegant Solution: Sinusoidal Encoding">
  <Description>
    The "Attention Is All You Need" paper introduced a beautiful solution using sine and cosine waves.
  </Description>

  <Description>
    Think of how we tell time. A clock uses **multiple cycles** of different periods:
  </Description>

  <div className="my-6 grid grid-cols-1 md:grid-cols-3 gap-4">
    <div className="p-4 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700 text-center">
      <div className="text-3xl mb-2">‚è±Ô∏è</div>
      <div className="font-bold text-slate-800 dark:text-slate-200">Seconds</div>
      <div className="text-sm text-slate-600 dark:text-slate-400 mt-1">Cycles every 60</div>
      <div className="text-xs text-slate-500 mt-2 font-mono">Fast oscillation</div>
    </div>
    <div className="p-4 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700 text-center">
      <div className="text-3xl mb-2">üïê</div>
      <div className="font-bold text-slate-800 dark:text-slate-200">Minutes</div>
      <div className="text-sm text-slate-600 dark:text-slate-400 mt-1">Cycles every 60</div>
      <div className="text-xs text-slate-500 mt-2 font-mono">Medium oscillation</div>
    </div>
    <div className="p-4 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700 text-center">
      <div className="text-3xl mb-2">üïõ</div>
      <div className="font-bold text-slate-800 dark:text-slate-200">Hours</div>
      <div className="text-sm text-slate-600 dark:text-slate-400 mt-1">Cycles every 12/24</div>
      <div className="text-xs text-slate-500 mt-2 font-mono">Slow oscillation</div>
    </div>
  </div>

  <Description>
    Each "hand" on the clock gives you different granularity. The second hand tells you about fine timing. The hour hand tells you about the broader time of day. Together, they uniquely identify any moment.
  </Description>

  <Description>
    Positional encoding does the same thing with sine waves at different frequencies!
  </Description>

  <h4 className="text-lg font-semibold text-slate-800 dark:text-slate-200 mt-8 mb-4">The Formula</h4>

  <div className="my-6 p-5 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
    <div className="text-center mb-4">
      <span className="text-sm font-medium text-slate-500 dark:text-slate-400">Sinusoidal Positional Encoding</span>
    </div>
    <div className="text-center space-y-2">
      <div className="text-lg font-mono text-slate-800 dark:text-slate-200">
        $$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$
      </div>
      <div className="text-lg font-mono text-slate-800 dark:text-slate-200">
        $$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$
      </div>
    </div>
    <div className="mt-4 text-center text-sm text-slate-600 dark:text-slate-400">
      Where $pos$ is the position, $i$ is the dimension index, and $d_{model}$ is the embedding dimension.
    </div>
  </div>

  <Description>
    Let's break this down piece by piece:
  </Description>

  <div className="my-4 space-y-3">
    <div className="p-3 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
      <div className="font-bold text-slate-800 dark:text-slate-200 mb-1">$pos$ = Position in the sequence</div>
      <div className="text-sm text-slate-600 dark:text-slate-400">
        0, 1, 2, 3... The word's location.
      </div>
    </div>

    <div className="p-3 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
      <div className="font-bold text-slate-800 dark:text-slate-200 mb-1">$i$ = Dimension index (0 to d_model/2)</div>
      <div className="text-sm text-slate-600 dark:text-slate-400">
        Each pair of dimensions (2i and 2i+1) gets its own frequency. Low $i$ = high frequency (fast wiggle), high $i$ = low frequency (slow wiggle).
      </div>
    </div>

    <div className="p-3 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
      <div className="font-bold text-slate-800 dark:text-slate-200 mb-1">$10000^{2i/d_{model}}$ = The wavelength scaler</div>
      <div className="text-sm text-slate-600 dark:text-slate-400">
        This creates wavelengths from $2\pi$ (dimension 0) to $10000 \cdot 2\pi$ (last dimension). A huge range of frequencies!
      </div>
    </div>

    <div className="p-3 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
      <div className="font-bold text-slate-800 dark:text-slate-200 mb-1">$\sin$ and $\cos$ alternating</div>
      <div className="text-sm text-slate-600 dark:text-slate-400">
        Even dimensions use sine, odd dimensions use cosine. This gives us both the "position" and "velocity" of each wave, making relative positions easy to compute.
      </div>
    </div>
  </div>
</Step>


<Step title="6. Visualizing the Waves">
  <Description>
    Let's see what these waves actually look like across different dimensions:
  </Description>

  <div className="my-6 p-5 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
    <div className="text-center mb-4">
      <span className="text-sm font-medium text-slate-500 dark:text-slate-400">
        Sinusoidal Encoding Across Dimensions
      </span>
    </div>

    <div className="flex justify-center">
      <svg viewBox="0 0 500 300" className="w-full max-w-lg">
        {/* Axes */}
        <line x1="50" y1="250" x2="480" y2="250" className="stroke-slate-300 dark:stroke-slate-600" strokeWidth="1"/>
        <line x1="50" y1="20" x2="50" y2="250" className="stroke-slate-300 dark:stroke-slate-600" strokeWidth="1"/>
        
        {/* Y-axis labels */}
        <text x="25" y="55" className="fill-slate-500 dark:fill-slate-400 text-xs">+1</text>
        <text x="25" y="140" className="fill-slate-500 dark:fill-slate-400 text-xs">0</text>
        <text x="25" y="225" className="fill-slate-500 dark:fill-slate-400 text-xs">-1</text>
        
        {/* X-axis label */}
        <text x="250" y="275" textAnchor="middle" className="fill-slate-600 dark:fill-slate-400 text-xs">Position ‚Üí</text>
        
        {/* High frequency wave (dim 0) - fast oscillation */}
        <path 
          d="M 50 135 Q 65 55 80 135 Q 95 215 110 135 Q 125 55 140 135 Q 155 215 170 135 Q 185 55 200 135 Q 215 215 230 135 Q 245 55 260 135 Q 275 215 290 135 Q 305 55 320 135 Q 335 215 350 135 Q 365 55 380 135 Q 395 215 410 135 Q 425 55 440 135 Q 455 215 470 135"
          className="stroke-slate-500 dark:stroke-slate-400" 
          strokeWidth="2" 
          fill="none"
        />
        
        {/* Medium frequency wave (dim 128) */}
        <path 
          d="M 50 135 Q 110 55 170 135 Q 230 215 290 135 Q 350 55 410 135 Q 470 215 480 175"
          className="stroke-slate-400 dark:stroke-slate-500" 
          strokeWidth="2" 
          fill="none"
          strokeDasharray="8,4"
        />
        
        {/* Low frequency wave (dim 384) - slow oscillation */}
        <path 
          d="M 50 135 Q 200 55 350 135 Q 480 200 480 180"
          className="stroke-emerald-500 dark:stroke-emerald-400" 
          strokeWidth="2" 
          fill="none"
        />

        {/* Legend */}
        <g transform="translate(280, 20)">
          <rect x="0" y="0" width="180" height="75" rx="6" className="fill-white dark:fill-slate-900 stroke-slate-200 dark:stroke-slate-700"/>
          <line x1="15" y1="20" x2="45" y2="20" className="stroke-slate-500 dark:stroke-slate-400" strokeWidth="2"/>
          <text x="55" y="24" className="fill-slate-600 dark:fill-slate-400 text-[10px]">Dim 0 (fast)</text>
          <line x1="15" y1="40" x2="45" y2="40" className="stroke-slate-400 dark:stroke-slate-500" strokeWidth="2" strokeDasharray="8,4"/>
          <text x="55" y="44" className="fill-slate-600 dark:fill-slate-400 text-[10px]">Dim 128 (medium)</text>
          <line x1="15" y1="60" x2="45" y2="60" className="stroke-emerald-500 dark:stroke-emerald-400" strokeWidth="2"/>
          <text x="55" y="64" className="fill-slate-600 dark:fill-slate-400 text-[10px]">Dim 384 (slow)</text>
        </g>
      </svg>
    </div>

    <p className="text-center text-xs text-slate-500 dark:text-slate-400 mt-4">
      Each dimension oscillates at a different frequency. Low dimensions change rapidly, high dimensions change slowly.
    </p>
  </div>

  <Description>
    Think of each dimension as a different "clock hand":
  </Description>

  <div className="my-4 space-y-2">
    <div className="flex items-center gap-3 p-3 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
      <span className="font-bold text-slate-700 dark:text-slate-300 w-32 shrink-0">Dimension 0:</span>
      <span className="text-sm text-slate-600 dark:text-slate-400">Completes a full cycle every ~6 positions (like a second hand)</span>
    </div>
    <div className="flex items-center gap-3 p-3 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
      <span className="font-bold text-slate-700 dark:text-slate-300 w-32 shrink-0">Dimension 128:</span>
      <span className="text-sm text-slate-600 dark:text-slate-400">Completes a cycle every ~100 positions (like a minute hand)</span>
    </div>
    <div className="flex items-center gap-3 p-3 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
      <span className="font-bold text-emerald-600 dark:text-emerald-400 w-32 shrink-0">Dimension 384:</span>
      <span className="text-sm text-slate-600 dark:text-slate-400">Completes a cycle every ~10,000 positions (like an hour hand)</span>
    </div>
  </div>

  <Description>
    By combining all these frequencies, each position gets a unique "fingerprint" - a specific combination of wave values that identifies exactly where it is in the sequence.
  </Description>
</Step>


<Step title="7. Why Sinusoids Are Brilliant">
  <Description>
    Let's check our requirements:
  </Description>

  <div className="my-6 space-y-3">
    <div className="flex items-start gap-3 p-3 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
      <div className="w-6 h-6 rounded-full bg-slate-200 dark:bg-slate-700 flex items-center justify-center shrink-0 mt-0.5">
        <svg className="w-3.5 h-3.5 text-slate-600 dark:text-slate-400" fill="none" viewBox="0 0 24 24" stroke="currentColor" strokeWidth="2.5"><path strokeLinecap="round" strokeLinejoin="round" d="M5 13l4 4L19 7" /></svg>
      </div>
      <div>
        <div className="font-bold text-slate-800 dark:text-slate-200">Bounded Values</div>
        <div className="text-sm text-slate-600 dark:text-slate-400 mt-1">
          Sine and cosine always output values between -1 and +1. Position 1 million? Still bounded!
        </div>
      </div>
    </div>

    <div className="flex items-start gap-3 p-3 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
      <div className="w-6 h-6 rounded-full bg-slate-200 dark:bg-slate-700 flex items-center justify-center shrink-0 mt-0.5">
        <svg className="w-3.5 h-3.5 text-slate-600 dark:text-slate-400" fill="none" viewBox="0 0 24 24" stroke="currentColor" strokeWidth="2.5"><path strokeLinecap="round" strokeLinejoin="round" d="M5 13l4 4L19 7" /></svg>
      </div>
      <div>
        <div className="font-bold text-slate-800 dark:text-slate-200">Unique for Each Position</div>
        <div className="text-sm text-slate-600 dark:text-slate-400 mt-1">
          The combination of all frequencies creates a unique pattern for each position. With 768 dimensions, we have 384 different frequency channels creating an astronomical number of unique combinations.
        </div>
      </div>
    </div>

    <div className="flex items-start gap-3 p-3 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
      <div className="w-6 h-6 rounded-full bg-slate-200 dark:bg-slate-700 flex items-center justify-center shrink-0 mt-0.5">
        <svg className="w-3.5 h-3.5 text-slate-600 dark:text-slate-400" fill="none" viewBox="0 0 24 24" stroke="currentColor" strokeWidth="2.5"><path strokeLinecap="round" strokeLinejoin="round" d="M5 13l4 4L19 7" /></svg>
      </div>
      <div>
        <div className="font-bold text-slate-800 dark:text-slate-200">Consistent Relative Distances</div>
        <div className="text-sm text-slate-600 dark:text-slate-400 mt-1">
          This is the magic property! For any fixed offset $k$, we can express $PE_{pos+k}$ as a linear function of $PE_{pos}$. The model can learn "3 positions ago" as a single transformation that works everywhere.
        </div>
      </div>
    </div>

    <div className="flex items-start gap-3 p-3 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
      <div className="w-6 h-6 rounded-full bg-slate-200 dark:bg-slate-700 flex items-center justify-center shrink-0 mt-0.5">
        <svg className="w-3.5 h-3.5 text-slate-600 dark:text-slate-400" fill="none" viewBox="0 0 24 24" stroke="currentColor" strokeWidth="2.5"><path strokeLinecap="round" strokeLinejoin="round" d="M5 13l4 4L19 7" /></svg>
      </div>
      <div>
        <div className="font-bold text-slate-800 dark:text-slate-200">Generalizes to Unseen Lengths</div>
        <div className="text-sm text-slate-600 dark:text-slate-400 mt-1">
          The formula works for any position. Position 10,000 is computed the same way as position 10 - just plug in the number. No learning needed!
        </div>
      </div>
    </div>

    <div className="flex items-start gap-3 p-3 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
      <div className="w-6 h-6 rounded-full bg-slate-200 dark:bg-slate-700 flex items-center justify-center shrink-0 mt-0.5">
        <svg className="w-3.5 h-3.5 text-slate-600 dark:text-slate-400" fill="none" viewBox="0 0 24 24" stroke="currentColor" strokeWidth="2.5"><path strokeLinecap="round" strokeLinejoin="round" d="M5 13l4 4L19 7" /></svg>
      </div>
      <div>
        <div className="font-bold text-slate-800 dark:text-slate-200">Deterministic (No Learning Required)</div>
        <div className="text-sm text-slate-600 dark:text-slate-400 mt-1">
          Pure math. No parameters to train. Just compute and add. Simple and elegant.
        </div>
      </div>
    </div>
  </div>

  <h4 className="text-lg font-semibold text-slate-800 dark:text-slate-200 mt-8 mb-4">The Relative Distance Property</h4>

  <Description>
    The most powerful feature deserves a deeper look. Why can the model learn relative positions?
  </Description>

  <Description>
    Remember the trigonometric identity for angle addition:
  </Description>

  <div className="my-4 p-4 bg-slate-100 dark:bg-slate-800/60 rounded-lg border border-slate-200 dark:border-slate-700">
    <div className="text-center font-mono text-sm text-slate-800 dark:text-slate-200">
      $\sin(a + b) = \sin(a)\cos(b) + \cos(a)\sin(b)$
    </div>
  </div>

  <Description>
    This means that $PE_{pos+k}$ (the encoding at position $pos + k$) can be written as a linear combination of $PE_{pos}$. In matrix form:
  </Description>

  <div className="my-4 p-4 bg-slate-100 dark:bg-slate-800/60 rounded-lg border border-slate-200 dark:border-slate-700">
    <div className="text-center font-mono text-sm text-slate-800 dark:text-slate-200">
      $PE_{pos+k} = M_k \cdot PE_{pos}$
    </div>
    <div className="text-center text-xs text-slate-500 mt-2">
      where $M_k$ is a fixed rotation matrix that depends only on $k$
    </div>
  </div>

  <Description>
    The model can learn a single matrix that means "look 3 positions back" and apply it anywhere in the sequence. This is why attention mechanisms can easily learn patterns like "the word right before the verb" regardless of where that verb appears.
  </Description>
</Step>


<Step title="8. Combining Position with Meaning">
  <Description>
    Now we have two vectors for each token: the **embedding** (what it means) and the **positional encoding** (where it is). How do we combine them?
  </Description>

  <div className="my-6 p-5 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
    <div className="text-center mb-4">
      <span className="text-sm font-medium text-slate-500 dark:text-slate-400">The Combination Formula</span>
    </div>
    <div className="text-center text-lg font-mono text-slate-800 dark:text-slate-200">
      $Input = Embedding + PositionalEncoding$
    </div>
  </div>

  <Description>
    Simple addition! Element by element. The 768-dimensional embedding plus the 768-dimensional positional encoding gives a 768-dimensional input.
  </Description>

  <h4 className="text-lg font-semibold text-slate-800 dark:text-slate-200 mt-6 mb-3">Why Addition Works</h4>

  <Description>
    You might wonder: won't addition mix up the information? How can the model tell meaning from position?
  </Description>

  <Description>
    Think of it like mixing **paint color** and **paint texture**:
  </Description>

  <div className="my-4 space-y-3">
    <div className="flex items-center gap-3 p-3 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
      <span className="font-bold text-slate-700 dark:text-slate-300 w-36 shrink-0">Embedding (Color):</span>
      <span className="text-sm text-slate-700 dark:text-slate-300">"Apple" is <span className="font-semibold text-emerald-600 dark:text-emerald-400">Green</span></span>
    </div>
    <div className="flex items-center gap-3 p-3 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
      <span className="font-bold text-slate-700 dark:text-slate-300 w-36 shrink-0">Position (Texture):</span>
      <span className="text-sm text-slate-700 dark:text-slate-300">"Position 0" is <span className="font-semibold">Smooth</span>. "Position 5" is <span className="font-semibold">Bumpy</span>.</span>
    </div>
  </div>

  <Description>
    When we add them, we get a "Smooth Red" or "Bumpy Red" vector. Your brain can still perceive both the color and the texture as separate qualities. They don't get "lost."
  </Description>

  <Description>
    Neural networks work the same way. With enough training, the model learns to "un-mix" the combined signal. It recognizes the semantic features ("Red" ‚Üí fruit) and the positional features ("Smooth" ‚Üí early in sentence) as separate patterns.
  </Description>

  <Callout type="tip" title="Why Not Concatenate?">
    <div className="space-y-2">
      <div>We could glue the vectors end-to-end (concatenation), but that would <strong>double</strong> the vector size from 768 to 1536 dimensions.</div>
      <div>Addition keeps dimensions small and efficient, which matters when you're processing millions of tokens. The model is powerful enough to separate the signals.</div>
    </div>
  </Callout>
</Step>


<Step title="9. The Complete Pipeline">
  <Description>
    Let's trace through the entire process for the sentence "Alice gave Bob":
  </Description>

  <div className="my-4 relative">
    <div className="absolute left-6 top-0 bottom-0 w-0.5 bg-slate-200 dark:bg-slate-700" />
    
    <div className="space-y-6">
      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-slate-100 dark:bg-slate-800 border-2 border-slate-300 dark:border-slate-600 flex items-center justify-center text-slate-700 dark:text-slate-300 font-bold z-0">1</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-slate-800 dark:text-slate-200">Tokenize</div>
          <div className="font-mono text-sm text-slate-600 dark:text-slate-400 mt-1">
            "Alice gave Bob" ‚Üí [15496, 2921, 9352]
          </div>
        </div>
      </div>

      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-slate-100 dark:bg-slate-800 border-2 border-slate-300 dark:border-slate-600 flex items-center justify-center text-slate-700 dark:text-slate-300 font-bold z-0">2</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-slate-800 dark:text-slate-200">Look Up Embeddings</div>
          <div className="text-sm text-slate-600 dark:text-slate-400 mt-1">
            Each token ID retrieves its 768-dimensional semantic vector.
          </div>
          <div className="font-mono text-xs text-slate-500 mt-2">
            embed[15496] ‚Üí [0.12, -0.45, 0.78, ...] (768 values)
          </div>
        </div>
      </div>

      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-slate-100 dark:bg-slate-800 border-2 border-slate-300 dark:border-slate-600 flex items-center justify-center text-slate-700 dark:text-slate-300 font-bold z-0">3</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-slate-800 dark:text-slate-200">Compute Positional Encodings</div>
          <div className="text-sm text-slate-600 dark:text-slate-400 mt-1">
            Generate sine/cosine values for positions 0, 1, 2.
          </div>
          <div className="font-mono text-xs text-slate-500 mt-2">
            PE(0) ‚Üí [0.00, 1.00, 0.00, 1.00, ...] (768 values)<br/>
            PE(1) ‚Üí [0.84, 0.54, 0.01, 1.00, ...]<br/>
            PE(2) ‚Üí [0.91, -0.42, 0.02, 1.00, ...]
          </div>
        </div>
      </div>

      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-slate-100 dark:bg-slate-800 border-2 border-slate-300 dark:border-slate-600 flex items-center justify-center text-slate-700 dark:text-slate-300 font-bold z-0">4</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-slate-800 dark:text-slate-200">Add Together</div>
          <div className="text-sm text-slate-600 dark:text-slate-400 mt-1">
            Combine meaning and position into final input vectors.
          </div>
          <div className="font-mono text-xs text-emerald-600 dark:text-emerald-400 mt-2">
            input[0] = embed("Alice") + PE(0) ‚Üí "Alice at position 0"<br/>
            input[1] = embed("gave") + PE(1) ‚Üí "gave at position 1"<br/>
            input[2] = embed("Bob") + PE(2) ‚Üí "Bob at position 2"
          </div>
        </div>
      </div>

      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-slate-100 dark:bg-slate-800 border-2 border-slate-300 dark:border-slate-600 flex items-center justify-center text-slate-700 dark:text-slate-300 font-bold z-0">5</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-slate-800 dark:text-slate-200">Ready for Attention!</div>
          <div className="text-sm text-slate-600 dark:text-slate-400 mt-1">
            These position-aware embeddings flow into the attention mechanism, where tokens will "talk" to each other knowing exactly who sits where.
          </div>
        </div>
      </div>
    </div>
  </div>
</Step>


<Step title="10. Learned vs. Sinusoidal: A Brief Note">
  <Description>
    The original Transformer paper used sinusoidal encodings, but there's an alternative: **learned positional embeddings**.
  </Description>

  <div className="my-6 grid grid-cols-1 md:grid-cols-2 gap-4">
    <div className="p-5 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
      <div className="flex items-center gap-3 mb-3">
        <div className="w-8 h-8 rounded-full bg-slate-200 dark:bg-slate-700 flex items-center justify-center text-slate-600 dark:text-slate-400 font-bold text-sm">S</div>
        <span className="font-bold text-lg text-slate-800 dark:text-slate-200">Sinusoidal</span>
      </div>
      <div className="text-sm text-slate-600 dark:text-slate-400 space-y-2">
        <div className="flex items-center gap-2"><span className="text-slate-400">‚Ä¢</span> No parameters to learn</div>
        <div className="flex items-center gap-2"><span className="text-slate-400">‚Ä¢</span> Generalizes to any length</div>
        <div className="flex items-center gap-2"><span className="text-slate-400">‚Ä¢</span> Built-in relative distance</div>
        <div className="flex items-center gap-2"><span className="text-slate-400">‚Ä¢</span> Fixed pattern (not task-specific)</div>
      </div>
    </div>

    <div className="p-5 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
      <div className="flex items-center gap-3 mb-3">
        <div className="w-8 h-8 rounded-full bg-slate-200 dark:bg-slate-700 flex items-center justify-center text-slate-600 dark:text-slate-400 font-bold text-sm">L</div>
        <span className="font-bold text-lg text-slate-800 dark:text-slate-200">Learned</span>
      </div>
      <div className="text-sm text-slate-600 dark:text-slate-400 space-y-2">
        <div className="flex items-center gap-2"><span className="text-slate-400">‚Ä¢</span> Can adapt to the task</div>
        <div className="flex items-center gap-2"><span className="text-slate-400">‚Ä¢</span> Potentially better performance</div>
        <div className="flex items-center gap-2"><span className="text-slate-400">‚Ä¢</span> Limited to trained length</div>
        <div className="flex items-center gap-2"><span className="text-slate-400">‚Ä¢</span> More parameters</div>
      </div>
    </div>
  </div>

  <Description>
    Modern models like GPT often use **learned** positional embeddings. They simply add a learnable embedding table (like we have for tokens) where row 0 is "position 0", row 1 is "position 1", etc. The model learns what each position should "feel like" during training.
  </Description>

  <Description>
    The original paper found both approaches work about equally well. Sinusoidal has the elegance of extrapolation; learned has the flexibility of adaptation. Many newer architectures explore even more sophisticated approaches like **Rotary Position Embedding (RoPE)** or **ALiBi**, but they all build on these foundational ideas.
  </Description>
</Step>


<Step title="11. Summary">
  <Callout type="success" title="What We Learned">
    * **Position matters**: "Alice gave Bob" ‚â† "Bob gave Alice"
    * **Integer indexing fails**: Unbounded values, no generalization, magnitude problems
    * **0-1 normalization fails**: Resolution varies with length, requires future knowledge
    * **Sinusoidal encoding solves everything**:
      - Bounded values (-1 to +1)
      - Unique fingerprint for each position
      - Relative distances are learnable linear transforms
      - Generalizes to any sequence length
      - No parameters needed
    * **Simple addition** combines meaning (embedding) with position (PE)
    * **The result**: Position-aware vectors ready for attention
  </Callout>

  <Description>
    We've now completed the input representation pipeline:
  </Description>

  <div className="my-4 p-4 bg-slate-100 dark:bg-slate-800/60 rounded-lg border border-slate-200 dark:border-slate-700">
    <div className="text-center font-mono text-sm text-slate-800 dark:text-slate-200">
      Text ‚Üí Bytes ‚Üí Tokens ‚Üí Embeddings ‚Üí <span className="text-emerald-600 dark:text-emerald-400 font-bold">+ Positional Encoding</span> ‚Üí Input Vectors
    </div>
  </div>

  <Description>
    Each word is now a rich 768-dimensional vector that captures both **what** it means and **where** it appears. These vectors are ready to flow into the core of the Transformer: the **Attention Mechanism**.
  </Description>

  <Description>
    In the next chapter, we'll discover how attention lets words "talk" to each other, allowing "bank" to figure out whether it means a financial institution or the edge of a river based on the words around it.
  </Description>
</Step>
