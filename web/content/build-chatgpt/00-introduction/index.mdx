---
title: "Introduction"
step: 0
description: "The big picture: how Large Language Models actually work."
---

<div className="flex flex-col gap-4">
<Description>
  You type a question into ChatGPT. A few seconds later, a thoughtful response appears, word by word. It feels like magic, like the computer is *thinking*. But what's actually happening inside?
</Description>

<Description>
  In this chapter, we will see the full picture of how LLMs work before diving into technical details. Understanding how all the pieces fit together will make each chapter that follows much easier to grasp.
</Description>
</div>


<Step title="1. The Core Idea">
  <Description>
    At its heart, a Large Language Model does one thing: **predict the next word**.
  </Description>

  <Description>
    When you give it "The cat sat on the", the model looks at those words and outputs a probability for every possible next word in its vocabulary.
  </Description>

  <div className="text-xs font-medium text-muted uppercase tracking-wider mb-3">Iteration 1</div>
  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800">
    <div className="flex items-start gap-4">
      <div className="flex items-center gap-1.5 flex-wrap font-mono text-base">
        <span className="px-2.5 py-1 bg-zinc-800 rounded">The</span>
        <span className="px-2.5 py-1 bg-zinc-800 rounded">cat</span>
        <span className="px-2.5 py-1 bg-zinc-800 rounded">sat</span>
        <span className="px-2.5 py-1 bg-zinc-800 rounded">on</span>
        <span className="px-2.5 py-1 bg-zinc-800 rounded">the</span>
      </div>
      <span className="text-muted mt-1 text-lg">→</span>
      <div className="flex flex-col gap-1.5 font-mono text-base">
        <span className="px-2.5 py-1 bg-emerald-500/10 border border-emerald-500/30 rounded text-emerald-400">mat (15%)</span>
        <span className="px-2.5 py-1 bg-surface rounded text-muted">floor (12%)</span>
        <span className="px-2.5 py-1 bg-surface rounded text-muted">roof (3%)</span>
        <span className="text-muted text-sm">...</span>
      </div>
    </div>
  </div>

  <Description>
    The model picks one word (usually sampling from the high-probability ones), appends it to the input, and repeats.
  </Description>

  <div className="text-xs font-medium text-muted uppercase tracking-wider mb-3">Iteration 2</div>
  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800">
    <div className="flex items-start gap-4">
      <div className="flex items-center gap-1.5 flex-wrap font-mono text-base">
        <span className="px-2.5 py-1 bg-zinc-800 rounded">The</span>
        <span className="px-2.5 py-1 bg-zinc-800 rounded">cat</span>
        <span className="px-2.5 py-1 bg-zinc-800 rounded">sat</span>
        <span className="px-2.5 py-1 bg-zinc-800 rounded">on</span>
        <span className="px-2.5 py-1 bg-zinc-800 rounded">the</span>
        <span className="px-2.5 py-1 bg-emerald-500/20 rounded text-emerald-400">mat</span>
      </div>
      <span className="text-muted mt-1 text-lg">→</span>
      <div className="flex flex-col gap-1.5 font-mono text-base">
        <span className="px-2.5 py-1 bg-emerald-500/10 border border-emerald-500/30 rounded text-emerald-400">and (18%)</span>
        <span className="px-2.5 py-1 bg-surface rounded text-muted">. (14%)</span>
        <span className="px-2.5 py-1 bg-surface rounded text-muted">while (8%)</span>
        <span className="text-muted text-sm">...</span>
      </div>
    </div>
  </div>

  <Description>
    This loop continues until the response is complete. This simple mechanism, repeated many times during training on massive text data, produces behavior that looks remarkably like understanding.
  </Description>
</Step>


<Step title="2. The End-to-End Journey">
  <Description>
    When you send a message to an LLM and receive a response, your text goes through a series of transformations. Each step solves a specific problem.
  </Description>

  <div className="my-4 relative">
    <div className="absolute left-6 top-0 bottom-0 w-0.5 bg-zinc-800" />
    
    <div className="space-y-6">
      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-surface border-2 border-border flex items-center justify-center text-primary font-bold z-0">1</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-primary">Text → Bytes</div>
          <div className="font-mono text-sm text-secondary mt-1">"Hello" → [72, 101, 108, 108, 111]</div>
          <p className="text-sm text-muted mt-2">Computers only understand numbers. Every character must be converted to a numerical representation.</p>
        </div>
      </div>

      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-surface border-2 border-border flex items-center justify-center text-primary font-bold z-0">2</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-primary">Bytes → Tokens</div>
          <div className="font-mono text-sm text-secondary mt-1">[72, 101, 108, 108, 111] → [15496]</div>
          <p className="text-sm text-muted mt-2">Processing individual bytes is inefficient. Common patterns like "the" or "ing" become single tokens, making sequences shorter.</p>
        </div>
      </div>

      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-surface border-2 border-border flex items-center justify-center text-primary font-bold z-0">3</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-primary">Tokens → Vectors</div>
          <div className="font-mono text-sm text-secondary mt-1">[15496] → [0.12, -0.48, 0.91, ...]</div>
          <p className="text-sm text-muted mt-2">Token IDs are arbitrary labels. Vectors let the model learn semantic meaning, like understanding that "king" and "queen" are related.</p>
        </div>
      </div>

      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-surface border-2 border-border flex items-center justify-center text-primary font-bold z-0">4</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-primary">Vectors → Transformer → Next Token</div>
          <p className="text-sm text-secondary mt-1">A Transformer is the architecture behind modern LLMs. It processes the vectors and predicts which token comes next.</p>
          <p className="text-sm text-muted mt-2">It scans the sentence for context and uses it to pick the next word that keeps the meaning and flow on track.</p>
        </div>
      </div>

      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-surface border-2 border-border flex items-center justify-center text-primary font-bold z-0">5</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-primary">Token → Text</div>
          <p className="text-sm text-secondary mt-1">The predicted token is converted back to text and shown to you. This loops until the response is complete.</p>
          <p className="text-sm text-muted mt-2">This is why you see the response appear word by word.</p>
        </div>
      </div>
    </div>
  </div>
</Step>


<Step title="3. Learning from Data">
  <Description>
    LLMs are built to predict the next word, but how do they get *good* at it? A freshly created model is just millions of random numbers. It knows nothing about language, grammar, or the world. If you asked it to complete "The cat sat on the", it might confidently answer "purple" or "seventeen".
  </Description>

  <Description>
    The magic happens through training. We show the model enormous amounts of text and let it learn from its mistakes, over and over, until patterns emerge.
  </Description>

  <div className="my-6 relative">
    <div className="absolute left-6 top-0 bottom-0 w-0.5 bg-zinc-800" />
    
    <div className="space-y-6">
      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-surface border-2 border-border flex items-center justify-center text-primary font-bold z-0">1</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-primary">Input</div>
          <div className="font-mono text-sm text-secondary mt-1">"The king sat on the"</div>
        </div>
      </div>

      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-surface border-2 border-border flex items-center justify-center text-primary font-bold z-0">2</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-primary">Prediction</div>
          <p className="text-sm text-secondary mt-1">The model guesses the next word. At first, it's random (e.g., "banana").</p>
        </div>
      </div>

      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-surface border-2 border-border flex items-center justify-center text-primary font-bold z-0">3</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-primary">Comparison</div>
          <p className="text-sm text-secondary mt-1">The actual next word is "throne". We measure the error.</p>
        </div>
      </div>

      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-surface border-2 border-border flex items-center justify-center text-primary font-bold z-0">4</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-primary">Update</div>
          <p className="text-sm text-secondary mt-1">The model adjusts its internal numbers to make "throne" more likely next time.</p>
        </div>
      </div>
    </div>
  </div>

  <Description>
    By repeating this process billions of times on massive datasets, the model's parameters gradually tune themselves. Words with similar meanings align in the vector space, and the model learns to recognize complex patterns, grammar, and facts.
  </Description>
</Step>


<Step title="4. What You Will Build">
  <Description>
    In this course, we will build every piece of this pipeline from scratch. By the end, you will understand not just *what* an LLM does, but *how* and *why* each component exists.
  </Description>

  <Description>
    Let's start with the very first step: how computers see text.
  </Description>
</Step>
