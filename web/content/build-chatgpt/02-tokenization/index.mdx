---
title: "Tokenization"
step: 2
description: "How BPE compresses text into the atoms of language models."
---

<div className="flex flex-col gap-4">
<Description>
  In the previous chapter, we learned that computers see text as a stream of raw bytes. We also realized that feeding these bytes directly into the model is inefficient. Sequences become too long, and the model wastes capacity recognizing basic patterns like "the" over and over.
</Description>

<Description>
  We need a way to group bytes into meaningful chunks. This process is called **Tokenization**, and the industry-standard algorithm, used by GPT-4, Llama, and Claude, is **Byte Pair Encoding (BPE)**.
</Description>
</div>


<Step title="1. The Core Intuition: Tokenization is Compression">
  <Description>
    While the name "Byte Pair Encoding" sounds technical, the intuition is surprisingly simple: **BPE is a compression algorithm**.
  </Description>

  <Description>
    Think about how you text. You don't type "Laughing Out Loud" every time. That's 17 characters. Instead, you type "LOL" (3 characters). You've mentally agreed that this sequence appears so frequently it deserves to be a single unit.
  </Description>

  <div className="my-6 flex items-center justify-center gap-8">
    <div className="text-center">
      <div className="font-mono text-sm text-secondary">"Laughing Out Loud"</div>
      <div className="text-xs text-muted mt-1">17 characters</div>
    </div>
    <div className="text-xl text-muted">→</div>
    <div className="text-center">
      <div className="font-mono text-sm font-bold text-primary">"LOL"</div>
      <div className="text-xs text-muted mt-1">3 characters</div>
    </div>
  </div>

  <Description>
    BPE does exactly this, but **automatically**. It reads through massive amounts of text (like Wikipedia) and asks: *"Which sequences of characters appear together most often?"*
  </Description>

  <div className="my-6">
    <div className="text-xs font-medium text-muted uppercase tracking-wider mb-3">Common Patterns Learned</div>
    <div className="bg-[#121212] rounded-lg border border-zinc-800 divide-y divide-zinc-800">
      <div className="flex items-center gap-3 px-4 py-3">
        <span className="font-mono text-secondary">t + h</span>
        <span className="text-muted">→</span>
        <span className="font-mono font-bold text-primary">th</span>
        <span className="text-sm text-muted ml-auto">appears constantly together</span>
      </div>
      <div className="flex items-center gap-3 px-4 py-3">
        <span className="font-mono text-secondary">th + e</span>
        <span className="text-muted">→</span>
        <span className="font-mono font-bold text-primary">the</span>
        <span className="text-sm text-muted ml-auto">one of the most common words</span>
      </div>
      <div className="flex items-center gap-3 px-4 py-3">
        <span className="font-mono text-secondary">in + g</span>
        <span className="text-muted">→</span>
        <span className="font-mono font-bold text-primary">ing</span>
        <span className="text-sm text-muted ml-auto">common suffix</span>
      </div>
    </div>
  </div>

  <Description>
    By the end of this process, common words like "apple" become single tokens, while rare words remain as smaller chunks. This allows the model to process text much more efficiently.
  </Description>
</Step>


<Step title="2. The Algorithm: A Visual Walkthrough">
  <Description>
    To truly understand BPE, we need to run the algorithm by hand. We'll use a simple sequence to see the mechanics clearly before scaling up to real text.
  </Description>

  <h4 className="text-lg font-semibold text-primary mt-6 mb-3">The Dataset</h4>

  <Description>
    Imagine we are training a tokenizer on this string:
  </Description>

  <div className="my-6 flex flex-wrap justify-center gap-2">
    {['a', 'a', 'a', 'b', 'd', 'a', 'a', 'a', 'b', 'a', 'c'].map((char, i) => (
      <span key={i} className="w-8 h-8 flex items-center justify-center font-mono text-sm text-primary bg-zinc-800 rounded">{char}</span>
    ))}
  </div>

  <h4 className="text-lg font-semibold text-primary mt-6 mb-3">Iteration 0: The Starting Point</h4>

  <Description>
    Our vocabulary consists only of individual characters. The sequence has **11 tokens**.
  </Description>

  <div className="my-4 flex flex-wrap gap-2">
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">a</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">a</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">a</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">b</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">d</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">a</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">a</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">a</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">b</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">a</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">c</div>
  </div>

  <h4 className="text-lg font-semibold text-primary mt-6 mb-3">Step 1: Count the Pairs</h4>

  <Description>
    The algorithm scans every **adjacent pair**, each token paired with the one right next to it. Here's how to see the pairs:
  </Description>

  <div className="text-xs font-medium text-muted uppercase tracking-wider mb-3">Adjacent pairs in our sequence (same color = same pair type)</div>
  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800">
    <div className="flex flex-wrap gap-x-1 gap-y-4 font-mono text-lg">
      {/* (a,a) - amber */}
      <div className="flex items-end">
        <span className="px-2 py-1 bg-amber-100 dark:bg-amber-900/40 rounded-t border-b-2 border-amber-500">a</span>
        <span className="px-2 py-1 bg-amber-100 dark:bg-amber-900/40 rounded-t border-b-2 border-amber-500">a</span>
      </div>
      {/* (a,a) - amber */}
      <div className="flex items-end">
        <span className="px-2 py-1 bg-amber-100 dark:bg-amber-900/40 rounded-t border-b-2 border-amber-500">a</span>
        <span className="px-2 py-1 bg-amber-100 dark:bg-amber-900/40 rounded-t border-b-2 border-amber-500">a</span>
      </div>
      {/* (a,b) - violet */}
      <div className="flex items-end">
        <span className="px-2 py-1 bg-violet-100 dark:bg-violet-900/40 rounded-t border-b-2 border-violet-500">a</span>
        <span className="px-2 py-1 bg-violet-100 dark:bg-violet-900/40 rounded-t border-b-2 border-violet-500">b</span>
      </div>
      {/* (b,d) - rose */}
      <div className="flex items-end">
        <span className="px-2 py-1 bg-rose-100 dark:bg-rose-900/40 rounded-t border-b-2 border-rose-500">b</span>
        <span className="px-2 py-1 bg-rose-100 dark:bg-rose-900/40 rounded-t border-b-2 border-rose-500">d</span>
      </div>
      {/* (d,a) - orange */}
      <div className="flex items-end">
        <span className="px-2 py-1 bg-orange-100 dark:bg-orange-900/40 rounded-t border-b-2 border-orange-500">d</span>
        <span className="px-2 py-1 bg-orange-100 dark:bg-orange-900/40 rounded-t border-b-2 border-orange-500">a</span>
      </div>
      {/* (a,a) - amber */}
      <div className="flex items-end">
        <span className="px-2 py-1 bg-amber-100 dark:bg-amber-900/40 rounded-t border-b-2 border-amber-500">a</span>
        <span className="px-2 py-1 bg-amber-100 dark:bg-amber-900/40 rounded-t border-b-2 border-amber-500">a</span>
      </div>
      {/* (a,a) - amber */}
      <div className="flex items-end">
        <span className="px-2 py-1 bg-amber-100 dark:bg-amber-900/40 rounded-t border-b-2 border-amber-500">a</span>
        <span className="px-2 py-1 bg-amber-100 dark:bg-amber-900/40 rounded-t border-b-2 border-amber-500">a</span>
      </div>
      {/* (a,b) - violet */}
      <div className="flex items-end">
        <span className="px-2 py-1 bg-violet-100 dark:bg-violet-900/40 rounded-t border-b-2 border-violet-500">a</span>
        <span className="px-2 py-1 bg-violet-100 dark:bg-violet-900/40 rounded-t border-b-2 border-violet-500">b</span>
      </div>
      {/* (b,a) - teal */}
      <div className="flex items-end">
        <span className="px-2 py-1 bg-teal-100 dark:bg-teal-900/40 rounded-t border-b-2 border-teal-500">b</span>
        <span className="px-2 py-1 bg-teal-100 dark:bg-teal-900/40 rounded-t border-b-2 border-teal-500">a</span>
      </div>
      {/* (a,c) - pink */}
      <div className="flex items-end">
        <span className="px-2 py-1 bg-pink-100 dark:bg-pink-900/40 rounded-t border-b-2 border-pink-500">a</span>
        <span className="px-2 py-1 bg-pink-100 dark:bg-pink-900/40 rounded-t border-b-2 border-pink-500">c</span>
      </div>
    </div>
    <div className="mt-4 flex flex-wrap gap-x-4 gap-y-2 text-sm text-secondary">
      <span className="flex items-center gap-1.5"><span className="w-3 h-0.5 bg-amber-500"></span>(a,a) × 4</span>
      <span className="flex items-center gap-1.5"><span className="w-3 h-0.5 bg-violet-500"></span>(a,b) × 2</span>
      <span className="flex items-center gap-1.5"><span className="w-3 h-0.5 bg-rose-500"></span>(b,d) × 1</span>
      <span className="flex items-center gap-1.5"><span className="w-3 h-0.5 bg-orange-500"></span>(d,a) × 1</span>
      <span className="flex items-center gap-1.5"><span className="w-3 h-0.5 bg-teal-500"></span>(b,a) × 1</span>
      <span className="flex items-center gap-1.5"><span className="w-3 h-0.5 bg-pink-500"></span>(a,c) × 1</span>
    </div>
  </div>

  <div className="my-4 flex flex-wrap gap-2">
    <div className="px-3 py-2 bg-emerald-500/10 rounded border-2 border-emerald-400 text-center">
      <div className="font-mono text-sm font-bold text-emerald-400">(a,a)</div>
      <div className="text-lg font-bold text-emerald-400">4</div>
    </div>
    <div className="px-3 py-2 bg-surface rounded border border-border text-center">
      <div className="font-mono text-sm text-secondary">(a,b)</div>
      <div className="text-lg font-bold text-muted">2</div>
    </div>
    <div className="px-3 py-2 bg-surface rounded border border-border text-center">
      <div className="font-mono text-sm text-secondary">(b,d)</div>
      <div className="text-lg font-bold text-muted">1</div>
    </div>
    <div className="px-3 py-2 bg-surface rounded border border-border text-center">
      <div className="font-mono text-sm text-secondary">(d,a)</div>
      <div className="text-lg font-bold text-muted">1</div>
    </div>
    <div className="px-3 py-2 bg-surface rounded border border-border text-center">
      <div className="font-mono text-sm text-secondary">(b,a)</div>
      <div className="text-lg font-bold text-muted">1</div>
    </div>
    <div className="px-3 py-2 bg-surface rounded border border-border text-center">
      <div className="font-mono text-sm text-secondary">(a,c)</div>
      <div className="text-lg font-bold text-muted">1</div>
    </div>
  </div>

  <h4 className="text-lg font-semibold text-primary mt-6 mb-3">Step 2: The First Merge</h4>

  <Description>
    The winner is `(a, a)` with 4 occurrences. The algorithm creates a **new token** to represent this pair. Let's call it `Z`. We add `Z` to our vocabulary and replace every `aa` in our data.
  </Description>

  <div className="text-xs font-medium text-muted uppercase tracking-wider mb-3">After Merge #1: (a, a) → Z</div>
  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800">
    <div className="flex flex-wrap gap-2">
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-emerald-500 text-white font-mono font-bold">Z</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">a</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">b</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">d</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-emerald-500 text-white font-mono font-bold">Z</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">a</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">b</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">a</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">c</div>
    </div>
    <div className="mt-3 text-sm text-secondary">
      <strong>Length: 9 tokens</strong> (down from 11)
    </div>
  </div>

  <Callout type="tip" title="Key Insight">
    We just **compressed** the data. The information is identical, but the sequence is shorter. This is the core of BPE.
  </Callout>

  <h4 className="text-lg font-semibold text-primary mt-6 mb-3">Step 3: Repeat</h4>

  <Description>
    Now we count pairs again on the new sequence. Both `(Z, a)` and `(a, b)` appear twice. The algorithm picks one (let's choose `ab`). We create token `Y`.
  </Description>

  <div className="text-xs font-medium text-muted uppercase tracking-wider mb-3">After Merge #2: (a, b) → Y</div>
  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800">
    <div className="flex flex-wrap gap-2">
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-emerald-500 text-white font-mono font-bold">Z</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-emerald-400/20 border-2 border-emerald-400 text-emerald-400 font-mono font-bold">Y</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">d</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-emerald-500 text-white font-mono font-bold">Z</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-emerald-400/20 border-2 border-emerald-400 text-emerald-400 font-mono font-bold">Y</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">a</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-surface border-2 border-border font-mono font-bold text-primary">c</div>
    </div>
    <div className="mt-3 text-sm text-secondary">
      <strong>Length: 7 tokens</strong> (down from 9)
    </div>
  </div>

  <h4 className="text-lg font-semibold text-primary mt-6 mb-3">The Result</h4>

  <div className="my-4 p-5 bg-surface rounded-lg border border-border">
    <div className="grid grid-cols-3 gap-4 text-center">
      <div>
        <div className="text-3xl font-bold text-muted">11</div>
        <div className="text-sm text-muted">Starting tokens</div>
      </div>
      <div className="flex items-center justify-center">
        <div className="text-2xl text-muted">→</div>
      </div>
      <div>
        <div className="text-3xl font-bold text-primary">7</div>
        <div className="text-sm text-muted">After 2 merges</div>
      </div>
    </div>
    <div className="mt-4 pt-4 border-t border-border text-center text-sm text-secondary">
      Scaled to the entire internet, BPE typically achieves <strong>50-60% compression</strong> compared to raw bytes.
    </div>
  </div>
</Step>


<Step title="3. Implementing BPE in Python">
  <Description>
    Now let's build the actual code. This is the exact logic used inside GPT-2 and GPT-4.
  </Description>

  <Description>
    In the walkthrough above, we used letters like `a`, `b`, `c` to make the algorithm easy to follow. But remember what we learned in Chapter 1: text is already a sequence of integers. When you encode `"hello"` as UTF-8, you get `[104, 101, 108, 108, 111]`. These byte values range from 0 to 255, giving us our **base vocabulary of 256 tokens**. When BPE merges a frequent pair, it needs to assign a new ID to that merged token. Since 0-255 are already taken by the raw bytes, new tokens start at **256**, then 257, 258, and so on.
  </Description>

  <h4 className="text-lg font-semibold text-primary mt-6 mb-3">Breaking Down the Algorithm</h4>

  <Description>
    Looking back at our BPE walkthrough, the algorithm repeats three operations: **count** pairs, **pick** the most frequent one, and **merge** it. To implement this, we need three functions that work together:
  </Description>

  <div className="my-6 relative">
    <div className="absolute left-6 top-0 bottom-0 w-0.5 bg-zinc-800" />
    
    <div className="space-y-5">
      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-surface border-2 border-border flex items-center justify-center text-primary font-bold z-0">1</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-primary font-mono">get_stats</div>
          <p className="text-sm text-secondary mt-1">Counts how often each adjacent pair appears</p>
        </div>
      </div>

      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-surface border-2 border-border flex items-center justify-center text-primary font-bold z-0">2</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-primary font-mono">merge</div>
          <p className="text-sm text-secondary mt-1">Replaces all occurrences of a pair with a new token</p>
        </div>
      </div>

      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-surface border-2 border-border flex items-center justify-center text-primary font-bold z-0">3</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-primary font-mono">train_bpe</div>
          <p className="text-sm text-secondary mt-1">The main loop that calls the above until we reach target vocab size</p>
        </div>
      </div>
    </div>
  </div>

  <Description>
    Let's implement each one.
  </Description>

  <h4 className="text-lg font-semibold text-primary mt-6 mb-3">Function 1: Count Pair Statistics</h4>

  <Description>
    The first function scans a list of integers and returns a dictionary counting how often each adjacent pair occurs. For example, `[1, 2, 3, 1, 2]` should return `{(1, 2): 2, (2, 3): 1, (3, 1): 1}`.
  </Description>

```python
def get_stats(ids: list[int]) -> dict[tuple, int]:
    """
    Given a list of integers, return a dictionary of counts of consecutive pairs.
    Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}
    """
    counts = {}
    # Loop through the list and look at each element and its neighbor
    # Your implementation here...
    return counts
```

  <Callout type="note" title="Challenge: The Pair Counter">
    Try implementing this yourself! Switch to the **Challenge** tab to test your solution.
  </Callout>

  <h4 className="text-lg font-semibold text-primary mt-6 mb-3">Function 2: Merge a Pair</h4>

  <Description>
    The second function takes the token list and replaces every occurrence of a target pair with a new token ID.
  </Description>

```python
def merge(ids: list[int], pair: tuple[int, int], new_id: int) -> list[int]:
    """
    In the list of integers (ids), replace all consecutive occurrences 
    of pair with the new token new_id.
    """
    newids = []
    i = 0
    while i < len(ids):
        # Check if we found the pair (and aren't at the end)
        # If match: append new_id and skip ahead by 2 (we consumed both elements)
        # If no match: append current element and move ahead by 1
        pass
    return newids
```

  <Callout type="note" title="Challenge: The Token Merger">
    This one requires careful index management. Switch to the **Challenge** tab to try it!
  </Callout>

  <h4 className="text-lg font-semibold text-primary mt-6 mb-3">Function 3: The Training Loop</h4>

  <Description>
    Finally, we combine everything into a training loop. We start with UTF-8 bytes (vocab size 256) and keep merging until we reach a target vocabulary size.
  </Description>

  <Description>
    GPT-4, for example, stops at roughly **100,000 tokens**. This is a *hyperparameter*, a number we choose before training to balance efficiency (more tokens = shorter sequences) vs. memory (larger embedding table).
  </Description>

```python
def train_bpe(text: str, num_merges: int) -> tuple[list[int], dict]:
    ids = list(text.encode("utf-8"))  # Start with raw bytes
    merges = {}  # Will store our learned merge rules
    
    for i in range(num_merges):
        # Find the most frequent pair, merge it, record the rule
        # New token IDs start at 256 (after the 0-255 byte range)
        pass
    
    return ids, merges
```

  <Callout type="note" title="Challenge: The BPE Trainer">
    Put it all together! Switch to the **Challenge** tab to implement the complete training function.
  </Callout>

  <Description>
    After training, `merges` contains all the rules needed to tokenize new text, and `ids` contains your compressed training data.
  </Description>
</Step>


<Step title="4. Regex Pre-splitting">
  <Description>
    If you ran the code above on English text, you'd have a working tokenizer, but it would have a flaw.
  </Description>

  <Description>
    Consider the string: `"dog dog. dog!"`
  </Description>

  <Description>
    To a human, the core concept is **"dog"**. But our simple algorithm treats spaces and punctuation as characters to merge. It might see `dog.` as completely different from `dog ` (with a space). It might even merge `g` and `.` into a weird token `g.`.
  </Description>

  <div className="text-xs font-medium text-muted uppercase tracking-wider mb-3">The Problem</div>
  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800">
    <div className="font-mono text-sm space-y-1 text-secondary">
      <div>"dog" + space → one pattern</div>
      <div>"dog" + period → different pattern</div>
      <div>"dog" + exclamation → yet another pattern</div>
    </div>
    <div className="mt-2 text-sm text-secondary">
      The model has to learn "dog" three separate times!
    </div>
  </div>

  <h4 className="text-lg font-semibold text-primary mt-6 mb-3">The Fix: Regex Splitting</h4>

  <Description>
    Modern tokenizers (like GPT-4's) use **Regular Expressions** to pre-split text into meaningful chunks *before* applying BPE. They force boundaries between words and punctuation.
  </Description>

  <div className="text-xs font-medium text-muted uppercase tracking-wider mb-3">The Solution</div>
  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800">
    <div className="space-y-2">
      <div className="flex items-center gap-3">
        <span className="text-sm text-secondary">Input:</span>
        <span className="font-mono bg-background px-2 py-1 rounded border border-border">"dog. dog!"</span>
      </div>
      <div className="flex items-center gap-3 flex-wrap">
        <span className="text-sm text-secondary">After regex:</span>
        <div className="flex gap-1 flex-wrap">
          <span className="font-mono bg-zinc-800 px-2 py-1 rounded text-primary">"dog"</span>
          <span className="font-mono bg-surface px-2 py-1 rounded text-muted">"."</span>
          <span className="font-mono bg-zinc-800 px-2 py-1 rounded text-primary">" dog"</span>
          <span className="font-mono bg-surface px-2 py-1 rounded text-muted">"!"</span>
        </div>
      </div>
    </div>
    <div className="mt-3 text-sm text-secondary">
      BPE runs on each chunk independently. "dog" is always tokenized as "dog".
    </div>
  </div>

  <Description>
    This semantic separation helps the model learn language patterns much faster because it doesn't waste capacity learning that "dog." and "dog," are the same word.
  </Description>
</Step>


<Step title="5. Encoding & Decoding">
  <Description>
    We've trained a tokenizer and learned merge rules. Now we need two more functions: one to **decode** tokens back into text, and one to **encode** new text into tokens. Let's start with the easier one.
  </Description>

  <h4 className="text-lg font-semibold text-primary mt-6 mb-3">The Vocabulary Table</h4>

  <Description>
    First, we need a **vocabulary table** that maps each token ID to its byte sequence. Remember from Chapter 1: text is just a sequence of integers (bytes). The letter `"a"` is byte `97`, `"b"` is `98`, and so on. Our vocabulary stores these byte sequences directly:
  </Description>

  <div className="text-xs font-medium text-muted uppercase tracking-wider mb-3">The Vocabulary: Token ID → Bytes</div>
  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800">
    <div className="space-y-3 font-mono text-sm">
      <div>
        <div className="text-muted mb-1"># Base vocabulary: single bytes (0-255)</div>
        <div className="text-primary">vocab[97] = [97] <span className="text-muted"># "a"</span></div>
        <div className="text-primary">vocab[98] = [98] <span className="text-muted"># "b"</span></div>
        <div className="text-primary">vocab[99] = [99] <span className="text-muted"># "c"</span></div>
        <div className="text-muted">...</div>
      </div>
      <div className="pt-2 border-t border-border">
        <div className="text-muted mb-1"># Merged tokens: pre-computed byte sequences</div>
        <div className="text-primary">vocab[256] = [97, 97] <span className="text-muted"># "aa"</span></div>
        <div className="text-primary">vocab[257] = [97, 98] <span className="text-muted"># "ab"</span></div>
        <div className="text-primary">vocab[258] = [97, 97, 98] <span className="text-muted"># "aab"</span></div>
        <div className="text-muted">...</div>
      </div>
    </div>
  </div>

  <Description>
    The key insight: each token ID points directly to its byte sequence, no calculation needed at runtime. When we see token `256`, we just grab `[97, 97]` from the dictionary. This is why decoding is so fast.
  </Description>

  <Callout type="tip" title="Building the Vocabulary During Training">
    When you create a new merged token, immediately store its bytes by concatenating the bytes of the first token and the bytes of the second token: `vocab[new_id] = vocab[pair[0]] + vocab[pair[1]]`. After training, every token’s bytes are ready for instant lookup.
  </Callout>

  <h4 className="text-lg font-semibold text-primary mt-6 mb-3">Decoding: Tokens → Text</h4>

  <Description>
    With a flattened vocabulary, decoding is just a dictionary lookup:
  </Description>

```python
def decode(ids: list[int], vocab: dict[int, list[int]]) -> str:
    """Convert token IDs back to text."""
    # Look up each ID in vocab, concatenate the bytes, decode as UTF-8
    pass
```

  <Callout type="note" title="Challenge: The Decoder">
    An easy win! Switch to the **Challenge** tab to implement it.
  </Callout>

  <h4 className="text-lg font-semibold text-primary mt-6 mb-3">Visual Example: Decoding</h4>

  <div className="text-xs font-medium text-muted uppercase tracking-wider mb-3">Decoding [258, 99] back to text</div>
  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800">
    <div className="space-y-4">
      <div className="flex items-center gap-3">
        <div className="text-sm text-muted w-28">Token IDs:</div>
        <div className="flex gap-2">
          <div className="px-3 py-2 bg-zinc-800 rounded font-mono text-sm">258</div>
          <div className="px-3 py-2 bg-zinc-800 rounded font-mono text-sm">99</div>
        </div>
      </div>
      <div className="flex items-center gap-3">
        <div className="text-sm text-muted w-28">Look up bytes:</div>
        <div className="flex gap-3 items-center">
          <div className="flex flex-col items-center">
            <div className="text-xs text-muted mb-1">vocab[258]</div>
            <div className="px-3 py-2 bg-emerald-500/10 rounded font-mono text-sm border border-emerald-500/30">[97, 97, 98]</div>
          </div>
          <div className="flex flex-col items-center">
            <div className="text-xs text-muted mb-1">vocab[99]</div>
            <div className="px-3 py-2 bg-emerald-500/10 rounded font-mono text-sm border border-emerald-500/30">[99]</div>
          </div>
        </div>
      </div>
      <div className="flex items-center gap-3">
        <div className="text-sm text-muted w-28">Concatenate:</div>
        <div className="px-3 py-2 bg-emerald-500/10 rounded font-mono text-sm border border-emerald-500/30">[97, 97, 98, 99]</div>
      </div>
      <div className="flex items-center gap-3">
        <div className="text-sm text-muted w-28">Decode UTF-8:</div>
        <div className="px-3 py-2 bg-emerald-500 text-white rounded font-mono text-sm font-bold">"aabc"</div>
      </div>
    </div>
  </div>

  <h4 className="text-lg font-semibold text-primary mt-6 mb-3">Encoding: Text → Tokens</h4>

  <Description>
    Encoding involves a few more steps. To tokenize new text, we apply our learned merge rules **in the exact order we learned them**.
  </Description>

```python
def encode(text: str, merges: dict[tuple[int, int], int]) -> list[int]:
    """Tokenize text using learned merge rules."""
    ids = list(text.encode("utf-8"))  # Start with raw bytes
    
    # Apply each merge rule in the order it was learned
    # Hint: iterate through merges.items() and use your merge function
    pass
```
  <Description>
    Since Python dictionaries preserve insertion order, we can simply iterate through `merges` and apply each rule. But why does order matter? Some merge rules depend on tokens created by earlier merges. If we learned `(97,97)→256` first and `(256,98)→257` second, the second rule can only match after the first rule has created token 256. Applying them out of order means the second rule looks for a token that doesn't exist yet, so it gets skipped. Let's see this with an example.
  </Description>

  {/* Why Order Matters - Terminal Style Comparison */}
  <div className="my-8">
    <h4 className="text-base font-semibold text-primary mb-2">Why Order Matters: Encoding "aab" with Two Merges</h4>
    <p className="text-sm text-secondary mb-5">
      We have two merge rules: <code className="font-mono text-xs bg-zinc-800 px-1.5 py-0.5 rounded">merge #1: (97,97)→256</code> and <code className="font-mono text-xs bg-zinc-800 px-1.5 py-0.5 rounded">merge #2: (256,98)→257</code>. The order we apply them changes the result.
    </p>
    
    <div className="grid grid-cols-1 md:grid-cols-2 gap-4">
      {/* Valid Path */}
      <div className="p-4 bg-background rounded-lg border border-emerald-500/20">
        <div className="flex items-center gap-2 mb-4">
          <span className="text-emerald-400">✓</span>
          <span className="text-sm font-semibold text-primary">Correct Order: #1 then #2</span>
        </div>
        
        <div className="space-y-4">
          {/* Step 1 */}
          <div className="pb-3 border-b border-border">
            <div className="text-xs text-emerald-400 mb-1.5 font-medium">Step 1 · Apply merge #1: (97,97)→256</div>
            <div className="font-mono text-sm text-secondary">
              [97, 97, 98] <span className="text-muted">→</span> [<span className="text-emerald-400 font-bold">256</span>, 98]
            </div>
          </div>
          
          {/* Step 2 */}
          <div>
            <div className="text-xs text-emerald-400 mb-1.5 font-medium">Step 2 · Apply merge #2: (256,98)→257</div>
            <div className="font-mono text-sm text-secondary">
              [256, 98] <span className="text-muted">→</span> [<span className="text-emerald-400 font-bold">257</span>]
            </div>
          </div>
        </div>
        
        {/* Result */}
        <div className="mt-4 pt-3 border-t border-border">
          <span className="text-xs text-emerald-400 font-semibold">Result: [257]</span>
          <span className="text-xs text-muted ml-2">(1 token)</span>
        </div>
      </div>
      
      {/* Invalid Path */}
      <div className="p-4 bg-background rounded-lg border border-rose-500/20">
        <div className="flex items-center gap-2 mb-4">
          <span className="text-rose-400">✗</span>
          <span className="text-sm font-semibold text-primary">Wrong Order: #2 then #1</span>
        </div>
        
        <div className="space-y-4">
          {/* Step 1 */}
          <div className="pb-3 border-b border-border">
            <div className="text-xs text-rose-400 mb-1.5 font-medium">Step 1 · Apply merge #2: (256,98)→257</div>
            <div className="font-mono text-sm text-secondary">
              [97, 97, 98] <span className="text-muted">→</span> [97, 97, 98] <span className="text-muted text-xs">(no 256 yet, skipped!)</span>
            </div>
          </div>
          
          {/* Step 2 */}
          <div>
            <div className="text-xs text-rose-400 mb-1.5 font-medium">Step 2 · Apply merge #1: (97,97)→256</div>
            <div className="font-mono text-sm text-secondary">
              [97, 97, 98] <span className="text-muted">→</span> [<span className="text-rose-400 font-bold">256</span>, 98]
            </div>
          </div>
        </div>
        
        {/* Result */}
        <div className="mt-4 pt-3 border-t border-border">
          <span className="text-xs text-rose-400 font-semibold">Result: [256, 98]</span>
          <span className="text-xs text-muted ml-2">(2 tokens, missed merge!)</span>
        </div>
      </div>
    </div>
  </div>

  <h4 className="text-lg font-semibold text-primary mt-6 mb-3">Visual Example: Encoding</h4>

  <div className="text-xs font-medium text-muted uppercase tracking-wider mb-3">Encoding "aaab" with merges: (97,97)→256, (256,97)→257</div>
  <div className="my-4 p-4 bg-[#121212] rounded-lg border border-zinc-800">
    <div className="space-y-3 font-mono text-sm">
      <div className="flex items-center gap-3">
        <span className="text-muted w-24">Start:</span>
        <span className="text-primary">[97, 97, 97, 98]</span>
        <span className="text-muted text-xs">← raw bytes: a, a, a, b</span>
      </div>
      <div className="flex items-center gap-3">
        <span className="text-muted w-24">Merge (a,a):</span>
        <span className="text-primary">[<span className="text-emerald-400 font-bold">256</span>, 97, 98]</span>
        <span className="text-muted text-xs">← (97,97) becomes 256</span>
      </div>
      <div className="flex items-center gap-3">
        <span className="text-muted w-24">Merge (256,a):</span>
        <span className="text-primary">[<span className="text-emerald-400 font-bold">257</span>, 98]</span>
        <span className="text-muted text-xs">← (256,97) becomes 257</span>
      </div>
      <div className="flex items-center gap-3 pt-2 border-t border-border">
        <span className="text-muted w-24">Result:</span>
        <span className="text-emerald-400 font-bold">[257, 98]</span>
        <span className="text-muted text-xs">← 4 bytes compressed to 2 tokens</span>
      </div>
    </div>
  </div>

  <Callout type="note" title="Challenge: The Encoder">
    The final boss! Implement encoding to tokenize new text. Switch to the **Challenge** tab.
  </Callout>
</Step>

<Step title="6. The Complete Pipeline">
  <Description>
    Let's step back and see the entire journey of text before it reaches the neural network.
  </Description>

  <div className="my-4 relative">
    <div className="absolute left-6 top-0 bottom-0 w-0.5 bg-zinc-800" />
    
    <div className="space-y-6">
      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-surface border-2 border-border flex items-center justify-center text-primary font-bold z-0">1</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-primary">Raw Text</div>
          <div className="font-mono text-sm text-secondary mt-1">"Hello World"</div>
        </div>
      </div>

      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-surface border-2 border-border flex items-center justify-center text-primary font-bold z-0">2</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-primary">Regex Split</div>
          <div className="font-mono text-sm text-secondary mt-1">["Hello", " World"]</div>
        </div>
      </div>

      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-surface border-2 border-border flex items-center justify-center text-primary font-bold z-0">3</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-primary">UTF-8 Encoding</div>
          <div className="font-mono text-sm text-secondary mt-1">[72, 101, 108, 108, 111], [32, 87, 111, 114, 108, 100]</div>
        </div>
      </div>

      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-surface border-2 border-border flex items-center justify-center text-primary font-bold z-0">4</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-primary">BPE Merging</div>
          <div className="text-sm text-secondary mt-1">Apply learned merge rules to compress bytes into tokens</div>
        </div>
      </div>

      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-surface border-2 border-border flex items-center justify-center text-primary font-bold z-0">5</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-primary">Token IDs</div>
          <div className="font-mono text-sm text-secondary mt-1">[15496, 995]</div>
          <div className="text-sm text-secondary mt-1">This is what the neural network actually sees</div>
        </div>
      </div>
    </div>
  </div>

  <Callout type="success" title="Summary">
    * **Tokenization** compresses text into efficient chunks called tokens
    * **BPE** learns merge rules by iteratively combining frequent pairs
    * **Regex pre-splitting** ensures words stay consistent regardless of punctuation
    * **Decoding** is a simple vocabulary lookup since we store each token's bytes
    * **Encoding** applies learned merges in order to tokenize new text
    * The final output is a sequence of **integer IDs**: the atoms of language models
  </Callout>

  <Description>
    These integers are the only thing the neural network ever sees. They are the atoms of the Large Language Model universe. In the next chapter, we'll see how the model turns these integers into **Vectors** (Embeddings) to begin understanding their meaning.
  </Description>
</Step>
