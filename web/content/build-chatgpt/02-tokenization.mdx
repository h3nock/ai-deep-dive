---
title: "Tokenization"
step: 2
description: "How BPE compresses text into the atoms of language models."
---

<div className="flex flex-col gap-4">
<Description>
  In the previous chapter, we learned that computers see text as a stream of raw bytes. We also realized that feeding these bytes directly into the model is inefficient. Sequences become too long, and the model wastes capacity recognizing basic patterns like "the" over and over.
</Description>

<Description>
  We need a way to group bytes into meaningful chunks. This process is called **Tokenization**, and the industry-standard algorithm, used by GPT-4, Llama, and Claude, is **Byte Pair Encoding (BPE)**.
</Description>
</div>


<Step title="1. The Core Intuition: Tokenization is Compression">
  <Description>
    While the name "Byte Pair Encoding" sounds technical, the intuition is surprisingly simple: **BPE is a compression algorithm**.
  </Description>

  <Description>
    Think about how you text. You don't type "Laughing Out Loud" every time. That's 17 characters. Instead, you type "LOL" (3 characters). You've mentally agreed that this sequence appears so frequently it deserves to be a single unit.
  </Description>

  <div className="my-4 p-4 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
    <div className="flex items-center justify-center gap-8">
      <div className="text-center">
        <div className="font-mono text-sm text-slate-500 dark:text-slate-400 px-3 py-2 bg-white dark:bg-slate-900 rounded border border-slate-200 dark:border-slate-700">"Laughing Out Loud"</div>
        <div className="text-xs text-slate-400 mt-2">17 characters</div>
      </div>
      <div className="text-xl text-slate-400">→</div>
      <div className="text-center">
        <div className="font-mono text-sm font-bold text-slate-800 dark:text-slate-200 px-3 py-2 bg-slate-100 dark:bg-slate-700 rounded border border-slate-300 dark:border-slate-600">"LOL"</div>
        <div className="text-xs text-slate-500 mt-2">3 characters</div>
      </div>
    </div>
  </div>

  <Description>
    BPE does exactly this, but **automatically**. It reads through massive amounts of text (like Wikipedia) and asks: *"Which sequences of characters appear together most often?"*
  </Description>

  <div className="my-4 space-y-2">
    <div className="flex items-center gap-3 p-3 bg-slate-50 dark:bg-slate-800/40 rounded-lg border border-slate-200 dark:border-slate-700">
      <span className="font-mono text-slate-500">t + h</span>
      <span className="text-slate-400">→</span>
      <span className="font-mono font-bold text-slate-800 dark:text-slate-200">th</span>
      <span className="text-sm text-slate-500 ml-auto">appears constantly together</span>
    </div>
    <div className="flex items-center gap-3 p-3 bg-slate-50 dark:bg-slate-800/40 rounded-lg border border-slate-200 dark:border-slate-700">
      <span className="font-mono text-slate-500">th + e</span>
      <span className="text-slate-400">→</span>
      <span className="font-mono font-bold text-slate-800 dark:text-slate-200">the</span>
      <span className="text-sm text-slate-500 ml-auto">one of the most common words</span>
    </div>
    <div className="flex items-center gap-3 p-3 bg-slate-50 dark:bg-slate-800/40 rounded-lg border border-slate-200 dark:border-slate-700">
      <span className="font-mono text-slate-500">in + g</span>
      <span className="text-slate-400">→</span>
      <span className="font-mono font-bold text-slate-800 dark:text-slate-200">ing</span>
      <span className="text-sm text-slate-500 ml-auto">common suffix</span>
    </div>
  </div>

  <Description>
    By the end of this process, common words like "apple" become single tokens, while rare words remain as smaller chunks. This allows the model to process text much more efficiently.
  </Description>
</Step>


<Step title="2. The Algorithm: A Visual Walkthrough">
  <Description>
    To truly understand BPE, we need to run the algorithm by hand. We'll use a simple sequence to see the mechanics clearly before scaling up to real text.
  </Description>

  <h4 className="text-lg font-semibold text-slate-800 dark:text-slate-200 mt-6 mb-3">The Dataset</h4>

  <Description>
    Imagine we are training a tokenizer on this string:
  </Description>

  <div className="my-4 p-4 bg-slate-900 dark:bg-slate-950 rounded-lg border border-slate-700">
    <div className="font-mono text-xl text-center text-white tracking-wider">
      a a a b d a a a b a c
    </div>
  </div>

  <h4 className="text-lg font-semibold text-slate-800 dark:text-slate-200 mt-6 mb-3">Iteration 0: The Starting Point</h4>

  <Description>
    Our vocabulary consists only of individual characters. The sequence has **11 tokens**.
  </Description>

  <div className="my-4 flex flex-wrap gap-2">
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-slate-100 dark:bg-slate-800 border-2 border-slate-200 dark:border-slate-700 font-mono font-bold text-slate-700 dark:text-slate-300">a</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-slate-100 dark:bg-slate-800 border-2 border-slate-200 dark:border-slate-700 font-mono font-bold text-slate-700 dark:text-slate-300">a</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-slate-100 dark:bg-slate-800 border-2 border-slate-200 dark:border-slate-700 font-mono font-bold text-slate-700 dark:text-slate-300">a</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-slate-100 dark:bg-slate-800 border-2 border-slate-200 dark:border-slate-700 font-mono font-bold text-slate-700 dark:text-slate-300">b</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-slate-100 dark:bg-slate-800 border-2 border-slate-200 dark:border-slate-700 font-mono font-bold text-slate-700 dark:text-slate-300">d</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-slate-100 dark:bg-slate-800 border-2 border-slate-200 dark:border-slate-700 font-mono font-bold text-slate-700 dark:text-slate-300">a</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-slate-100 dark:bg-slate-800 border-2 border-slate-200 dark:border-slate-700 font-mono font-bold text-slate-700 dark:text-slate-300">a</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-slate-100 dark:bg-slate-800 border-2 border-slate-200 dark:border-slate-700 font-mono font-bold text-slate-700 dark:text-slate-300">a</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-slate-100 dark:bg-slate-800 border-2 border-slate-200 dark:border-slate-700 font-mono font-bold text-slate-700 dark:text-slate-300">b</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-slate-100 dark:bg-slate-800 border-2 border-slate-200 dark:border-slate-700 font-mono font-bold text-slate-700 dark:text-slate-300">a</div>
    <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-slate-100 dark:bg-slate-800 border-2 border-slate-200 dark:border-slate-700 font-mono font-bold text-slate-700 dark:text-slate-300">c</div>
  </div>

  <h4 className="text-lg font-semibold text-slate-800 dark:text-slate-200 mt-6 mb-3">Step 1: Count the Pairs</h4>

  <Description>
    The algorithm scans every adjacent pair and counts how often they appear:
  </Description>

  <div className="my-4 grid grid-cols-2 md:grid-cols-4 gap-3">
    <div className="p-3 bg-emerald-50 dark:bg-emerald-900/30 rounded-lg border-2 border-emerald-500 dark:border-emerald-400 text-center">
      <div className="font-mono text-lg font-bold text-emerald-700 dark:text-emerald-300">(a, a)</div>
      <div className="text-2xl font-bold text-emerald-600 dark:text-emerald-400">4</div>
      <div className="text-xs text-emerald-600 dark:text-emerald-400">← most frequent</div>
    </div>
    <div className="p-3 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700 text-center">
      <div className="font-mono text-lg font-bold text-slate-600 dark:text-slate-400">(a, b)</div>
      <div className="text-2xl font-bold text-slate-500">2</div>
    </div>
    <div className="p-3 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700 text-center">
      <div className="font-mono text-lg font-bold text-slate-600 dark:text-slate-400">(b, d)</div>
      <div className="text-2xl font-bold text-slate-500">1</div>
    </div>
    <div className="p-3 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700 text-center">
      <div className="font-mono text-lg font-bold text-slate-600 dark:text-slate-400">(d, a)</div>
      <div className="text-2xl font-bold text-slate-500">1</div>
    </div>
  </div>

  <h4 className="text-lg font-semibold text-slate-800 dark:text-slate-200 mt-6 mb-3">Step 2: The First Merge</h4>

  <Description>
    The winner is `(a, a)` with 4 occurrences. The algorithm creates a **new token** to represent this pair. Let's call it `Z`. We add `Z` to our vocabulary and replace every `aa` in our data.
  </Description>

  <div className="my-4 p-4 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
    <div className="text-sm font-medium text-slate-700 dark:text-slate-300 mb-3">After Merge #1: (a, a) → Z</div>
    <div className="flex flex-wrap gap-2">
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-emerald-600 dark:bg-emerald-500 text-white font-mono font-bold">Z</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-slate-100 dark:bg-slate-800 border-2 border-slate-200 dark:border-slate-700 font-mono font-bold text-slate-700 dark:text-slate-300">a</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-slate-100 dark:bg-slate-800 border-2 border-slate-200 dark:border-slate-700 font-mono font-bold text-slate-700 dark:text-slate-300">b</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-slate-100 dark:bg-slate-800 border-2 border-slate-200 dark:border-slate-700 font-mono font-bold text-slate-700 dark:text-slate-300">d</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-emerald-600 dark:bg-emerald-500 text-white font-mono font-bold">Z</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-slate-100 dark:bg-slate-800 border-2 border-slate-200 dark:border-slate-700 font-mono font-bold text-slate-700 dark:text-slate-300">a</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-slate-100 dark:bg-slate-800 border-2 border-slate-200 dark:border-slate-700 font-mono font-bold text-slate-700 dark:text-slate-300">b</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-slate-100 dark:bg-slate-800 border-2 border-slate-200 dark:border-slate-700 font-mono font-bold text-slate-700 dark:text-slate-300">a</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-slate-100 dark:bg-slate-800 border-2 border-slate-200 dark:border-slate-700 font-mono font-bold text-slate-700 dark:text-slate-300">c</div>
    </div>
    <div className="mt-3 text-sm text-slate-600 dark:text-slate-400">
      <strong>Length: 9 tokens</strong> (down from 11)
    </div>
  </div>

  <Callout type="tip" title="Key Insight">
    We just **compressed** the data. The information is identical, but the sequence is shorter. This is the core of BPE.
  </Callout>

  <h4 className="text-lg font-semibold text-slate-800 dark:text-slate-200 mt-6 mb-3">Step 3: Repeat</h4>

  <Description>
    Now we count pairs again on the new sequence. Both `(Z, a)` and `(a, b)` appear twice. The algorithm picks one (let's choose `ab`). We create token `Y`.
  </Description>

  <div className="my-4 p-4 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
    <div className="text-sm font-medium text-slate-700 dark:text-slate-300 mb-3">After Merge #2: (a, b) → Y</div>
    <div className="flex flex-wrap gap-2">
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-emerald-600 dark:bg-emerald-500 text-white font-mono font-bold">Z</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-emerald-500/20 dark:bg-emerald-400/20 border-2 border-emerald-500 dark:border-emerald-400 text-emerald-700 dark:text-emerald-300 font-mono font-bold">Y</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-slate-100 dark:bg-slate-800 border-2 border-slate-200 dark:border-slate-700 font-mono font-bold text-slate-700 dark:text-slate-300">d</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-emerald-600 dark:bg-emerald-500 text-white font-mono font-bold">Z</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-emerald-500/20 dark:bg-emerald-400/20 border-2 border-emerald-500 dark:border-emerald-400 text-emerald-700 dark:text-emerald-300 font-mono font-bold">Y</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-slate-100 dark:bg-slate-800 border-2 border-slate-200 dark:border-slate-700 font-mono font-bold text-slate-700 dark:text-slate-300">a</div>
      <div className="w-10 h-10 flex items-center justify-center rounded-lg bg-slate-100 dark:bg-slate-800 border-2 border-slate-200 dark:border-slate-700 font-mono font-bold text-slate-700 dark:text-slate-300">c</div>
    </div>
    <div className="mt-3 text-sm text-slate-600 dark:text-slate-400">
      <strong>Length: 7 tokens</strong> (down from 9)
    </div>
  </div>

  <h4 className="text-lg font-semibold text-slate-800 dark:text-slate-200 mt-6 mb-3">The Result</h4>

  <div className="my-4 p-5 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
    <div className="grid grid-cols-3 gap-4 text-center">
      <div>
        <div className="text-3xl font-bold text-slate-400">11</div>
        <div className="text-sm text-slate-500">Starting tokens</div>
      </div>
      <div className="flex items-center justify-center">
        <div className="text-2xl text-slate-400">→</div>
      </div>
      <div>
        <div className="text-3xl font-bold text-slate-800 dark:text-slate-200">7</div>
        <div className="text-sm text-slate-500">After 2 merges</div>
      </div>
    </div>
    <div className="mt-4 pt-4 border-t border-slate-200 dark:border-slate-700 text-center text-sm text-slate-600 dark:text-slate-400">
      Scaled to the entire internet, BPE typically achieves <strong>50-60% compression</strong> compared to raw bytes.
    </div>
  </div>
</Step>


<Step title="3. Implementing BPE in Python">
  <Description>
    Now let's build the actual code. This is the exact logic used inside GPT-2 and GPT-4.
  </Description>

  <Description>
    In the walkthrough above, we used letters like `a`, `b`, `c` to make the algorithm easy to follow. But remember what we learned in Chapter 1: text is already a sequence of integers. When you encode `"hello"` as UTF-8, you get `[104, 101, 108, 108, 111]`. These byte values range from 0 to 255, giving us our **base vocabulary of 256 tokens**. When BPE merges a frequent pair, it needs to assign a new ID to that merged token. Since 0-255 are already taken by the raw bytes, new tokens start at **256**, then 257, 258, and so on.
  </Description>

  <h4 className="text-lg font-semibold text-slate-800 dark:text-slate-200 mt-6 mb-3">Function 1: Count Pair Statistics</h4>

  <Description>
    The first function scans a list of integers and returns a dictionary counting how often each adjacent pair occurs. For example, `[1, 2, 3, 1, 2]` should return `{(1, 2): 2, (2, 3): 1, (3, 1): 1}`.
  </Description>

```python
def get_stats(ids):
    """
    Given a list of integers, return a dictionary of counts of consecutive pairs.
    Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}
    """
    counts = {}
    # Loop through the list and look at each element and its neighbor
    # Your implementation here...
    return counts
```

  <Callout type="note" title="Challenge: The Pair Counter">
    Try implementing this yourself! Switch to the **Challenge** tab to test your solution.
  </Callout>

  <h4 className="text-lg font-semibold text-slate-800 dark:text-slate-200 mt-6 mb-3">Function 2: Merge a Pair</h4>

  <Description>
    The second function takes the token list and replaces every occurrence of a target pair with a new token ID. The tricky part: when you merge a pair, you consume both elements and skip ahead by 2, not 1.
  </Description>

```python
def merge(ids, pair, new_id):
    """
    In the list of integers (ids), replace all consecutive occurrences 
    of pair with the new token new_id.
    """
    newids = []
    i = 0
    while i < len(ids):
        # Check if we found the pair (and aren't at the end)
        # If yes: append new_id, skip ahead by 2
        # If no: append current element, move ahead by 1
        pass
    return newids
```

  <Callout type="note" title="Challenge: The Token Merger">
    This one requires careful index management. Switch to the **Challenge** tab to try it!
  </Callout>

  <h4 className="text-lg font-semibold text-slate-800 dark:text-slate-200 mt-6 mb-3">Function 3: The Training Loop</h4>

  <Description>
    Finally, we combine everything into a training loop. We start with UTF-8 bytes, then repeatedly find the most frequent pair and merge it with a new token ID.
  </Description>

```python
def train_bpe(text, num_merges):
    ids = list(text.encode("utf-8"))  # Start with raw bytes
    merges = {}  # Will store our learned merge rules
    
    for i in range(num_merges):
        # Find the most frequent pair, merge it, record the rule
        # New token IDs start at 256 (after the 0-255 byte range)
        pass
    
    return ids, merges
```

  <Callout type="note" title="Challenge: The BPE Trainer">
    Put it all together! Switch to the **Challenge** tab to implement the complete training function.
  </Callout>

  <Description>
    After training, `merges` contains all the rules needed to tokenize new text, and `ids` contains your compressed training data.
  </Description>
</Step>


<Step title="4. Regex Pre-splitting">
  <Description>
    If you ran the code above on English text, you'd have a working tokenizer, but it would have a flaw.
  </Description>

  <Description>
    Consider the string: `"dog dog. dog!"`
  </Description>

  <Description>
    To a human, the core concept is **"dog"**. But our simple algorithm treats spaces and punctuation as characters to merge. It might see `dog.` as completely different from `dog ` (with a space). It might even merge `g` and `.` into a weird token `g.`.
  </Description>

  <div className="my-4 p-4 bg-slate-100 dark:bg-slate-800/60 rounded-lg border border-slate-300 dark:border-slate-600">
    <div className="text-sm font-medium text-slate-700 dark:text-slate-300 mb-2">The Problem</div>
    <div className="font-mono text-sm space-y-1 text-slate-600 dark:text-slate-400">
      <div>"dog" + space → one pattern</div>
      <div>"dog" + period → different pattern</div>
      <div>"dog" + exclamation → yet another pattern</div>
    </div>
    <div className="mt-2 text-sm text-slate-600 dark:text-slate-400">
      The model has to learn "dog" three separate times!
    </div>
  </div>

  <h4 className="text-lg font-semibold text-slate-800 dark:text-slate-200 mt-6 mb-3">The Fix: Regex Splitting</h4>

  <Description>
    Modern tokenizers (like GPT-4's) use **Regular Expressions** to pre-split text into meaningful chunks *before* applying BPE. They force boundaries between words and punctuation.
  </Description>

  <div className="my-4 p-4 bg-slate-50 dark:bg-slate-800/50 rounded-lg border border-slate-200 dark:border-slate-700">
    <div className="text-sm font-medium text-slate-700 dark:text-slate-300 mb-2">The Solution</div>
    <div className="space-y-2">
      <div className="flex items-center gap-3">
        <span className="text-sm text-slate-600 dark:text-slate-400">Input:</span>
        <span className="font-mono bg-white dark:bg-slate-900 px-2 py-1 rounded border border-slate-200 dark:border-slate-700">"dog. dog!"</span>
      </div>
      <div className="flex items-center gap-3 flex-wrap">
        <span className="text-sm text-slate-600 dark:text-slate-400">After regex:</span>
        <div className="flex gap-1 flex-wrap">
          <span className="font-mono bg-slate-200 dark:bg-slate-700 px-2 py-1 rounded text-slate-700 dark:text-slate-300">"dog"</span>
          <span className="font-mono bg-slate-100 dark:bg-slate-800 px-2 py-1 rounded text-slate-500 dark:text-slate-400">"."</span>
          <span className="font-mono bg-slate-200 dark:bg-slate-700 px-2 py-1 rounded text-slate-700 dark:text-slate-300">" dog"</span>
          <span className="font-mono bg-slate-100 dark:bg-slate-800 px-2 py-1 rounded text-slate-500 dark:text-slate-400">"!"</span>
        </div>
      </div>
    </div>
    <div className="mt-3 text-sm text-slate-600 dark:text-slate-400">
      BPE runs on each chunk independently. "dog" is always tokenized as "dog".
    </div>
  </div>

  <Description>
    This semantic separation helps the model learn language patterns much faster because it doesn't waste capacity learning that "dog." and "dog," are the same word.
  </Description>
</Step>


<Step title="5. The Complete Pipeline">
  <Description>
    We've now covered the entire journey of text before it reaches the neural network.
  </Description>

  <div className="my-4 relative">
    <div className="absolute left-6 top-0 bottom-0 w-0.5 bg-slate-200 dark:bg-slate-700" />
    
    <div className="space-y-6">
      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-slate-100 dark:bg-slate-800 border-2 border-slate-300 dark:border-slate-600 flex items-center justify-center text-slate-700 dark:text-slate-300 font-bold z-0">1</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-slate-800 dark:text-slate-200">Raw Text</div>
          <div className="font-mono text-sm text-slate-600 dark:text-slate-400 mt-1">"Hello World"</div>
        </div>
      </div>

      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-slate-100 dark:bg-slate-800 border-2 border-slate-300 dark:border-slate-600 flex items-center justify-center text-slate-700 dark:text-slate-300 font-bold z-0">2</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-slate-800 dark:text-slate-200">Regex Split</div>
          <div className="font-mono text-sm text-slate-600 dark:text-slate-400 mt-1">["Hello", " World"]</div>
        </div>
      </div>

      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-slate-100 dark:bg-slate-800 border-2 border-slate-300 dark:border-slate-600 flex items-center justify-center text-slate-700 dark:text-slate-300 font-bold z-0">3</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-slate-800 dark:text-slate-200">UTF-8 Encoding</div>
          <div className="font-mono text-sm text-slate-600 dark:text-slate-400 mt-1">[72, 101, 108, 108, 111], [32, 87, 111, 114, 108, 100]</div>
        </div>
      </div>

      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-slate-100 dark:bg-slate-800 border-2 border-slate-300 dark:border-slate-600 flex items-center justify-center text-slate-700 dark:text-slate-300 font-bold z-0">4</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-slate-800 dark:text-slate-200">BPE Merging</div>
          <div className="text-sm text-slate-600 dark:text-slate-400 mt-1">Apply learned merge rules to compress bytes into tokens</div>
        </div>
      </div>

      <div className="relative flex gap-4">
        <div className="w-12 h-12 rounded-full bg-slate-100 dark:bg-slate-800 border-2 border-slate-300 dark:border-slate-600 flex items-center justify-center text-slate-700 dark:text-slate-300 font-bold z-0">5</div>
        <div className="flex-1 pt-2">
          <div className="font-semibold text-slate-800 dark:text-slate-200">Token IDs</div>
          <div className="font-mono text-sm text-slate-600 dark:text-slate-400 mt-1">[15496, 995]</div>
          <div className="text-sm text-slate-600 dark:text-slate-400 mt-1">This is what the neural network actually sees</div>
        </div>
      </div>
    </div>
  </div>

  <Callout type="success" title="Summary">
    * **Tokenization** compresses text into efficient chunks called tokens
    * **BPE** learns merge rules by iteratively combining frequent pairs
    * **Regex pre-splitting** ensures words stay consistent regardless of punctuation
    * The final output is a sequence of **integer IDs**: the atoms of language models
  </Callout>

  <Description>
    These integers are the only thing the neural network ever sees. They are the atoms of the Large Language Model universe. In the next chapter, we'll see how the model turns these integers into **Vectors** (Embeddings) to begin understanding their meaning.
  </Description>
</Step>