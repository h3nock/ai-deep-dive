{
  "id": "build-gpt/02-tokenization/05-encoder",
  "version": "v1",
  "runner": "encode(text, merges)",
  "requires_torch": false,
  "time_limit_s": 5,
  "memory_mb": 1024,
  "comparison": {
    "type": "exact"
  }
}
