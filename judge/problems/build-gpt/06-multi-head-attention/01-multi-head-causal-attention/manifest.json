{
  "id": "build-gpt/06-multi-head-attention/01-multi-head-causal-attention",
  "version": "v1",
  "runner": "multi_head_causal_attention(X, W_Q, W_K, W_V, W_O, num_heads)",
  "requires_torch": true,
  "time_limit_s": 5,
  "memory_mb": 1024,
  "comparison": {
    "type": "allclose",
    "rtol": 1e-05,
    "atol": 1e-05
  }
}
